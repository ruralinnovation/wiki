[
  {
    "objectID": "grant_aws.html",
    "href": "grant_aws.html",
    "title": "Grant",
    "section": "",
    "text": "3 categories:\n- Momentum to Modernize, \n\n- Go Further, Faster, \n\n- Pathfinder ‚Äì Generative AI \n\n\n\nWe can submit only submit one application (one categories)\nRound one: open to everyone, round two on invitation\nProposal w/o any personal info\nUS nonprofit with active 501\n\n\n\n\nportal will open on May 3, 2024\nDead line for round one: June 3 2024\nNotification for round 2 July 17 2024\n\n\n\n\n\n\nTutorial Video\nMy takes on it (1.5 speed video):\n\nImportance of holistics approach ie: provide tech + support\n‚ÄúMomentum to modernize‚Äù: We are really looking for organization that view these types of project as fundational steps‚Äù (setting up the fundation to unlock futur doors)\n‚ÄúTo go further faster award‚Äù: ‚ÄúThing big‚Äù, ‚Äúhighly innovative program‚Äù, ‚ÄúDevOps‚Äù\n‚ÄúAlways lead the mission with technology as a core strategic enablement‚Äù\n‚ÄúSenior leader who is the project champion‚Äù\n‚Äúconvey urgency and importance of your project‚Äù\n‚Äúwork backward from the end user‚Äù: we are lacking here, I think first end user is us, them bb team, then ISP / BB consultant, then citizen\n‚Äúend goal connect your mission‚Äù\n‚Äúdynamic dashboard‚Äù: should we look into quicksight?\n‚Äúwhat services will it use‚Äù: S3, next is do we have a spark question or a duck db question\n‚Äúhow do the technical elements fits together‚Äù:\n\nI think our data warehouse is good we want to leverage a data lake to select more valuable data to move to teh data warehouse/product phase\n\n‚Äúwhat skill resources do you have in house‚Äù\n‚Äúwhat you plan to acquire the most‚Äù\norganizing asset in data lake and build an infrastructure to query it\nexpose it to partner / first end user\nopen it to more partners\nMaybe we can use some AI ML on checking FCC data aginst location (presence or not) and MLAB on quality of services\n‚Äúarticulate why each piece of the imagine grant is useful not only cash‚Äù\nresearch long term (maybe more for phase 2)\nautomate the ingestion delivery\ntechnical diagram: AWS architecture diagrams\n\nWe can contact the AWS Nonprofits team (form), and should, we need to prepare an high level summary before\n\n\n\n\n\nDecide on an what we want to do: Dead line May 16\nWrite high level summary and get feedback from AWS Team May 171\n\n1¬†p11 of AWS Imagine 2024\nWrite the grant with feedback from AWS\nMDA proposition should be done May 24\nGiving us buffer we (Robin?/Mike?) want to submit May 30\n\n\n\n\nAt CORI we are helping rural communities and their leaders understand the rural divide and reduce it. This is a critical moment for broadband funding and infrastructure expansion that will have major impacts on communities across the nation. Our broadband team contributes to it and our research team provides insights on how this affects communities.\nBoth rely on fresh, actionable and easy to share data. To do that our mapping and data team have been ingested, processed FCC data and shared those results either with web apps or with in-house analytics. Sadly our manual processes do not allow us: - to keep up with the volume and frequencies of FCC releases - to add different source of data and combine them - get the granularity that expert on the ground needs - keep memory over time, trends, to drive thought leadership - expand our audience\nIn its ever evolving landscape, being able to efficiently capture and store data with the help of AWS infrastructure will create valuable resources for our partner and communities. Both the ‚ÄúMomentum to Modernize‚Äù and ‚ÄúGo Further, Faster‚Äù grants could potentially help us and provide new opportunities to become a hub and source of truth/expertise on broadband data."
  },
  {
    "objectID": "grant_aws.html#rules",
    "href": "grant_aws.html#rules",
    "title": "Grant",
    "section": "",
    "text": "We can submit only submit one application (one categories)\nRound one: open to everyone, round two on invitation\nProposal w/o any personal info\nUS nonprofit with active 501\n\n\n\n\nportal will open on May 3, 2024\nDead line for round one: June 3 2024\nNotification for round 2 July 17 2024"
  },
  {
    "objectID": "grant_aws.html#good-to-know-resources",
    "href": "grant_aws.html#good-to-know-resources",
    "title": "Grant",
    "section": "",
    "text": "Tutorial Video\nMy takes on it (1.5 speed video):\n\nImportance of holistics approach ie: provide tech + support\n‚ÄúMomentum to modernize‚Äù: We are really looking for organization that view these types of project as fundational steps‚Äù (setting up the fundation to unlock futur doors)\n‚ÄúTo go further faster award‚Äù: ‚ÄúThing big‚Äù, ‚Äúhighly innovative program‚Äù, ‚ÄúDevOps‚Äù\n‚ÄúAlways lead the mission with technology as a core strategic enablement‚Äù\n‚ÄúSenior leader who is the project champion‚Äù\n‚Äúconvey urgency and importance of your project‚Äù\n‚Äúwork backward from the end user‚Äù: we are lacking here, I think first end user is us, them bb team, then ISP / BB consultant, then citizen\n‚Äúend goal connect your mission‚Äù\n‚Äúdynamic dashboard‚Äù: should we look into quicksight?\n‚Äúwhat services will it use‚Äù: S3, next is do we have a spark question or a duck db question\n‚Äúhow do the technical elements fits together‚Äù:\n\nI think our data warehouse is good we want to leverage a data lake to select more valuable data to move to teh data warehouse/product phase\n\n‚Äúwhat skill resources do you have in house‚Äù\n‚Äúwhat you plan to acquire the most‚Äù\norganizing asset in data lake and build an infrastructure to query it\nexpose it to partner / first end user\nopen it to more partners\nMaybe we can use some AI ML on checking FCC data aginst location (presence or not) and MLAB on quality of services\n‚Äúarticulate why each piece of the imagine grant is useful not only cash‚Äù\nresearch long term (maybe more for phase 2)\nautomate the ingestion delivery\ntechnical diagram: AWS architecture diagrams\n\nWe can contact the AWS Nonprofits team (form), and should, we need to prepare an high level summary before"
  },
  {
    "objectID": "grant_aws.html#retro-planning-for-the-mda",
    "href": "grant_aws.html#retro-planning-for-the-mda",
    "title": "Grant",
    "section": "",
    "text": "Decide on an what we want to do: Dead line May 16\nWrite high level summary and get feedback from AWS Team May 171\n\n1¬†p11 of AWS Imagine 2024\nWrite the grant with feedback from AWS\nMDA proposition should be done May 24\nGiving us buffer we (Robin?/Mike?) want to submit May 30"
  },
  {
    "objectID": "grant_aws.html#first-brainstorm",
    "href": "grant_aws.html#first-brainstorm",
    "title": "Grant",
    "section": "",
    "text": "At CORI we are helping rural communities and their leaders understand the rural divide and reduce it. This is a critical moment for broadband funding and infrastructure expansion that will have major impacts on communities across the nation. Our broadband team contributes to it and our research team provides insights on how this affects communities.\nBoth rely on fresh, actionable and easy to share data. To do that our mapping and data team have been ingested, processed FCC data and shared those results either with web apps or with in-house analytics. Sadly our manual processes do not allow us: - to keep up with the volume and frequencies of FCC releases - to add different source of data and combine them - get the granularity that expert on the ground needs - keep memory over time, trends, to drive thought leadership - expand our audience\nIn its ever evolving landscape, being able to efficiently capture and store data with the help of AWS infrastructure will create valuable resources for our partner and communities. Both the ‚ÄúMomentum to Modernize‚Äù and ‚ÄúGo Further, Faster‚Äù grants could potentially help us and provide new opportunities to become a hub and source of truth/expertise on broadband data."
  },
  {
    "objectID": "grant_aws.html#organizational-information",
    "href": "grant_aws.html#organizational-information",
    "title": "Grant",
    "section": "Organizational Information",
    "text": "Organizational Information\n\nFull, Registered Organization Name\nTax ID\nWebsite URL\nOrganization Full Address\n\n\nStreet,\nCity,\nState,\nZip Code\n\n\nAnnual Organizational Revenue (most recent fiscal year‚Äôs, in USD) [Select one]\n\n\nLess than $50,000\n$50,000 - $999,999\n$1,000,000 - $4,999,999\n$5,000,000 - $9,999,999\n$10,000,000 - $99,999,999\n$100,000,000 or more\nDon‚Äôt know/ Not sure\n\n\nAnnual IT Budget (most recent fiscal year‚Äôs expenses in USD)\n\n\nLess than $10,000\n$10,000 - $50,000\n$50,000 - $150,000\n$150,000 - $500,000\n$500,000 or more\n\n\nProposal Contact\n\n\nFirst and Last Name,\nEmail Address,\nPhone Number\n\n8. Project Lead (First and Last Name, Email Address, Phone Number) Not in the portal\n\nRole in Organization\n\n\nPresident or CEO\nOther C-Level of executive management\nVice president or equivalent\nBoard of Directors\nDirector or equivalent\nManager or equivalent\nGeneralist, staff, associate, or equivalent\nOther\n\n\nFunctional Area in Organization\n\n\nLeadership or overall management\nProgram management\nEvent management\nFundraising/ Development\nCommunity outreach\nTechnology\nHuman resources\nMarketing\nFinance\nAdministration\nOther\n\n\nPrimary Mission Area\n\n\nArts, Culture, and Humanities\nCredit Union\nEducation\nEnvironment and Animals\nHealth\nHuman Services\nInternational, Foreign Affairs\nPublic, Societal Benefit\nReligion Related\nScience, Technology & Social Science\nYouth Development\nMutual/Membership Benefit\nPublic Utilities\nUnknown, Unclassified\n\n\nDoes your organization have an AWS account? [Yes/No]\n\n\nIf selected ‚ÄúYes,‚Äù please provide your account ID (if your organization has multiple IDs, you can provide any one of them)\n\n\nHave you applied for the IMAGINE Grant before? [Yes/No]\n\n-&gt; it will need to confirmed"
  },
  {
    "objectID": "grant_aws.html#project-information",
    "href": "grant_aws.html#project-information",
    "title": "Grant",
    "section": "Project Information",
    "text": "Project Information\n\nWhich AWS IMAGINE Grant category are you applying for? [select one]\n\n\nMomentum to Modernize award &lt;‚Äì The one I think we should apply, was my first tought but maybe we should go for the other one\nGo Further, Faster award\nPathfinder ‚Äì Generative AI award\n\n\nWhat is your project and how does it relate to your organization‚Äôs mission? [200 - 350 words]\n\n3. (For Pathfinder applicants only) How does generative AI fit into your overall project design? How will it strategically enhance your mission achievement? [100 - 250 words]\n4. (For Pathfinder applicants only) How is your organization using data to make decisions today? [100 - 250 words]\n\nWhat are the intended outcomes of your project? What new capabilities would it unlock for your organization? [200 - 350 words]\nWhat is driving the need for this specific project? Why now? [200 - 350 words]\nAt what stage is your project currently? [Select one]\n\n\nConcept\nPlanning\nUnder development\nCurrently running\n\n\nWhat most closely aligns with what this project will enable your organization to do? [Select one]\n\n\nMigrate legacy IT systems, applications, and data centers to the cloud.\nIncrease productivity with modernized tools, applications, and databases. &lt;- this one\nIncrease cost efficiency with optimized IT infrastructure.\nEnable business continuity with archive, backup, and disaster recovery.\nIncrease resilience, scaling, and availability of existing workloads and databases. &lt;- this one\nIncrease security and governance of data/ ensure security compliance. (e.g.¬†HIPAA, SOC)\nInform better decision making by aggregating disparate data sources and applying analytics (e.g., visualization, AI/ML). &lt;- this one\nEnable accurate forecasting based on past data with predictive analysis using machine learning.\nBetter understand and engage members/donors/beneficiaries by creating a 360 view with constituent data.\nEnhance digital content delivery for donors, members, beneficiaries, or other stakeholders (e.g., website, video, mobile app) &lt;- maybe\nEnable multichannel services for constituents and automate simple, clerical tasks for employees (e.g.¬†chatbots, contact centers, GenAI-powered)\nPersonalize and streamline communications for more effective donor/member/constituent engagement and fundraising.\nI don‚Äôt know how AWS fits into my project at this time, but I am interested in learning more.\nOther"
  },
  {
    "objectID": "grant_aws.html#technical-design",
    "href": "grant_aws.html#technical-design",
    "title": "Grant",
    "section": "Technical Design",
    "text": "Technical Design\n\nDescribe your project‚Äôs technical design at a high level. What does it do and how? [200 - 350 words]\n\nThe goals are to builds an analytics data lake that centralise broadband data to support broadband expension, grant applications and research on broadband equity and development.\nWe are curently using a data warehouse solution that work great once we have clear understanding of what our partners need but the rapid change in the broadband landscape make that approach to slow to innovate and serve our communities. We need intermediate places and process that allow us to catalog our assets, then work on non structured data and provide quick insights for important feedback loops with partners and stackholders. Then we need to build the infrastucture to ease the query of such data: do we have spark question or duck DB question (scope neded here prob: R package use of AWS solution?).\nIdea on machine learning (FCC data is declarative data and need to be confronted either on local knowledge and/or other data source : census, MLAB ?: anomaly detection?) come after can we use multiple source of data to get more close to reallity on the ground and build better bb infrastructure?\nQuestion on being publics on that.\n\nWhat type(s) of workload(s) will your project include? [Select all that apply] (Hint: Learn more about types of workloads in our latest publication on how nonprofits leverage the cloud today)\n\n\nDigital content management (e.g.¬†hosting and scaling websites)\nDigital content delivery (e.g.¬†virtual events, virtual learning platform)\nNet-new application development (e.g.¬†web app, mobile app, SaaS platform)\nApplication integration (e.g.¬†serverless, microservices, distributed systems)\nDevOps pipeline (e.g.¬†CI/CD)\nContainer orchestration and management\nManaged security services\nContent storage and backup (e.g.¬†disaster recovery)\nMigration and optimization (e.g.¬†systems, data, application)\nVirtual desktop\nData lake\nData warehouse\nData analytics and visualization\nManaged AI/ML services (e.g.¬†intelligent document processing, image recognition)\nAI/ML for research\nAI/ML for predictive modeling\nGenerative AI\nCustomer experience (e.g.¬†call/contact center, virtual assistant)\nInternet of things (IoT)\nBlockchain\nOther. Please specify:\nNone of the above\n\n\nDescribe the resources and technical skills you may need to complete the project successfully. Do you have these resources in-house? If no, what will be your plan to acquire the skills needed? [200 - 350 words]\nTo successfully complete your project, would you need support from a technology and/or implementation partner? (Hint: An AWS Partner is an external expert who leverages AWS to build solutions and services for customers. See a list of AWS Partners)\n\n\nYes, and I already work with a partner/ plan to work with a specific partner.\n\n\n[If selected] Please list partner:\n\n\nYes, I do not have a partner identified but would like a recommendation.\nNo, I do not plan to work with a partner/ we plan to do the work in-house\nOther. Please explain:\n\n\nIs your organization‚Äôs IT infrastructure currently: [Select one]\n\n\nOn-premises\nOn AWS\nWith another provider\nHybrid\n\n\nIs the IT infrastructure for the project you are proposing for this grant opportunity currently: [Select one]\n\n\nOn-premises\nOn AWS\nNet-new to your organization\nWith another provider\nHybrid\n\n\nIf you were to not receive funding or be awarded the full amount of funding for either award, would your organization still be able to implement this project?\n\n\nYes\nNo"
  },
  {
    "objectID": "Working-with-the-GraphQL-Schemas.html",
    "href": "Working-with-the-GraphQL-Schemas.html",
    "title": "Working With The Apollo GraphQL Schema",
    "section": "",
    "text": "The schema design and definition has been purposely decoupled from the Apollo Server specification located in the infrastructure (CDK) codebase. The schema is built as part of the sequence of build steps for the entire project, and the Apollo Server (Lambda) imports the compiled SDL for use by your GraphQL server.\n\nThe Apollo GraphQL Schema is a separate package located in the packages/schemas directory.\n\n\n\n‚îú‚îÄ‚îÄ dist                                    # build output (auto-generated during compilation)\n‚îú‚îÄ‚îÄ schema                                  # Schema definitions written in Typescript\n    ‚îú‚îÄ‚îÄ queries                             # Query definitions written in Typescript\n‚îú‚îÄ‚îÄ merge-schemas.ts                      # Script that imports merged schemas, converts to `SDL` and outputs to `dist` directory.\n‚îî‚îÄ‚îÄ index.ts                                # Entry File that imports and merges queries\n\n\n\n\nWe leverage GraphQL Tools in order to design and spec out schemas in Typescript.\n\nGraphQL Tools is a set of NPM packages and an opinionated structure for how to build a GraphQL schema and resolvers in JavaScript, following the GraphQL-first development workflow.\n\n\n\n\n\nThe JavaScript reference implementation for GraphQL, a query language for APIs created by Facebook.\n\nWe import constructs from graphql node library and leverage them to build schemas and queries.\n\n\n\nThere is a custom GeoJson Type (following the geojson spec) located in the root of the schemas/schemas directory. This is the primary return type used by the all the resolvers in the Api. As your application and API grows, this will obviously be supplemented with different return types.\n\n\n\n\nWe have organized individual service queries into their own file (e.g.¬†bcatQueries). You can follow this structure, or develop another structure that is more friendly to your organization.\n\n\nCreate a new query file in schemas/schemas/queries.\nImport your required libraries at the top of the file\n\nFor example:\nimport { GraphQLBoolean, GraphQLList, GraphQLString } from 'graphql';\nimport GeoJSON from '../geojson';\n\nCreate queries (You can use the bcatQueries as an scaffolding)\n\n\n\n\n  // Query Name\n  auction_904_subsidy_awards_county_geojson: {\n    // Query Return Type\n    type: GeoJSON.FeatureCollectionObject,\n    // Query Arguments\n    args: {\n      county: {\n        type: GraphQLString!,\n      },\n      skipCache: {\n        type: GraphQLBoolean,\n      },\n    },\n    // Query Resolver\n    resolve: async (\n      _: any,\n      { county, skipCache }: { county: string; skipCache: boolean | undefined },\n      { dataSources: { pythonApi }, redisClient }: any,\n      info: any\n    ) =&gt; {\n      return skipCache\n        ? await pythonApi.getItem(`bcat/auction_904_subsidy_awards/geojson?geoid_co=${county}`)\n        : await redisClient.checkCache(`auction_904_subsidy_awards-${county}`, async () =&gt; {\n            return await pythonApi.getItem(`bcat/auction_904_subsidy_awards/geojson?geoid_co=${county}`);\n          });\n    },\n  }\n\n\nApollo Studio Resolver Docs\nconst resolvers = {\n  Query: {\n    user(parent, args, context, info) {\n      return users.find(user =&gt; user.id === args.id);\n    }\n  }\n}\nA resolver can optionally accept four positional arguments: (parent, args, context, info)\n\n\n\nimage\n\n\nThe Apollo Server has a single Data Source (Python RestAPI) and the Redis Cache Adapter. Both of these adapters are available on the context argument.\nYou will see in the BCAT query above, the context argument is destructured (more info) into:\n{ dataSources: { pythonApi }, redisClient }: any,\nThis will allow you to reference these adapters without writing something like context.dataSources.pythonApi. It makes for cleaner code.\nPassing these adapters in as arguments to your resolvers also allows you to decouple the schema from the server code (where these adapters are configured and instantiated).\nBoth of these adapters are instantiated in your Apollo Server Lambda and added to the apollo configuration. Then they are available in these arguments for all resolvers.\nWe have included a skipCache optional argument in all of the bcatQueries. If this argument is defined and true, the resolver will skip checking the RedisCache for an existing value for the query. This is a good option for your testing.\nOtherwise, every resolver first checks the RedisCache using the checkCache method on the RedisAdapter client with a key that resembles:\n{queryName}-{argument} e.g.¬†auction_904_subsidy_awards-${county}\nThe checkCache method on the RedisClient accepts a second argument that provided the python data source query to use if there is not data located in the cache for that query. It will make a request using this method then automatically store the return value in the cache before returning it to the resolver.\n  checkCache(key: string, cb: Function, maxAge: number = globalTTL): Promise&lt;unknown&gt; {\n    return new Promise((resolve, reject) =&gt; {\n      try {\n        this.getCacheValue(key).then(cacheRes =&gt; {\n          if (!cacheRes) {\n            cb().then(dbValue =&gt; {\n              if (!dbValue) {\n                dbValue = null;\n              }\n              this.rawCache.setex(key, maxAge, JSON.stringify(dbValue));\n              resolve(dbValue);\n            });\n          } else {\n            resolve(cacheRes);\n          }\n        });\n      } catch (err) {\n        reject(err);\n      }\n    });\n  }\n\n\nThe return value(s) from your PythonAPI calls MUST match the return schema defined in your GraphQL query. The request will error if it does not (e.g.¬†if a required attribute is missing).\n\n\n\n\n\n\nData sources are classes that Apollo Server can use to encapsulate fetching data from a particular source, such as a database or a REST API. These classes help handle caching, deduplication, and errors while resolving operations. Your server can use any number of different data sources. You don‚Äôt have to use data sources to fetch data, but they‚Äôre strongly recommended.\n\n\nNOTE: The following data sources are configured, instantiated and connected to the Apollo Server in:\n\npackages/infrastructure/src/constructs/api/ApolloGraphqlServer\nWe ahve included this information here to shed more light on how your schema queries connect to the source data.\nThe only data source for the Apollo Server is the existing Python Data RestAPI.\nThis data source is a RESTDataSource and is fully supported by Apollo Studio.\nApollo Studio Data Sources\nApollo Studio RESTDataSource\n\n\nWe have created a BaseDataSource that implements the core getItem request for the RestAPI. For extensibility we created another Data Source class that inherits from this Base class, to add custom logic to in the future.\nAs of now the API is READONLY which means the getItem method should be enough for most cases. As the API grows and other request methods are includes (POST, PUT), you can add these core methods to the Base class for all other DataSource classes to inherit from.\nimport { RESTDataSource, RequestOptions } from 'apollo-datasource-rest';\nimport { EnvConfig } from '../../EnvConfig';\nexport class BaseDataSource extends RESTDataSource {\n  constructor() {\n    super();\n    this.baseURL = `${EnvConfig.PYTHON_API_URL}`;\n  }\n\n  willSendRequest(request: RequestOptions) {\n    request.headers.set('Authorization', this.context.headers.Authorization);\n  }\n  async getItem(path?: string) {\n    const res = await this.get(path ? path : '', undefined);\n    return res;\n  }\n}\nimport { BaseDataSource } from './BaseDataSource';\nexport class PythonRestApi extends BaseDataSource {\n  constructor() {\n    super();\n  }\n}\n\n\n\nData Sources are added to the Apollo Server Configuration and are passed into resolvers through the the context.\n\n\n\nimage"
  },
  {
    "objectID": "Working-with-the-GraphQL-Schemas.html#directory-structure",
    "href": "Working-with-the-GraphQL-Schemas.html#directory-structure",
    "title": "Working With The Apollo GraphQL Schema",
    "section": "",
    "text": "‚îú‚îÄ‚îÄ dist                                    # build output (auto-generated during compilation)\n‚îú‚îÄ‚îÄ schema                                  # Schema definitions written in Typescript\n    ‚îú‚îÄ‚îÄ queries                             # Query definitions written in Typescript\n‚îú‚îÄ‚îÄ merge-schemas.ts                      # Script that imports merged schemas, converts to `SDL` and outputs to `dist` directory.\n‚îî‚îÄ‚îÄ index.ts                                # Entry File that imports and merges queries"
  },
  {
    "objectID": "Working-with-the-GraphQL-Schemas.html#graphql-tools",
    "href": "Working-with-the-GraphQL-Schemas.html#graphql-tools",
    "title": "Working With The Apollo GraphQL Schema",
    "section": "",
    "text": "We leverage GraphQL Tools in order to design and spec out schemas in Typescript.\n\nGraphQL Tools is a set of NPM packages and an opinionated structure for how to build a GraphQL schema and resolvers in JavaScript, following the GraphQL-first development workflow."
  },
  {
    "objectID": "Working-with-the-GraphQL-Schemas.html#graphql.js",
    "href": "Working-with-the-GraphQL-Schemas.html#graphql.js",
    "title": "Working With The Apollo GraphQL Schema",
    "section": "",
    "text": "The JavaScript reference implementation for GraphQL, a query language for APIs created by Facebook.\n\nWe import constructs from graphql node library and leverage them to build schemas and queries."
  },
  {
    "objectID": "Working-with-the-GraphQL-Schemas.html#geojson-type",
    "href": "Working-with-the-GraphQL-Schemas.html#geojson-type",
    "title": "Working With The Apollo GraphQL Schema",
    "section": "",
    "text": "There is a custom GeoJson Type (following the geojson spec) located in the root of the schemas/schemas directory. This is the primary return type used by the all the resolvers in the Api. As your application and API grows, this will obviously be supplemented with different return types."
  },
  {
    "objectID": "Working-with-the-GraphQL-Schemas.html#adding-queries-that-connect-to-the-python-data-service-restapi",
    "href": "Working-with-the-GraphQL-Schemas.html#adding-queries-that-connect-to-the-python-data-service-restapi",
    "title": "Working With The Apollo GraphQL Schema",
    "section": "",
    "text": "We have organized individual service queries into their own file (e.g.¬†bcatQueries). You can follow this structure, or develop another structure that is more friendly to your organization.\n\n\nCreate a new query file in schemas/schemas/queries.\nImport your required libraries at the top of the file\n\nFor example:\nimport { GraphQLBoolean, GraphQLList, GraphQLString } from 'graphql';\nimport GeoJSON from '../geojson';\n\nCreate queries (You can use the bcatQueries as an scaffolding)"
  },
  {
    "objectID": "Working-with-the-GraphQL-Schemas.html#query-structure",
    "href": "Working-with-the-GraphQL-Schemas.html#query-structure",
    "title": "Working With The Apollo GraphQL Schema",
    "section": "",
    "text": "// Query Name\n  auction_904_subsidy_awards_county_geojson: {\n    // Query Return Type\n    type: GeoJSON.FeatureCollectionObject,\n    // Query Arguments\n    args: {\n      county: {\n        type: GraphQLString!,\n      },\n      skipCache: {\n        type: GraphQLBoolean,\n      },\n    },\n    // Query Resolver\n    resolve: async (\n      _: any,\n      { county, skipCache }: { county: string; skipCache: boolean | undefined },\n      { dataSources: { pythonApi }, redisClient }: any,\n      info: any\n    ) =&gt; {\n      return skipCache\n        ? await pythonApi.getItem(`bcat/auction_904_subsidy_awards/geojson?geoid_co=${county}`)\n        : await redisClient.checkCache(`auction_904_subsidy_awards-${county}`, async () =&gt; {\n            return await pythonApi.getItem(`bcat/auction_904_subsidy_awards/geojson?geoid_co=${county}`);\n          });\n    },\n  }\n\n\nApollo Studio Resolver Docs\nconst resolvers = {\n  Query: {\n    user(parent, args, context, info) {\n      return users.find(user =&gt; user.id === args.id);\n    }\n  }\n}\nA resolver can optionally accept four positional arguments: (parent, args, context, info)\n\n\n\nimage\n\n\nThe Apollo Server has a single Data Source (Python RestAPI) and the Redis Cache Adapter. Both of these adapters are available on the context argument.\nYou will see in the BCAT query above, the context argument is destructured (more info) into:\n{ dataSources: { pythonApi }, redisClient }: any,\nThis will allow you to reference these adapters without writing something like context.dataSources.pythonApi. It makes for cleaner code.\nPassing these adapters in as arguments to your resolvers also allows you to decouple the schema from the server code (where these adapters are configured and instantiated).\nBoth of these adapters are instantiated in your Apollo Server Lambda and added to the apollo configuration. Then they are available in these arguments for all resolvers.\nWe have included a skipCache optional argument in all of the bcatQueries. If this argument is defined and true, the resolver will skip checking the RedisCache for an existing value for the query. This is a good option for your testing.\nOtherwise, every resolver first checks the RedisCache using the checkCache method on the RedisAdapter client with a key that resembles:\n{queryName}-{argument} e.g.¬†auction_904_subsidy_awards-${county}\nThe checkCache method on the RedisClient accepts a second argument that provided the python data source query to use if there is not data located in the cache for that query. It will make a request using this method then automatically store the return value in the cache before returning it to the resolver.\n  checkCache(key: string, cb: Function, maxAge: number = globalTTL): Promise&lt;unknown&gt; {\n    return new Promise((resolve, reject) =&gt; {\n      try {\n        this.getCacheValue(key).then(cacheRes =&gt; {\n          if (!cacheRes) {\n            cb().then(dbValue =&gt; {\n              if (!dbValue) {\n                dbValue = null;\n              }\n              this.rawCache.setex(key, maxAge, JSON.stringify(dbValue));\n              resolve(dbValue);\n            });\n          } else {\n            resolve(cacheRes);\n          }\n        });\n      } catch (err) {\n        reject(err);\n      }\n    });\n  }\n\n\nThe return value(s) from your PythonAPI calls MUST match the return schema defined in your GraphQL query. The request will error if it does not (e.g.¬†if a required attribute is missing)."
  },
  {
    "objectID": "Working-with-the-GraphQL-Schemas.html#understanding-apollo-server-data-sources",
    "href": "Working-with-the-GraphQL-Schemas.html#understanding-apollo-server-data-sources",
    "title": "Working With The Apollo GraphQL Schema",
    "section": "",
    "text": "Data sources are classes that Apollo Server can use to encapsulate fetching data from a particular source, such as a database or a REST API. These classes help handle caching, deduplication, and errors while resolving operations. Your server can use any number of different data sources. You don‚Äôt have to use data sources to fetch data, but they‚Äôre strongly recommended.\n\n\nNOTE: The following data sources are configured, instantiated and connected to the Apollo Server in:\n\npackages/infrastructure/src/constructs/api/ApolloGraphqlServer\nWe ahve included this information here to shed more light on how your schema queries connect to the source data.\nThe only data source for the Apollo Server is the existing Python Data RestAPI.\nThis data source is a RESTDataSource and is fully supported by Apollo Studio.\nApollo Studio Data Sources\nApollo Studio RESTDataSource\n\n\nWe have created a BaseDataSource that implements the core getItem request for the RestAPI. For extensibility we created another Data Source class that inherits from this Base class, to add custom logic to in the future.\nAs of now the API is READONLY which means the getItem method should be enough for most cases. As the API grows and other request methods are includes (POST, PUT), you can add these core methods to the Base class for all other DataSource classes to inherit from.\nimport { RESTDataSource, RequestOptions } from 'apollo-datasource-rest';\nimport { EnvConfig } from '../../EnvConfig';\nexport class BaseDataSource extends RESTDataSource {\n  constructor() {\n    super();\n    this.baseURL = `${EnvConfig.PYTHON_API_URL}`;\n  }\n\n  willSendRequest(request: RequestOptions) {\n    request.headers.set('Authorization', this.context.headers.Authorization);\n  }\n  async getItem(path?: string) {\n    const res = await this.get(path ? path : '', undefined);\n    return res;\n  }\n}\nimport { BaseDataSource } from './BaseDataSource';\nexport class PythonRestApi extends BaseDataSource {\n  constructor() {\n    super();\n  }\n}\n\n\n\nData Sources are added to the Apollo Server Configuration and are passed into resolvers through the the context.\n\n\n\nimage"
  },
  {
    "objectID": "Spatial-Data-for-Mapping-and-Analysis.html",
    "href": "Spatial-Data-for-Mapping-and-Analysis.html",
    "title": "Spatial Data for Mapping and Analysis",
    "section": "",
    "text": "Spatial Data for Mapping and Analysis\nThere are two important spatial representations of Census geographies we work with at CORI/RISI.\n\nTIGER/Line\nThe TIGER/Line files are the most accurate spatial representations of Census geographies. These files should be used for any spatial data analysis. However, the polygons in these files are not suitable for mapping, as they occasionally overlap bodies of water and otherwise look strange when mapped.\n\n¬†¬†¬†¬†¬†¬†Schema: sch_census_tiger\n¬†¬†¬†¬†¬†¬†Source Table Format: source_tiger_{year}_{geography}\n¬†¬†¬†¬†¬†¬†Layer Table Format: tiger_{year}_{geography}\n¬†¬†¬†¬†¬†¬†Current Year: 2019\n\n\n\nCartographic Boundary Files\nAlso published by the Census, cartographic boundary files are significantly better for visual applications. However, cartographic boundary files are not suitable for spatial analysis and should never be used for such. These files should be applied as a final geometry before exporting to a mapping application.\n\n¬†¬†¬†¬†¬†¬†Schema: sch_census_tiger\n¬†¬†¬†¬†¬†¬†Source Table Format: source_cb_{year}_{geography}\n¬†¬†¬†¬†¬†¬†Current Year: 2019",
    "crumbs": [
      "Home",
      "Team Onboarding",
      "Spatial Data for Mapping and Analysis"
    ]
  },
  {
    "objectID": "parquet_bs.html",
    "href": "parquet_bs.html",
    "title": "CORI/RISI Ontology",
    "section": "",
    "text": "csv (R read into memory)\npmtiles ???\ndoes it need to download to your computer vs accessing directly to the cloud?\nparquet\nhandling permissions / credentials in R\n\nlisting\nreading\nwriting\n\ntargets?\nduckDB &lt;-&gt; s3\nback end: arrow vs duckdb vs polars?\n\n\n\n\nPins: can write parquet / arrow etc, dev. by the Posit team, I have not a clear view of their dependencies ..\narrow‚Äôs FileSystem classes or better help(\"FileSystem\", package = \"arrow\")\npaws package for the full suite of AWS services, wrapper of SDK. For s3 see here."
  },
  {
    "objectID": "parquet_bs.html#s3-storage",
    "href": "parquet_bs.html#s3-storage",
    "title": "CORI/RISI Ontology",
    "section": "",
    "text": "csv (R read into memory)\npmtiles ???\ndoes it need to download to your computer vs accessing directly to the cloud?\nparquet\nhandling permissions / credentials in R\n\nlisting\nreading\nwriting\n\ntargets?\nduckDB &lt;-&gt; s3\nback end: arrow vs duckdb vs polars?\n\n\n\n\nPins: can write parquet / arrow etc, dev. by the Posit team, I have not a clear view of their dependencies ..\narrow‚Äôs FileSystem classes or better help(\"FileSystem\", package = \"arrow\")\npaws package for the full suite of AWS services, wrapper of SDK. For s3 see here."
  },
  {
    "objectID": "parquet_bs.html#arrow-duckdb",
    "href": "parquet_bs.html#arrow-duckdb",
    "title": "CORI/RISI Ontology",
    "section": "arrow / duckDB",
    "text": "arrow / duckDB\n\nmaturity on OS\nmaturity in R\nstatus on spatial operations\nPG -&gt; parquet\nappending parquet?\n\n\nHow to partition a file:\nArrow support:\n\n‚Äúhive style‚Äù, a key value system: \"year=2019/month=1/file.parquet\"\n‚ÄúDirectory‚Äù: \"2019/01/file.parquet\"\n\nHadley Wickham suggest:\n\nAs a rough guide, arrow suggests that you avoid files smaller than 20MB and larger than 2GB and avoid partitions that produce more than 10,000 files. 1\n1¬†https://r4ds.hadley.nz/arrow.html#partitioning\n\n\n\n\n\n\nNote\n\n\n\nWhat could be a good partition scheme for FCC data?"
  },
  {
    "objectID": "parquet_bs.html#workflow-mind-map",
    "href": "parquet_bs.html#workflow-mind-map",
    "title": "CORI/RISI Ontology",
    "section": "Workflow ‚Äúmind map‚Äù",
    "text": "Workflow ‚Äúmind map‚Äù\n\n\nCode\nflowchart LR\ndplyr --&gt; arrow\narrow --&gt; filesystem[\"`**Filesystem**\n    multiple csv\n    parquet\n    more ..`\"]\n\narrow &lt;--&gt;|?| duckDB\n\ndplyr --&gt; dbplyr\ndplyr --&gt; DBI\ndbplyr --&gt; DBI \nDBI --&gt; duckDB\nduckDB --&gt; filesystem[\"`**Filesystem**\n    multiple csv\n    parquet\n    more ..`\"]\n\n\n\n\n\nflowchart LR\ndplyr --&gt; arrow\narrow --&gt; filesystem[\"`**Filesystem**\n    multiple csv\n    parquet\n    more ..`\"]\n\narrow &lt;--&gt;|?| duckDB\n\ndplyr --&gt; dbplyr\ndplyr --&gt; DBI\ndbplyr --&gt; DBI \nDBI --&gt; duckDB\nduckDB --&gt; filesystem[\"`**Filesystem**\n    multiple csv\n    parquet\n    more ..`\"]"
  },
  {
    "objectID": "parquet_bs.html#resources",
    "href": "parquet_bs.html#resources",
    "title": "CORI/RISI Ontology",
    "section": "Resources",
    "text": "Resources\n\nhttps://www.crunchydata.com/blog/parquet-and-postgres-in-the-data-lake\nhttps://mastodon.cloud/@milvus/112395302626488455\nhttps://grantmcdermott.com/duckdb-polars/\nhttps://duckdb.org/docs/\nhttps://r4ds.hadley.nz/arrow.html\non Arrow posts from Danielle: https://blog.djnavarro.net/posts/2022-12-26_strange-year/#writing-about-apache-arrow (usually awesome)"
  },
  {
    "objectID": "SAML-User-Pool-IdP-Authentication-Flow.html",
    "href": "SAML-User-Pool-IdP-Authentication-Flow.html",
    "title": "CORI/RISI Mapping & Data Analysis (MDA)",
    "section": "",
    "text": "The following has been extracted from AWS Docs (with added info for more clarity and context):\n\n\nSAML User Pool IdP Authentication Flow\n\nYou can integrate SAML-based IdPs directly from your user pool.\n\nThe app starts the sign-up and sign-in process by directing your user to the UI hosted by AWS. A mobile app can use web view to show the pages hosted by AWS.\nTypically, your user pool determines the IdP for your user from that user‚Äôs email address.\n\nAlternatively, if your app gathered information before directing the user to your user pool, it can provide that information to Amazon Cognito through a query parameter.\n\nYour user is redirected to the IdP.\nThe IdP authenticates the user if necessary. If the IdP recognizes that the user has an active session, the IdP skips the authentication to provide a single sign-in (SSO) experience.\nThe IdP POSTs the SAML assertion to the Amazon Cognito service.\n\n\nNote Amazon Cognito cancels authentication requests that do not complete within 5 minutes, and redirects the user to the hosted UI. The page displays a Something went wrong error message.\n\n\nAfter verifying the SAML assertion and collecting the user attributes (claims) from the assertion, Amazon Cognito internally creates or updates the user‚Äôs profile in the user pool. Amazon Cognito returns OIDC tokens to the app for the now signed-in user.\n\nThe following diagram shows the authentication flow for this process:\n\n\n\nimage"
  },
  {
    "objectID": "R-DBI-GDAL-PG-SF.html",
    "href": "R-DBI-GDAL-PG-SF.html",
    "title": "R DBI Geospatial Notes",
    "section": "",
    "text": "üèóÔ∏è This page is under construction! üèóÔ∏è",
    "crumbs": [
      "Home",
      "Infrastructure",
      "R DBI Geospatial Notes"
    ]
  },
  {
    "objectID": "PostgreSQL-RDS-Managment.html",
    "href": "PostgreSQL-RDS-Managment.html",
    "title": "PostgreSQL-RDS-Managment",
    "section": "",
    "text": "What kind of data is ‚Äúlegacy‚Äù data? what kind of data should be ‚Äúversionned‚Äù? here we need stable structure what can be candidate for that -&gt; fcc county? FCC annualized it (standardize?) -&gt; to what extend are we supporting this pipeline\nstoring archive in ‚Äúcheap format‚Äù that can be queried outside of PG (option are OLAP DB/filesystem)? -&gt; paws / targets\nMaybe do not use acs schema and just use it for/inside TED\ncaching -&gt; S3 / not team wide\nArchive -&gt; team wide",
    "crumbs": [
      "Home",
      "Infrastructure",
      "PostgreSQL-RDS-Managment"
    ]
  },
  {
    "objectID": "PostgreSQL-RDS-Managment.html#database-migration-update-2023-11-20",
    "href": "PostgreSQL-RDS-Managment.html#database-migration-update-2023-11-20",
    "title": "PostgreSQL-RDS-Managment",
    "section": "Database Migration Update 2023-11-20",
    "text": "Database Migration Update 2023-11-20\nhttps://docs.google.com/document/d/1nuYftMYqEKbH2i_wKhf63Kika7RtEoHb-A7NZYaSPwQ/edit\n\nGoal Overview:\nUse data from the single database instance in our old RDS cluster (cori-risi) to populate the following four new databases instances in our new RDS cluster (cori-risi-ad-postgresql):\n\napi-dev - Database used as a development environment with the main purpose of ‚Äúpromoting‚Äù data access to api-prod; serves as a test environment for the CORI Data API.\napi-prod - Database used to serve CORI Data API with data. Highest level of security with limited access. Products will or can be public facing.\ndata-prod - Database used to house vetted, documented, and production level datasets. Potential for very limited data sharing (with collaborators outside the org) with support.\ndata - Database used to support exploratory project work, research data, datasets under development.\n\nWe will do this in sequential steps, populating each new database instance one-at-a-time, with the database instance called ‚Äúdata‚Äù being the final one (equivalent to the original ‚Äúdata‚Äù instance in the old cori-risi RDS cluster). Some schemas will be database-specific from now on, while others (i.e., ‚Äúmetadata‚Äù) will be common to all instances.\n\n\nGlossary:\nDatabase: Collection of schemas (data domains), each with a specific set of data tables\nRDS: AWS service for hosting and administering networked database (DB) clusters\nRDS (cluster): Also used to reference a single cluster of databases\nRole: a generic specification that can do stuff in DB, (example: read_only) . For administration purposes some ‚Äúgroup‚Äù roles will be a collection of multiple users (example: bb_team )\nUser: a role that can login (example: Olivier)\n\n\nBasic activities plan:\n\n‚ÄúDumping‚Äù existing schemas in old RDS (pg_dump) (Oct 25, 2023~ Nov 1, 2023)\n\n\nSchemas intended for data-prod will be manually dumped/restored (by Olivier Nov 1, 2023)\n(MDA Team:) Should we restore these schemas? If yes, into which database?\n\nHistorical_census_data\n\nUsed in RWJF story 2\nNot actively developed\nWill not restore\n\nsch_layers\n\nCarto layers‚Ä¶\nVersion of higher ed drive times (last run?)\nWill not restore\n\nsch_source\n\nDumping ground during ETL\nPractice question: how should we deal with storing intermediate data in db?\n\nUtilize S3; document bucket/path in code README\n\nWill not restore; absence of schema will break some scripts\n\nvt_broadband\n\nOld VT 10 year work\nWill not restore\n\n\n\n\nCreating group roles (and clean a bit manually) for administrative management in new RDS (Oct 31, 2023)\nCreating new database instances in new RDS (Active Directory secured)\n\n\nPopulate them in this order:\n\napi-dev (Nov 1, 2023)\n-&gt; api-prod (Nov 2, 2023)\n-&gt; data-prod (Nov 6, 2023~ Nov 9, 2023)\n\nThis will use schemas manually dumped/restored (by Olivier)\nWe actually used ansible for both dumping schemas and restoring them to the individual database instances.\n\n-&gt; data (Nov 13, 2023)\n\nCheck-in AK and BB team about down-time (November 9)\nCheck-in Nora and RII team about down-time (November 9)\nNotify Org about app down-time (Nov 2, 2023)\nSet time with Nahum (MF) for API re-deploy/testing (Nov 2, 2023?)\nRe-deploy Broadband Climate Risk Mitigation Tool with downtime notification\n\n\nCreating users with role assignments to access the new database instances\n\n\nMaintaining a version-controlled YAML file as the source of truth on who can connect to what (and how)\n\ndb_password should be a shell env.\nWhat happens if users already exist and have set a password? (avoid idempotency; password reset -&gt; no_password_changes: is taking that into account )\n\n\n\n(Re-)Connecting ‚Äúclient‚Äù apps/tools to each new database instance (Nov 6, 2023-10)\n\n\nPrerequisite: updating cori.db (remember to increment the version)\n\nSet credential function (Nov 6, 2023)\nConnect to db functions (Nov 6, 2023)\n\nSpecify search path in params\nAdding one function to connect to data-prod (November 6)\n\n\nR processes/scripts (including DoctR =&gt; TED data pipeline)\nRe-deploy some of our shiny apps (TED, WIM, CORI Explorer) to shinyapps.io (November 9)\n\nRequired: uses .Renviron for db credentials\n\nRe-deploy API (November 2)\nRe-deploy BCAT and Broadband Risk Mitigation (CH) (TBD)\n\n\nTesting functionality (November 6; ongoing)\nOnboard BB team\n\n\nBB &lt;=&gt; MDA Database onboarding (2.0) (Nov 29, 2023)\n\n\nShut down old server RDS cluster (November 20)\nSharing (developing) best practices going forward (TBD)\n\n\nWriting/Updating documentation:\n\nArchitecture overview\ncori.db docmentation (README)\nData content/variables overview -\n\nCORI EXPLORER\nhttps://rpubs.com/drewrose/data_documentation\nhttps://ruralinnovation.github.io/data_documentation/\n\nAdding users\n\nSelf-managed password (procedures when pwd is forgotten/lost)?\n\nMoving data\n\nExamples:\n\nData set handling: data that is ‚Äúin-process/progress‚Äù stays in the data db instance. Metadata is essential and required for any data set that will be moved to the api-prod or data-prod db instances.\nQueries: Use less *, use alias (AS) and WHERE to help homogenize input in R or other scripting tools\n\n\n\n\nAppendix:\nSome examples of role inheritance: - Drew (User) &lt;- mda_team (‚Äúgroup‚Äù) &lt;- read_and_write (roles)\n\nKirstin (User) &lt;- bb_team(‚Äúgroup‚Äù) &lt;- read_only (roles)\nShinybot1 (User) &lt;- bot (‚Äúgroup‚Äù) &lt;- read_only (roles)\nCollaborator_bill (User) &lt;- group &lt;- read_only (roles)\n\n\n\nFollow-up:\n\nCreate architecture diagram\nPlan for communicating/socializing db best practices\nAWS costs (pull invoices):\n\nNovember 2023\nNovember 2022\nCompare change in RDS costs with Drew Rosebush",
    "crumbs": [
      "Home",
      "Infrastructure",
      "PostgreSQL-RDS-Managment"
    ]
  },
  {
    "objectID": "PostgreSQL-RDS-Managment.html#database-migration-2023-03-10",
    "href": "PostgreSQL-RDS-Managment.html#database-migration-2023-03-10",
    "title": "PostgreSQL-RDS-Managment",
    "section": "Database Migration 2023-03-10",
    "text": "Database Migration 2023-03-10",
    "crumbs": [
      "Home",
      "Infrastructure",
      "PostgreSQL-RDS-Managment"
    ]
  },
  {
    "objectID": "PostgreSQL-RDS-Managment.html#goals",
    "href": "PostgreSQL-RDS-Managment.html#goals",
    "title": "PostgreSQL-RDS-Managment",
    "section": "Goals:",
    "text": "Goals:\n\nReduce the size (50% ? we can always increase)\nUpgrade Postgres version (15)\nUpgrade PostGIS version (3.X)\nDefining Roles/Users\nSplit DB: yes 4 DB\nList remaining Challenges\n\nProcess to update data\nProcess to consolidate/clean/document previous data\nProcess to archive data\ndocumentations\nmerge branches / clean Ansible repo\nindividual password reset from ansible\n\nOnboard bb team -&gt; meeting 29/11\nDrop the previous instance (last dump?)\n\nThe list of Drew is a great start how can we improve it?\nI think it is outside of the scope of migrating DB but we should at least link the two and we should add a specific ticket/process on how we can build a better version of it.\nWhat is important is how long should we keep data for every source. If I am correct we are tracking the DB with a shiny app that target the metadata schema. In my opinion it is a good idea one question I am unclear is when do we update metadata: right after writing anything? after the data is in ‚Äúproduction‚Äù ? (other option). When we are storing past data we should think who do we clean metadata.\nThe ansible repo can be used has a documentation on how the schema were dump and restore:\n- **Schemas dumps**: https://github.com/ruralinnovation/ansible/blob/dce9174cbcd65bf6533a6a4516764d6760ea902c/playbooks/cori-risi-old-db/README.md   \n\n- **schema per databases** https://github.com/ruralinnovation/ansible/tree/dce9174cbcd65bf6533a6a4516764d6760ea902c/playbooks/cori-risi-ad-postgresql/vars   \n\nData lifecycle?\nCurrently, we just have one DB and multiple schemas.\nI will divide with at least:\n\nprojects\n\nschema for specific projects\n\nstaging place at schema level\ncan be called by bots\nno limitation in writing/reading\n\nsources\n\nmostly some raw data could be redundant with data\ncan‚Äôt be called by bots\nmore limited writing\n\ndata\n\nData that we need frequently and has been processed by us\nCan be called by bots?\nno limitation in writing/reading\n\n\nThis three seems to have different lifecycle (ie when we update them/store them/delete part of them)",
    "crumbs": [
      "Home",
      "Infrastructure",
      "PostgreSQL-RDS-Managment"
    ]
  },
  {
    "objectID": "PostgreSQL-RDS-Managment.html#ted",
    "href": "PostgreSQL-RDS-Managment.html#ted",
    "title": "PostgreSQL-RDS-Managment",
    "section": "TED",
    "text": "TED\nshinyapp: - entry point app.R - it is unsing cori.apps for component - it is using doctR and doctR needs fonts - renv.lock is used by shinyapps.io to download the correct packages and version - it needs core_data",
    "crumbs": [
      "Home",
      "Infrastructure",
      "PostgreSQL-RDS-Managment"
    ]
  },
  {
    "objectID": "Git-Workflows.html",
    "href": "Git-Workflows.html",
    "title": "Git!",
    "section": "",
    "text": "Git is a Distributed Version Control System (yup non distributed exist). Being ‚Äúdistributed‚Äù allows you to own nearly all the files.\nGitHub started has an hosting plateform for remote repository of Git. Now it is a social media, a forge, a CI/CD plateform and more. Github is an ‚Äúopinionated‚Äù version of Git.\nFew big difference between git and other VCS worth noting: git track change at the line level and it just add stuff.\nGit works with a ‚Äútree‚Äù structure hence you can keep that analogy in mind. You are always working on a ‚Äúbranch‚Äù (by default just one branch: main/master, trunck is an other commun name).\n\n\n\n\ngit clone &lt;link&gt;: Clone a GitHub repository. The link to clone a repo is available in the green Code dropdown on the main page. Use the SSH option.\n\n\n\nclone_link\n\n\n\ngit pull: Pull changes and files from GitHub into your local repository. By default (ie with a non modified setup) a git pull does a git fetch and a git merge (or attempt to).\n\ngit checkout -b &lt;new_branch&gt;: Allow you to create a new branch and go into it (git checkout allow you to navigate git graph)\n\ngit add (filename&gt;: Add new files to git\n\ngit commit: The expanded form is git commit -am 'commit message'. This tells Git to commit everything on the working tree (-a) and add a message (-m).\n\ngit status is telling you what is going one. git diff is more precise and usually integrated in your IDE.\n\ngit push: Push local changes and files up to GitHub\n\n\n\nYou will have a repository in one local computer(s) and one in GH. Sometimes I like having one in my GH space, one in CORI. The benefit is that I can mess up my GH space while not screwing the CORI‚Äôs one. The draw backs are that I need to be more careful how my git local is setup (ie what is define as my remotes and upstream).\nYou can also use git without having a remote repository.\nThe connection to git is done with SSH.\nIn git a file can be not tracked (see .gitignore/.gitkeep), unmodified, modified, staged. If a file is tracked (you asked git to keep track of it with git add [file]) then this file can be modified or not. The first time you add a file it is modified because git knows nothing about it. Then you can ‚Äústaged‚Äù it, if a file ‚Äúmodified‚Äù is ‚Äústaged‚Äù it goes to unmodified (until modification were done on it). A commit is creating a new ‚Äúnode‚Äù in your tree of files that are ‚Äústaged‚Äù.\n\n\n\nIt is good practices to add important changes in a new branch and send it for a PR (Pull Request):\n\nCreate a PR with your commits (push it to remote repo)\nGet it reviewed by someone on the team - in GH / their local repository.\n\nYou can inside of GitHub ask for a Reviewers, assign someone (Assignees) to do the merge (a Pull Request, is a merge inside of GitHub).\nOnce approved the PR you should not keep track of it inside of GitHub (the branch will still be inside of your local repo) and you should delete it. Github allow you to automatically do it: it is in the setting of the repo nearly at the bottom:\n\n\n\nScreenshot 2023-02-07 at 11 52 21 AM\n\n\n\n\n\n\nhappy git with R : https://happygitwithr.com/\ngit pro : https://git-scm.com/book/en/v2\nhttps://ohshitgit.com/ (NB most of it concerns git not github)\nA comics version: https://wizardzines.com/zines/oh-shit-git/ (a new zines should be available soon)\nMIT missing semester: https://missing.csail.mit.edu/2020/version-control/",
    "crumbs": [
      "Home",
      "Team Onboarding",
      "Git!"
    ]
  },
  {
    "objectID": "Git-Workflows.html#quick-definitions",
    "href": "Git-Workflows.html#quick-definitions",
    "title": "Git!",
    "section": "",
    "text": "Git is a Distributed Version Control System (yup non distributed exist). Being ‚Äúdistributed‚Äù allows you to own nearly all the files.\nGitHub started has an hosting plateform for remote repository of Git. Now it is a social media, a forge, a CI/CD plateform and more. Github is an ‚Äúopinionated‚Äù version of Git.\nFew big difference between git and other VCS worth noting: git track change at the line level and it just add stuff.\nGit works with a ‚Äútree‚Äù structure hence you can keep that analogy in mind. You are always working on a ‚Äúbranch‚Äù (by default just one branch: main/master, trunck is an other commun name).",
    "crumbs": [
      "Home",
      "Team Onboarding",
      "Git!"
    ]
  },
  {
    "objectID": "Git-Workflows.html#git-from-the-command-line-core-commands",
    "href": "Git-Workflows.html#git-from-the-command-line-core-commands",
    "title": "Git!",
    "section": "",
    "text": "git clone &lt;link&gt;: Clone a GitHub repository. The link to clone a repo is available in the green Code dropdown on the main page. Use the SSH option.\n\n\n\nclone_link\n\n\n\ngit pull: Pull changes and files from GitHub into your local repository. By default (ie with a non modified setup) a git pull does a git fetch and a git merge (or attempt to).\n\ngit checkout -b &lt;new_branch&gt;: Allow you to create a new branch and go into it (git checkout allow you to navigate git graph)\n\ngit add (filename&gt;: Add new files to git\n\ngit commit: The expanded form is git commit -am 'commit message'. This tells Git to commit everything on the working tree (-a) and add a message (-m).\n\ngit status is telling you what is going one. git diff is more precise and usually integrated in your IDE.\n\ngit push: Push local changes and files up to GitHub",
    "crumbs": [
      "Home",
      "Team Onboarding",
      "Git!"
    ]
  },
  {
    "objectID": "Git-Workflows.html#a-typical-gitgithub-set-up-workflow",
    "href": "Git-Workflows.html#a-typical-gitgithub-set-up-workflow",
    "title": "Git!",
    "section": "",
    "text": "You will have a repository in one local computer(s) and one in GH. Sometimes I like having one in my GH space, one in CORI. The benefit is that I can mess up my GH space while not screwing the CORI‚Äôs one. The draw backs are that I need to be more careful how my git local is setup (ie what is define as my remotes and upstream).\nYou can also use git without having a remote repository.\nThe connection to git is done with SSH.\nIn git a file can be not tracked (see .gitignore/.gitkeep), unmodified, modified, staged. If a file is tracked (you asked git to keep track of it with git add [file]) then this file can be modified or not. The first time you add a file it is modified because git knows nothing about it. Then you can ‚Äústaged‚Äù it, if a file ‚Äúmodified‚Äù is ‚Äústaged‚Äù it goes to unmodified (until modification were done on it). A commit is creating a new ‚Äúnode‚Äù in your tree of files that are ‚Äústaged‚Äù.",
    "crumbs": [
      "Home",
      "Team Onboarding",
      "Git!"
    ]
  },
  {
    "objectID": "Git-Workflows.html#github-workflow",
    "href": "Git-Workflows.html#github-workflow",
    "title": "Git!",
    "section": "",
    "text": "It is good practices to add important changes in a new branch and send it for a PR (Pull Request):\n\nCreate a PR with your commits (push it to remote repo)\nGet it reviewed by someone on the team - in GH / their local repository.\n\nYou can inside of GitHub ask for a Reviewers, assign someone (Assignees) to do the merge (a Pull Request, is a merge inside of GitHub).\nOnce approved the PR you should not keep track of it inside of GitHub (the branch will still be inside of your local repo) and you should delete it. Github allow you to automatically do it: it is in the setting of the repo nearly at the bottom:\n\n\n\nScreenshot 2023-02-07 at 11 52 21 AM",
    "crumbs": [
      "Home",
      "Team Onboarding",
      "Git!"
    ]
  },
  {
    "objectID": "Git-Workflows.html#ressources",
    "href": "Git-Workflows.html#ressources",
    "title": "Git!",
    "section": "",
    "text": "happy git with R : https://happygitwithr.com/\ngit pro : https://git-scm.com/book/en/v2\nhttps://ohshitgit.com/ (NB most of it concerns git not github)\nA comics version: https://wizardzines.com/zines/oh-shit-git/ (a new zines should be available soon)\nMIT missing semester: https://missing.csail.mit.edu/2020/version-control/",
    "crumbs": [
      "Home",
      "Team Onboarding",
      "Git!"
    ]
  },
  {
    "objectID": "Cori-Infrastructure.html",
    "href": "Cori-Infrastructure.html",
    "title": "CORI/RISI Mapping & Data Analysis (MDA)",
    "section": "",
    "text": "For CORI infrastructure see:\nhttps://github.com/orgs/ruralinnovation/projects/1/views/1\nhttps://github.com/ruralinnovation/cori-infrastructure"
  },
  {
    "objectID": "Data-Security.html",
    "href": "Data-Security.html",
    "title": "Data Security",
    "section": "",
    "text": "Introduction\nOur data security philosophy is pretty much the opposite of your standard privacy policy document. While our current Privacy Policy states,\n\nIn the event that any information under our control is compromised as a result of a breach of security, we will take commercially reasonable steps to investigate the situation and, where appropriate, notify those individuals whose information may have been compromised and take other steps, in accordance with any applicable laws and regulations. Please keep in mind that you are responsible for the security of any Personal Information in your control.\n\nby contrast, our data security obligations compel us, as a team, to accept responsibility for the transfer, receipt and usage of certain kinds of data. Even though the statements above claim to address ‚Äúany information under our control,‚Äù the MDA team‚Äôs data security measures supersede those statements and acknowledge that certain categories of data represent a major liability for our organization and are, in fact, our legal responsibility by way of contractual agreements asserting that we will protect that data against specific kinds of transfer or usage which are expressly or implicitly prohibited by the contract. The main purpose of a comprehensive Information Security practice is to ensure compliance with these prohibitions while also facilitating practical access and productive usage of the data for the benefit of our clients and users.\n\n\nData Access Points\nWith regard to protected and/or sensitive data, our information security considerations can be divided approximately into two high-level facets:\n\nEnvironments in which that data is stored and accessed.\nProcedures for receiving, transmitting and disposing of that data.\n\nWe process, store and access data within the following environments, as described with relevant security measures:\n\nAmazon Web Services (EC2 / RDS / S3):\n\nAWS is our primary cloud infrastructure platform\nAccess is controlled through IAM user accounts, using standard methods for authentication, authorization and delegation of platform privileges\nWe never use our AWS root user account to perform any changes to the system, so all changes can be tracked back to a specific user within our org\nSSH and SFTP access to our EC2 virtual machines is also regulated by individual user accounts, using encrypted private/public key pairs\nS3 access is restricted by IAM user accounts and bucket policies\n\nDatabase (PostgreSQL):\n\nThe ability to read and/or alter the contents of our database instances (on AWS RDS) is regulated by PostgreSQL roles; individual user roles are authorized to connect to the database by password protected login\nThese roles are defined and scoped by database administrators who are the only roles with the capability to create/alter database instances and grant/deny data access permissions to all other roles\nAccess to the database by database administrators is controlled by Active Directory and restricted to internal network connections facilitated by AWS VPN; AD & VPN together form an administrative security boundary\nRegular database user access also relies on whitelisting the user‚Äôs IP address within AWS Security Groups (firewall) for an additional layer of access control.\n\nLaptops/Workstations (Mac OS X + Bitfender + Uprise):\n\nAll members of our team work on some variation of MacBook Pro laptop, with its standard set of built-in security protocols and password protected login.\nAll laptops are equipped with Bitdefender Anitvirus software\nLocal and remote IT security services, as well as general support and maintenance of the organization‚Äôs laptops, workstations and local area networks are provided by Uprise, our external IT vendor.\n\n\nOur procedures for handling protected and/or sensitive data include the following:\n\nThe methods we use to transfer data from one authorized environment to another are restricted to:\n\nSSH/SFTP is used to transfer data to/from laptops and EC2 virtual machines\nS3 is used to transfer data to/from S3 buckets which can only be accessed by other authorized IAM users (including users external the organization, as authorized)\nPostgreSQL database connections (secured by password and SSL) are used to transfer data to/from our database instances\n\nS3 is our main staging area for receiving data assets in preparation for data ingestion\nProtected data assets and files are never checked into version control (git) and common data file type extensions are explicitly excluded by each project‚Äôs .gitignore file\nOn completion of a project or when deemed necessary any time beforehand, protected and/or sensitive data is removed from all database instances, S3 buckets and laptops/workstation hard drives\n\n\nPublic and/or Open Data\n\n‚Ä¶",
    "crumbs": [
      "Home",
      "Infrastructure",
      "Data Security"
    ]
  },
  {
    "objectID": "profile/accelerate.html",
    "href": "profile/accelerate.html",
    "title": "CORI/RISI Mapping & Data Analysis (MDA)",
    "section": "",
    "text": "The MDA team is at the forefront of an ongoing digital transformation at the Center on Rural Innovation. We are informed by and closely following the evidence-based team management and future-facing data architecture approaches outlined in the following books‚Ä¶\n\nAccelerate The Science of Lean Software and DevOps: Building and Scaling High Performing Technology Organizations Paperback by Nicole Forsgren PhD, Jez Humble, Gene Kim\n\n\nArchitecting Data and Machine Learning Platforms Enable Analytics and AI-Driven Innovation in the Cloud by Marco Tranquillin, Valliappa Lakshmanan, Firat Tekiner"
  },
  {
    "objectID": "BEAD-Changelog.html",
    "href": "BEAD-Changelog.html",
    "title": "BEAD Changelog",
    "section": "",
    "text": "Add county-level broadband stats (FCC) to the Broadband County Summary companion app\nAdd button to download broadband summary data for area selected on map\nAdd button to share URL for area selected on map\nFederal funding data is now sourced from https://fundingmap.fcc.gov/data-download/funding-data. The funding programs currently tracked on the rural broadband map include:\n\nBroadband Infrastructure Program\nCapital Projects Fund\nConnect America Fund Phase II\nCOMMUNITY CONNECT GRANT PROGRAM\nEnhanced Alternative Connect America Cost Model\nRDOF data (originally from https://www.fcc.gov/auction/904)\nRURAL ECONNECTIVITY PROGRAM\nState and Local Fiscal Recovery Fund\nTELEPHONE LOAN PROGRAM\nTribal Broadband Connectivity Program NOFO 1"
  },
  {
    "objectID": "BEAD-Changelog.html#updates-for-version-3",
    "href": "BEAD-Changelog.html#updates-for-version-3",
    "title": "BEAD Changelog",
    "section": "",
    "text": "Add county-level broadband stats (FCC) to the Broadband County Summary companion app\nAdd button to download broadband summary data for area selected on map\nAdd button to share URL for area selected on map\nFederal funding data is now sourced from https://fundingmap.fcc.gov/data-download/funding-data. The funding programs currently tracked on the rural broadband map include:\n\nBroadband Infrastructure Program\nCapital Projects Fund\nConnect America Fund Phase II\nCOMMUNITY CONNECT GRANT PROGRAM\nEnhanced Alternative Connect America Cost Model\nRDOF data (originally from https://www.fcc.gov/auction/904)\nRURAL ECONNECTIVITY PROGRAM\nState and Local Fiscal Recovery Fund\nTELEPHONE LOAN PROGRAM\nTribal Broadband Connectivity Program NOFO 1"
  },
  {
    "objectID": "BEAD-Changelog.html#updates-for-version-2",
    "href": "BEAD-Changelog.html#updates-for-version-2",
    "title": "BEAD Changelog",
    "section": "2024-03-27 Updates for version 2",
    "text": "2024-03-27 Updates for version 2\n\nCreate and deploy a Broadband County Summary companion app\nMove county-level ACS statistics the companion app\nImprove data fetching from CORI Data API (CDA) endpoints.\nSimplify display of summary stats for Census blocks, ISP technology, and Award info in the Detailed Info section"
  },
  {
    "objectID": "Apollo-Studio-Setup.html",
    "href": "Apollo-Studio-Setup.html",
    "title": "Create an Apollo Studio Account",
    "section": "",
    "text": "Getting started with Apollo Studio\n\nCreate an Apollo Studio Account\nYou can sign up for Studio with either your GitHub identity or a username and password.\n\nGo to studio.apollographql.com and click Let‚Äôs get started.\nComplete the signup flow, which includes:\nCreating an organization that you can invite teammates to\nSelecting a plan for your organization (the Free plan is always free, and the Team plan provides a free trial of paid features)\n\n\n\nIntegrate Existing Api\n\nNote: The Apollo Docs suggest you import/link an existing schema. We have found that using the explorer to dynamically access this information is the easiest way to get started.\n\n\nClick the Apollo Explorer Button on the top left of the screen.\n\n\n\n\nimage\n\n\n\nOn the top right of the screen is a sandbox url input, click the settings icon in this input.\n\n\n\n\nimage\n\n\n\n\n\nimage\n\n\n\nEnter in the Cloudfront GraphQL API URL and click save\n\n\n\n\nimage\n\n\n\nClick the headers tab on the bottom of the screen\n\n\n\n\nimage\n\n\n\nCopy the generated id token from your Postman Environment and enter it into this header field.\n\n\n\n\nimage\n\n\n\nApollo Studio will use introspection to access the schema of your API. If it has worked you should see your schema on the left side of the screen.\n\n\n\n\nimage\n\n\n\nPublish your schema by pressing the publish button to the right of the sandbox url input. Because this schema is connected to your DEV APi, using an appropriate naming convention.\n\n\n\n\nimage\n\n\n\n\n\nimage\n\n\n\n\nSetup Automated Authentication in a Preflight Script\n\nOnce your graph is published , you will br automatically navigated to that graphs screen.\n\n\n\n\nimage\n\n\n\nClick the Explorer navigation element.\n\n\n\n\nimage\n\n\n\nThere is a section Preflight script, click the add script button.\n\n\n\n\nimage\n\n\n\nAdd the following script:\n\nvar clientId = explorer.environment.get(\"cognitoClientId\");\nvar username = explorer.environment.get(\"cognitoUserName\");\nvar password = explorer.environment.get(\"cognitoUserPassword\");\ntry {\n       const response = await explorer.fetch(\"https://cognito-idp.us-east-1.amazonaws.com/\", {\n              method: 'POST',\n              headers: {\n                     'X-Amz-Target': 'AWSCognitoIdentityProviderService.InitiateAuth',\n                     'Content-Type': 'application/x-amz-json-1.1'\n              },\n              body: JSON.stringify({\n                     \"AuthParameters\": {\n                            \"USERNAME\": username,\n                            \"PASSWORD\": password\n                     },\n                     \"AuthFlow\": \"USER_PASSWORD_AUTH\",\n                     \"ClientId\": clientId\n              })\n       });\n\n       const body = JSON.parse(response.body);\n       explorer.environment.set(\"cognitoClientId\", clientId);\n       explorer.environment.set(\"cognitoUserName\", username);\n       explorer.environment.set(\"cognitoUserPassword\", password);\n       explorer.environment.set(\"cognitoIdToken\", body.AuthenticationResult.IdToken);\n}\ncatch (err) {\n       console.log('ERROR AUTHENTICATING', err);\n       explorer.environment.set(\"cognitoClientId\", clientId);\n       explorer.environment.set(\"cognitoUserName\", username);\n       explorer.environment.set(\"cognitoUserPassword\", password);\n}\n\nAdd an env variables json in the Environment variables section with values from your Postman/Cognito environment\n\n{\n  \"cognitoClientId\": \"6um99fv2qtb6f7ise3i037vna\",\n  \"cognitoUserName\": \"mf-int-test@yopmail.com\",\n  \"cognitoUserPassword\": \"PASSWORD\"\n}\n\n\n\nimage\n\n\n\nClick the Preflight script on\n\n\n\n\nimage\n\n\n\nUpdate the shared headers to reference the new id token variable.\n\n\n\n\nimage\n\n\n\n\nSend Requests\nSelect queries from your schema and add graphql variables. Then send requests!\nquery auction_904_subsidy_awards_geojson ($counties: [String]!, $skipCache: Boolean) {\n    auction_904_subsidy_awards_geojson (counties: $counties, skipCache: $skipCache) {\n        type\n        features {\n            type\n            id\n            properties\n            geometry {\n                coordinates\n                type\n            }\n        }\n    }\n}\n\n\n\nimage"
  },
  {
    "objectID": "Adding-New-Endpoints-&-Services-to-the-Python-RESTApi.html",
    "href": "Adding-New-Endpoints-&-Services-to-the-Python-RESTApi.html",
    "title": "Copy Python Scaffolding",
    "section": "",
    "text": "Copy Python Scaffolding\nWe have included a Scaffolding Lambda in the python-lambdas directory for easy setup.\n\n\n\nimage\n\n\n\nCopy this entire directory\nPaste it in the root of the python-lambdas directory\nrename the new directory the name of your new microservice (e.g.¬†bcat)\nrename the project in the pyproject.toml file\n\n\n\n\nimage\n\n\n\n\nUpdate your New Service\n\nYour New Service scaffold is the following:\nimport os\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.event_handler.api_gateway import APIGatewayRestResolver, Response\nfrom aws_lambda_powertools.event_handler.exceptions import BadRequestError\n\n\nlogger = Logger(service=\"Scaffolding\")\ntracer = Tracer(service=\"Scaffolding\")\napp = APIGatewayRestResolver(strip_prefixes=[\"/scaffolding\"])\n\n\n@app.get(rule=\"/bad-request-error\")\ndef bad_request_error(msg):\n    # HTTP  400\n    raise BadRequestError(msg)\n\n\n@app.get(\"/&lt;table&gt;/geojson\", compress=False)\ndef get(table):\n    print(table)\n\n# You can continue to use other utilities just as before\n@tracer.capture_lambda_handler\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.API_GATEWAY_REST, log_event=True)\ndef handler(event, context):\n    return app.resolve(event, context)\n\nIn the index.py file rename your service in:\n\n\nLogger(service=\"NEW_SERVICE_NAME\")\nTracer(service=\"NEW_SERVICE_NAME\")\n\n\nUpdate the ApiGatewayResolver strip_prefixes value with the corePath for this service.\n\n\n\nUnderstanding the Core Path\nThe core path is the path immediately following the Cloudfront URL and this path is how Api Gateway determines which lambda service to trigger when a request is made. The remaining params/path in the URL will be passed to the lambda to parse and act upon.\nExample: cloudfront_url.net/{corePath}/{customParam1}/customPath\n\nThe BCAT service the corePath is /bcat and all requests that start with this path will trigger the bcat microservice lambda in the corresponding directory (bcat). The lambda itself will parse the rest of the path and you can create custom endpoint methods to handle this within python (leveraging the Python Lambda Powertools library with similar patterns to Flask).\n\nWe suggest stripping the prefix (aka corePath) in the ApiGatewayResolver to reduce code smell in your custom lambda services. If you dont strip the prefix all your request methods in your lambda will have to include the corePath.\n\nInclude new request methods and logic.\n\n\n\n\nUpdate Microservices Config in the config/config.ts file.\nWe have created a configuration driven deployment for new services and endpoints. What this means is that you WILL NOT have to touch any of the Typescript/CDK code or constructs in order to deploy new service endpoints to the Python RestApi. This was always the goal, to ease the transition into using these new technologies while not inhibiting your ability to scale and add to the API.\nYour Deployment configuration accepts a microservicesConfig attribute which has the following type:\n\ninterface ServiceConfig {\n  /**\n   * The Logical Name of the service (NO SPACES) e.g. BCATService\n   */\n  logicalName: string;\n  /**\n   * The Core path to trigger the Microservice e.g. /bcat\n   */\n  corePath: string;\n  /**\n   * The name of the directory this service is located.  e.g. bcat\n   */\n  directoryName: string;\n}\n\n\nmicroservicesConfig: ServiceConfig[]\nAs of now we have a single BCAT Microservice so you configuration is the following:\nconst microservicesConfiguration: ServiceConfig[] = [\n  {\n    logicalName: 'BCATService',\n    corePath: '/bcat',\n    directoryName: 'bcat',\n  },\n];\n\nWhen you want to add a new service you add a new ServiceConfig to the list using the following attributes.\n\nlogicalName: Used by Cloudformation for the deployment (cannot have spaces)\ncorePath: As described above the core path is the path that when requested will trigger the corresponding lambda in the python-lambdas directory. This is how Api Gateway determines which lambda to call during requests.\ndirectoryName: This is the name of the directory that the lambda/service code is in. The Python Data Service construct needs to know where the lambda is in order build the code and link up the Api Gateway to the lambda.\n\nOnce you add a new service your config should resemble this:\n\nconst microservicesConfiguration: ServiceConfig[] = [\n  {\n    logicalName: 'BCATService',\n    corePath: '/bcat',\n    directoryName: 'bcat',\n  },\n  {\n    logicalName: 'CustomNewService',\n    corePath: '/custom-new-service',\n    directoryName: 'custom-new-service',\n  }\n];\n\n\nTest your code by running a local build:\nnpm run build\nIf there are no errors you can now:\n\ncommit your changes\npush to the DEV branch to trigger the pipeline to rebuild and deploy the changes."
  },
  {
    "objectID": "Postman-Workspace-Setup-and-Configuration.html",
    "href": "Postman-Workspace-Setup-and-Configuration.html",
    "title": "Postman Workspaces and Development Environments",
    "section": "",
    "text": "Open Postman Application\nClick the Workspaces tab.\nCreate a new Workspace (preferably a team workspace - although this will require a subscription after trial)\n\n\n\n\nimage\n\n\n\n\n\nimage\n\n\n\n\n\nimage\n\n\n\n\n\nWe have provided a complete configured collection to you as an json file.\n\n\nDownload Collection File\n\nClick Import file\nImport File from downloaded file.\nCheck that its a Collection Import and select import\n\n\n\n\nimage\n\n\n\n\n\nimage\n\n\n\n\nAll of your GraphQL and RestAPI requests are now configured for you.\n\n\n\nimage\n\n\n\n\n\n\nDownload Environment File\n\nClick Import file\nImport downloaded environment file.\nCheck that its a environment file and select import\n\n\n\n\nimage\n\n\n\n\n\nimage\n\n\n\n\nYour environment variables should be setup for your DEV deployment\n\n\n\nimage"
  },
  {
    "objectID": "Postman-Workspace-Setup-and-Configuration.html#create-a-workspace",
    "href": "Postman-Workspace-Setup-and-Configuration.html#create-a-workspace",
    "title": "Postman Workspaces and Development Environments",
    "section": "",
    "text": "Open Postman Application\nClick the Workspaces tab.\nCreate a new Workspace (preferably a team workspace - although this will require a subscription after trial)\n\n\n\n\nimage\n\n\n\n\n\nimage\n\n\n\n\n\nimage"
  },
  {
    "objectID": "Postman-Workspace-Setup-and-Configuration.html#import-collection",
    "href": "Postman-Workspace-Setup-and-Configuration.html#import-collection",
    "title": "Postman Workspaces and Development Environments",
    "section": "",
    "text": "We have provided a complete configured collection to you as an json file.\n\n\nDownload Collection File\n\nClick Import file\nImport File from downloaded file.\nCheck that its a Collection Import and select import\n\n\n\n\nimage\n\n\n\n\n\nimage\n\n\n\n\nAll of your GraphQL and RestAPI requests are now configured for you.\n\n\n\nimage\n\n\n\n\n\n\nDownload Environment File\n\nClick Import file\nImport downloaded environment file.\nCheck that its a environment file and select import\n\n\n\n\nimage\n\n\n\n\n\nimage\n\n\n\n\nYour environment variables should be setup for your DEV deployment\n\n\n\nimage"
  },
  {
    "objectID": "cori-dev.postman.collection.json.html",
    "href": "cori-dev.postman.collection.json.html",
    "title": "CORI/RISI Mapping & Data Analysis (MDA)",
    "section": "",
    "text": "{\n  \"info\": {\n    \"_postman_id\": \"331d1e58-7e7d-49bb-8a37-7afcbad025d9\",\n    \"name\": \"cori-data-api-dev\",\n    \"schema\": \"https://schema.getpostman.com/json/collection/v2.1.0/collection.json\",\n    \"_exporter_id\": \"13931905\"\n  },\n  \"item\": [\n    {\n      \"name\": \"bcat\",\n      \"item\": [\n        {\n          \"name\": \"auction_904_subsidy_awards\",\n          \"item\": [\n            {\n              \"name\": \"graphql\",\n              \"item\": [\n                {\n                  \"name\": \"auction_904_subsidy_awards_geojson\",\n                  \"event\": [\n                    {\n                      \"listen\": \"prerequest\",\n                      \"script\": {\n                        \"exec\": [\"\"],\n                        \"type\": \"text/javascript\"\n                      }\n                    }\n                  ],\n                  \"request\": {\n                    \"method\": \"POST\",\n                    \"header\": [],\n                    \"body\": {\n                      \"mode\": \"graphql\",\n                      \"graphql\": {\n                        \"query\": \"query ($counties: [String]!, $skipCache: Boolean) {\\n    auction_904_subsidy_awards_geojson (counties: $counties, skipCache: $skipCache) {\\n        type\\n        features {\\n            type\\n            id\\n            properties\\n            geometry {\\n                coordinates\\n                type\\n            }\\n        }\\n    }\\n}\",\n                        \"variables\": \"{\\n    \\\"counties\\\": [\\\"47167\\\", \\\"47017\\\"],\\n    \\\"skipCache\\\": true\\n}\"\n                      }\n                    },\n                    \"url\": {\n                      \"raw\": \"{{apolloApiUrl}}/graphql\",\n                      \"host\": [\"{{apolloApiUrl}}\"],\n                      \"path\": [\"graphql\"]\n                    }\n                  },\n                  \"response\": []\n                },\n                {\n                  \"name\": \"auction_904_subsidy_awards_county_geojson\",\n                  \"event\": [\n                    {\n                      \"listen\": \"prerequest\",\n                      \"script\": {\n                        \"exec\": [\"\"],\n                        \"type\": \"text/javascript\"\n                      }\n                    }\n                  ],\n                  \"request\": {\n                    \"method\": \"POST\",\n                    \"header\": [],\n                    \"body\": {\n                      \"mode\": \"graphql\",\n                      \"graphql\": {\n                        \"query\": \"query ($county: String!, $skipCache: Boolean) {\\n    auction_904_subsidy_awards_county_geojson (county: $county, skipCache: $skipCache) {\\n        type\\n        features {\\n            type\\n            id\\n            properties\\n            geometry {\\n                coordinates\\n                type\\n            }\\n        }\\n    }\\n}\",\n                        \"variables\": \"{\\n    \\\"county\\\": \\\"47167\\\",\\n    \\\"skipCache\\\": true\\n}\"\n                      }\n                    },\n                    \"url\": {\n                      \"raw\": \"{{apolloApiUrl}}/graphql\",\n                      \"host\": [\"{{apolloApiUrl}}\"],\n                      \"path\": [\"graphql\"]\n                    }\n                  },\n                  \"response\": []\n                }\n              ]\n            },\n            {\n              \"name\": \"restapi\",\n              \"item\": [\n                {\n                  \"name\": \"auction_904_subsidy_awards/geojson\",\n                  \"event\": [\n                    {\n                      \"listen\": \"prerequest\",\n                      \"script\": {\n                        \"exec\": [\"\"],\n                        \"type\": \"text/javascript\"\n                      }\n                    }\n                  ],\n                  \"request\": {\n                    \"method\": \"GET\",\n                    \"header\": [\n                      {\n                        \"key\": \"api\",\n                        \"value\": \"Xy4Gq51ixT3OYvZPqhb3D5OdemA3BYZ06ISg5Duo\",\n                        \"type\": \"text\"\n                      },\n                      {\n                        \"key\": \"Access\",\n                        \"value\": \"\",\n                        \"type\": \"text\"\n                      }\n                    ],\n                    \"url\": {\n                      \"raw\": \"{{pythonApiUrl}}/bcat/auction_904_subsidy_awards/geojson?geoid_co=47001,47003,47011\",\n                      \"host\": [\"{{pythonApiUrl}}\"],\n                      \"path\": [\"bcat\", \"auction_904_subsidy_awards\", \"geojson\"],\n                      \"query\": [\n                        {\n                          \"key\": \"geoid_co\",\n                          \"value\": \"47001,47003,47011\"\n                        }\n                      ]\n                    }\n                  },\n                  \"response\": []\n                },\n                {\n                  \"name\": \"auction_904_subsidy_awards/tiles\",\n                  \"event\": [\n                    {\n                      \"listen\": \"prerequest\",\n                      \"script\": {\n                        \"exec\": [\"\"],\n                        \"type\": \"text/javascript\"\n                      }\n                    }\n                  ],\n                  \"request\": {\n                    \"method\": \"GET\",\n                    \"header\": [\n                      {\n                        \"key\": \"api\",\n                        \"value\": \"Xy4Gq51ixT3OYvZPqhb3D5OdemA3BYZ06ISg5Duo\",\n                        \"type\": \"text\"\n                      },\n                      {\n                        \"key\": \"Access\",\n                        \"value\": \"\",\n                        \"type\": \"text\"\n                      }\n                    ],\n                    \"url\": {\n                      \"raw\": \"{{pythonApiUrl}}/bcat/auction_904_subsidy_awards/geojson?geoid_co=47001,47003,47011\",\n                      \"host\": [\"{{pythonApiUrl}}\"],\n                      \"path\": [\"bcat\", \"auction_904_subsidy_awards\", \"geojson\"],\n                      \"query\": [\n                        {\n                          \"key\": \"geoid_co\",\n                          \"value\": \"47001,47003,47011\"\n                        }\n                      ]\n                    }\n                  },\n                  \"response\": []\n                }\n              ]\n            },\n            {\n              \"name\": \"local\",\n              \"item\": [\n                {\n                  \"name\": \"auction_904_subsidy_awards/geojson\",\n                  \"event\": [\n                    {\n                      \"listen\": \"prerequest\",\n                      \"script\": {\n                        \"exec\": [\"\"],\n                        \"type\": \"text/javascript\"\n                      }\n                    }\n                  ],\n                  \"request\": {\n                    \"auth\": {\n                      \"type\": \"bearer\",\n                      \"bearer\": [\n                        {\n                          \"key\": \"token\",\n                          \"value\": \"eyJraWQiOiJJM3lHUGVjOTF2ejJ3RE1kRlk5YlJUMzJoSzZXc1UrMlJCN0lcL1JpNFJYdz0iLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiIwZmVkOTQxYS1hYmIxLTQ0MmEtYmUxYi1jYWNlNmZmM2ExYTkiLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZSwiaXNzIjoiaHR0cHM6XC9cL2NvZ25pdG8taWRwLnVzLWVhc3QtMS5hbWF6b25hd3MuY29tXC91cy1lYXN0LTFfN0RDZEQ0Q2dkIiwiY29nbml0bzp1c2VybmFtZSI6ImludC10ZXN0QHlvcG1haWwuY29tIiwib3JpZ2luX2p0aSI6ImQxNzdjYmQyLWJmZGUtNDE3OC1hNWQ1LTY2NjlmNjliODI0MSIsImF1ZCI6IjR1aW1zOTg0NGxmYjJlZmFjYjY4NGcyczZlIiwiZXZlbnRfaWQiOiIwNmNjMTgxYS01MjFiLTRmNDItYTUxYi1jYjM3OGI4YWRiNWYiLCJ0b2tlbl91c2UiOiJpZCIsImF1dGhfdGltZSI6MTY1NjcyMTE2OCwiZXhwIjoxNjU2NzI0NzY4LCJpYXQiOjE2NTY3MjExNjgsImp0aSI6IjllOGI0MzBmLWIxMmUtNGU4ZC05NjNkLTE4NWQ3NTA4NGVhNSIsImVtYWlsIjoiaW50LXRlc3RAeW9wbWFpbC5jb20ifQ.R8gG_OQsAZUheTLkph9_SU9d2EI-kpEaNzgNB-0xc5z00cHtsd4PPO4MCeZ3e2hGNLq6gy0pyQWEwUgsWv1ECNFh4G7APxZEGSUjnYHYo4nMDhi2Ss1M-p2HoDZueArrz5WMn-qiOhkfMWLhiELebAQ8PRWuE4rbP1g3v1l8ZJm18tenJkAxD2VpJyuXoCI6jnUBnhC_WYVxy6C1qhFzBASTOFD9xhnD8YFKMe5kPhw2n_ApgG35hTKxx1LpiEaXjJmqaxyIu1bA_84O0UjdUVgqUH_U_wDTgBks_fY8nvPql1nGRJvT1qcrB0vsysukN96hkETudbrGKRx16GN2p\",\n                          \"type\": \"string\"\n                        }\n                      ]\n                    },\n                    \"method\": \"GET\",\n                    \"header\": [\n                      {\n                        \"key\": \"api\",\n                        \"value\": \"Xy4Gq51ixT3OYvZPqhb3D5OdemA3BYZ06ISg5Duo\",\n                        \"type\": \"text\"\n                      },\n                      {\n                        \"key\": \"Access\",\n                        \"value\": \"\",\n                        \"type\": \"text\"\n                      }\n                    ],\n                    \"url\": {\n                      \"raw\": \"{{localApiUrl}}/bcat/auction_904_subsidy_awards/geojson?geoid_co=47001,47003\",\n                      \"host\": [\"{{localApiUrl}}\"],\n                      \"path\": [\"bcat\", \"auction_904_subsidy_awards\", \"geojson\"],\n                      \"query\": [\n                        {\n                          \"key\": \"geoid_co\",\n                          \"value\": \"47001,47003\"\n                        }\n                      ]\n                    }\n                  },\n                  \"response\": []\n                }\n              ]\n            }\n          ]\n        },\n        {\n          \"name\": \"broadband_unserved_blocks\",\n          \"item\": [\n            {\n              \"name\": \"graphql\",\n              \"item\": [\n                {\n                  \"name\": \"broadband_unserved_blocks_geojson\",\n                  \"event\": [\n                    {\n                      \"listen\": \"prerequest\",\n                      \"script\": {\n                        \"exec\": [\"\"],\n                        \"type\": \"text/javascript\"\n                      }\n                    }\n                  ],\n                  \"request\": {\n                    \"method\": \"POST\",\n                    \"header\": [],\n                    \"body\": {\n                      \"mode\": \"graphql\",\n                      \"graphql\": {\n                        \"query\": \"query ($counties: [String], $skipCache: Boolean) {\\n    broadband_unserved_blocks_geojson (counties: $counties, skipCache: $skipCache) {\\n        type\\n        features {\\n            type\\n            id\\n            properties\\n            geometry {\\n                coordinates\\n                type\\n            }\\n        }\\n    }\\n}\",\n                        \"variables\": \"{\\n    \\\"counties\\\": [\\\"47167\\\", \\\"47017\\\"],\\n    \\\"skipCache\\\": true\\n}\"\n                      }\n                    },\n                    \"url\": {\n                      \"raw\": \"{{apolloApiUrl}}/graphql\",\n                      \"host\": [\"{{apolloApiUrl}}\"],\n                      \"path\": [\"graphql\"]\n                    }\n                  },\n                  \"response\": []\n                },\n                {\n                  \"name\": \"broadband_unserved_blocks_county_geojson\",\n                  \"event\": [\n                    {\n                      \"listen\": \"prerequest\",\n                      \"script\": {\n                        \"exec\": [\"\"],\n                        \"type\": \"text/javascript\"\n                      }\n                    }\n                  ],\n                  \"request\": {\n                    \"method\": \"POST\",\n                    \"header\": [],\n                    \"body\": {\n                      \"mode\": \"graphql\",\n                      \"graphql\": {\n                        \"query\": \"query ($county: String!, $skipCache: Boolean) {\\n    broadband_unserved_blocks_county_geojson (county: $county, skipCache: $skipCache) {\\n        type\\n        features {\\n            type\\n            id\\n            properties\\n            geometry {\\n                coordinates\\n                type\\n            }\\n        }\\n    }\\n}\",\n                        \"variables\": \"{\\n    \\\"county\\\": \\\"47167\\\",\\n    \\\"skipCache\\\": true\\n}\"\n                      }\n                    },\n                    \"url\": {\n                      \"raw\": \"{{apolloApiUrl}}/graphql\",\n                      \"host\": [\"{{apolloApiUrl}}\"],\n                      \"path\": [\"graphql\"]\n                    }\n                  },\n                  \"response\": []\n                }\n              ]\n            },\n            {\n              \"name\": \"restapi\",\n              \"item\": [\n                {\n                  \"name\": \"broadband_unserved_blocks/geojson\",\n                  \"event\": [\n                    {\n                      \"listen\": \"prerequest\",\n                      \"script\": {\n                        \"exec\": [\"\"],\n                        \"type\": \"text/javascript\"\n                      }\n                    }\n                  ],\n                  \"request\": {\n                    \"method\": \"GET\",\n                    \"header\": [\n                      {\n                        \"key\": \"api\",\n                        \"value\": \"Xy4Gq51ixT3OYvZPqhb3D5OdemA3BYZ06ISg5Duo\",\n                        \"type\": \"text\"\n                      },\n                      {\n                        \"key\": \"Access\",\n                        \"value\": \"\",\n                        \"type\": \"text\"\n                      }\n                    ],\n                    \"url\": {\n                      \"raw\": \"{{pythonApiUrl}}/bcat/broadband_unserved_blocks/geojson?geoid_co=47001,47003\",\n                      \"host\": [\"{{pythonApiUrl}}\"],\n                      \"path\": [\"bcat\", \"broadband_unserved_blocks\", \"geojson\"],\n                      \"query\": [\n                        {\n                          \"key\": \"geoid_co\",\n                          \"value\": \"47001,47003\"\n                        }\n                      ]\n                    }\n                  },\n                  \"response\": []\n                }\n              ]\n            },\n            {\n              \"name\": \"local\",\n              \"item\": [\n                {\n                  \"name\": \"broadband_unserved_blocks/geojson\",\n                  \"event\": [\n                    {\n                      \"listen\": \"prerequest\",\n                      \"script\": {\n                        \"exec\": [\"\"],\n                        \"type\": \"text/javascript\"\n                      }\n                    }\n                  ],\n                  \"request\": {\n                    \"method\": \"GET\",\n                    \"header\": [\n                      {\n                        \"key\": \"api\",\n                        \"value\": \"Xy4Gq51ixT3OYvZPqhb3D5OdemA3BYZ06ISg5Duo\",\n                        \"type\": \"text\"\n                      },\n                      {\n                        \"key\": \"Access\",\n                        \"value\": \"\",\n                        \"type\": \"text\"\n                      }\n                    ],\n                    \"url\": {\n                      \"raw\": \"{{localApiUrl}}/bcat/broadband_unserved_blocks/geojson?geoid_co=47001,47003\",\n                      \"host\": [\"{{localApiUrl}}\"],\n                      \"path\": [\"bcat\", \"broadband_unserved_blocks\", \"geojson\"],\n                      \"query\": [\n                        {\n                          \"key\": \"geoid_co\",\n                          \"value\": \"47001,47003\"\n                        }\n                      ]\n                    }\n                  },\n                  \"response\": []\n                }\n              ]\n            }\n          ]\n        },\n        {\n          \"name\": \"county_broadband_farm_bill_eligibility\",\n          \"item\": [\n            {\n              \"name\": \"graphql\",\n              \"item\": [\n                {\n                  \"name\": \"county_broadband_farm_bill_eligibility_geojson\",\n                  \"event\": [\n                    {\n                      \"listen\": \"prerequest\",\n                      \"script\": {\n                        \"exec\": [\n                          \"var clientId = pm.environment.get(\\\"cognitoClientId\\\");\",\n                          \"var username = pm.environment.get(\\\"cognitoUserName\\\");\",\n                          \"var password = pm.environment.get(\\\"cognitoUserPassword\\\");\",\n                          \"console.log(username)\",\n                          \"console.log(password)\",\n                          \"pm.sendRequest({\",\n                          \"           url: \\\"https://cognito-idp.us-east-1.amazonaws.com/\\\",\",\n                          \"           method: 'POST',\",\n                          \"           header: {\",\n                          \"                    'X-Amz-Target':   'AWSCognitoIdentityProviderService.InitiateAuth',\",\n                          \"                    'Content-Type': 'application/x-amz-json-1.1'\",\n                          \"                   },\",\n                          \"            body: {\",\n                          \"                   mode: 'raw',\",\n                          \"                   raw: JSON.stringify({\",\n                          \"                   \\\"AuthParameters\\\": {\",\n                          \"                   \\\"USERNAME\\\": username,\",\n                          \"                   \\\"PASSWORD\\\": password\",\n                          \"                   },\",\n                          \"                  \\\"AuthFlow\\\": \\\"USER_PASSWORD_AUTH\\\",\",\n                          \"                  \\\"ClientId\\\": clientId\",\n                          \"  }),\",\n                          \"options: {\",\n                          \"raw: {\",\n                          \"language: 'json'\",\n                          \"}\",\n                          \"}\",\n                          \"}\",\n                          \"}, function (error, response) {\",\n                          \"console.log(response.json());\",\n                          \"pm.environment.set(\\\"cognitoAccessToken\\\", response.json().AuthenticationResult.AccessToken);\",\n                          \"pm.environment.set(\\\"cognitoIdToken\\\", response.json().AuthenticationResult.IdToken);\",\n                          \"});\"\n                        ],\n                        \"type\": \"text/javascript\"\n                      }\n                    }\n                  ],\n                  \"request\": {\n                    \"auth\": {\n                      \"type\": \"oauth2\",\n                      \"oauth2\": [\n                        {\n                          \"key\": \"tokenType\",\n                          \"value\": \"\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"accessToken\",\n                          \"value\": \"{{cognitoIdToken}}\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"redirect_uri\",\n                          \"value\": \"https://example.com/\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"authUrl\",\n                          \"value\": \"https://mf-data-api-dev.auth.us-east-1.amazoncognito.com\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"scope\",\n                          \"value\": \"phone email openid profile aws.cognito.signin.user.admin\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"useBrowser\",\n                          \"value\": false,\n                          \"type\": \"boolean\"\n                        },\n                        {\n                          \"key\": \"clientId\",\n                          \"value\": \"id5voqulhfbc55ko0n59h6ae4\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"grant_type\",\n                          \"value\": \"implicit\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"tokenName\",\n                          \"value\": \"MF Cori Api Cognito Token\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"addTokenTo\",\n                          \"value\": \"header\",\n                          \"type\": \"string\"\n                        }\n                      ]\n                    },\n                    \"method\": \"POST\",\n                    \"header\": [],\n                    \"body\": {\n                      \"mode\": \"graphql\",\n                      \"graphql\": {\n                        \"query\": \"query ($state_abbr: String!, $skipCache: Boolean) {\\n    county_broadband_farm_bill_eligibility_geojson (state_abbr: $state_abbr, skipCache: $skipCache) {\\n        type\\n        features {\\n            type\\n            id\\n            properties\\n            geometry {\\n                coordinates\\n                type\\n            }\\n        }\\n    }\\n}\",\n                        \"variables\": \"{\\n    \\\"state_abbr\\\": \\\"TN\\\",\\n    \\\"skipCache\\\": true\\n}\"\n                      }\n                    },\n                    \"url\": {\n                      \"raw\": \"{{apolloApiUrl}}/graphql\",\n                      \"host\": [\"{{apolloApiUrl}}\"],\n                      \"path\": [\"graphql\"]\n                    }\n                  },\n                  \"response\": []\n                }\n              ]\n            },\n            {\n              \"name\": \"restapi\",\n              \"item\": [\n                {\n                  \"name\": \"county_broadband_farm_bill_eligibility/geojson\",\n                  \"event\": [\n                    {\n                      \"listen\": \"prerequest\",\n                      \"script\": {\n                        \"exec\": [\"\"],\n                        \"type\": \"text/javascript\"\n                      }\n                    }\n                  ],\n                  \"request\": {\n                    \"method\": \"GET\",\n                    \"header\": [\n                      {\n                        \"key\": \"api\",\n                        \"value\": \"Xy4Gq51ixT3OYvZPqhb3D5OdemA3BYZ06ISg5Duo\",\n                        \"type\": \"text\"\n                      },\n                      {\n                        \"key\": \"Access\",\n                        \"value\": \"\",\n                        \"type\": \"text\"\n                      }\n                    ],\n                    \"url\": {\n                      \"raw\": \"{{pythonApiUrl}}/bcat/county_broadband_farm_bill_eligibility/geojson?state_abbr=TN\",\n                      \"host\": [\"{{pythonApiUrl}}\"],\n                      \"path\": [\n                        \"bcat\",\n                        \"county_broadband_farm_bill_eligibility\",\n                        \"geojson\"\n                      ],\n                      \"query\": [\n                        {\n                          \"key\": \"state_abbr\",\n                          \"value\": \"TN\"\n                        }\n                      ]\n                    }\n                  },\n                  \"response\": []\n                }\n              ]\n            },\n            {\n              \"name\": \"local\",\n              \"item\": [\n                {\n                  \"name\": \"county_broadband_farm_bill_eligibility/geojson\",\n                  \"event\": [\n                    {\n                      \"listen\": \"prerequest\",\n                      \"script\": {\n                        \"exec\": [\"\"],\n                        \"type\": \"text/javascript\"\n                      }\n                    }\n                  ],\n                  \"request\": {\n                    \"method\": \"GET\",\n                    \"header\": [\n                      {\n                        \"key\": \"api\",\n                        \"value\": \"Xy4Gq51ixT3OYvZPqhb3D5OdemA3BYZ06ISg5Duo\",\n                        \"type\": \"text\"\n                      },\n                      {\n                        \"key\": \"Access\",\n                        \"value\": \"\",\n                        \"type\": \"text\"\n                      }\n                    ],\n                    \"url\": {\n                      \"raw\": \"{{localApiUrl}}/bcat/county_broadband_farm_bill_eligibility/geojson?state_abbr=TN\",\n                      \"host\": [\"{{localApiUrl}}\"],\n                      \"path\": [\n                        \"bcat\",\n                        \"county_broadband_farm_bill_eligibility\",\n                        \"geojson\"\n                      ],\n                      \"query\": [\n                        {\n                          \"key\": \"state_abbr\",\n                          \"value\": \"TN\"\n                        }\n                      ]\n                    }\n                  },\n                  \"response\": []\n                }\n              ]\n            }\n          ]\n        },\n        {\n          \"name\": \"county_broadband_pending_rural_dev\",\n          \"item\": [\n            {\n              \"name\": \"graphql\",\n              \"item\": [\n                {\n                  \"name\": \"county_broadband_pending_rural_dev_geojson\",\n                  \"event\": [\n                    {\n                      \"listen\": \"prerequest\",\n                      \"script\": {\n                        \"exec\": [\n                          \"var clientId = pm.environment.get(\\\"cognitoClientId\\\");\",\n                          \"var username = pm.environment.get(\\\"cognitoUserName\\\");\",\n                          \"var password = pm.environment.get(\\\"cognitoUserPassword\\\");\",\n                          \"console.log(username)\",\n                          \"console.log(password)\",\n                          \"pm.sendRequest({\",\n                          \"           url: \\\"https://cognito-idp.us-east-1.amazonaws.com/\\\",\",\n                          \"           method: 'POST',\",\n                          \"           header: {\",\n                          \"                    'X-Amz-Target':   'AWSCognitoIdentityProviderService.InitiateAuth',\",\n                          \"                    'Content-Type': 'application/x-amz-json-1.1'\",\n                          \"                   },\",\n                          \"            body: {\",\n                          \"                   mode: 'raw',\",\n                          \"                   raw: JSON.stringify({\",\n                          \"                   \\\"AuthParameters\\\": {\",\n                          \"                   \\\"USERNAME\\\": username,\",\n                          \"                   \\\"PASSWORD\\\": password\",\n                          \"                   },\",\n                          \"                  \\\"AuthFlow\\\": \\\"USER_PASSWORD_AUTH\\\",\",\n                          \"                  \\\"ClientId\\\": clientId\",\n                          \"  }),\",\n                          \"options: {\",\n                          \"raw: {\",\n                          \"language: 'json'\",\n                          \"}\",\n                          \"}\",\n                          \"}\",\n                          \"}, function (error, response) {\",\n                          \"console.log(response.json());\",\n                          \"pm.environment.set(\\\"cognitoAccessToken\\\", response.json().AuthenticationResult.AccessToken);\",\n                          \"pm.environment.set(\\\"cognitoIdToken\\\", response.json().AuthenticationResult.IdToken);\",\n                          \"});\"\n                        ],\n                        \"type\": \"text/javascript\"\n                      }\n                    }\n                  ],\n                  \"request\": {\n                    \"auth\": {\n                      \"type\": \"oauth2\",\n                      \"oauth2\": [\n                        {\n                          \"key\": \"tokenType\",\n                          \"value\": \"\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"accessToken\",\n                          \"value\": \"{{cognitoIdToken}}\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"redirect_uri\",\n                          \"value\": \"https://example.com/\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"authUrl\",\n                          \"value\": \"https://mf-data-api-dev.auth.us-east-1.amazoncognito.com\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"scope\",\n                          \"value\": \"phone email openid profile aws.cognito.signin.user.admin\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"useBrowser\",\n                          \"value\": false,\n                          \"type\": \"boolean\"\n                        },\n                        {\n                          \"key\": \"clientId\",\n                          \"value\": \"id5voqulhfbc55ko0n59h6ae4\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"grant_type\",\n                          \"value\": \"implicit\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"tokenName\",\n                          \"value\": \"MF Cori Api Cognito Token\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"addTokenTo\",\n                          \"value\": \"header\",\n                          \"type\": \"string\"\n                        }\n                      ]\n                    },\n                    \"method\": \"POST\",\n                    \"header\": [],\n                    \"body\": {\n                      \"mode\": \"graphql\",\n                      \"graphql\": {\n                        \"query\": \"query ($state_abbr: String!, $skipCache: Boolean) {\\n    county_broadband_pending_rural_dev_geojson (state_abbr: $state_abbr, skipCache: $skipCache) {\\n        type\\n        features {\\n            type\\n            id\\n            properties\\n            geometry {\\n                coordinates\\n                type\\n            }\\n        }\\n    }\\n}\",\n                        \"variables\": \"{\\n    \\\"state_abbr\\\": \\\"TN\\\",\\n    \\\"skipCache\\\": true\\n}\"\n                      }\n                    },\n                    \"url\": {\n                      \"raw\": \"{{apolloApiUrl}}/graphql\",\n                      \"host\": [\"{{apolloApiUrl}}\"],\n                      \"path\": [\"graphql\"]\n                    }\n                  },\n                  \"response\": []\n                }\n              ]\n            },\n            {\n              \"name\": \"restapi\",\n              \"item\": [\n                {\n                  \"name\": \"county_broadband_pending_rural_dev/geojson\",\n                  \"event\": [\n                    {\n                      \"listen\": \"prerequest\",\n                      \"script\": {\n                        \"exec\": [\"\"],\n                        \"type\": \"text/javascript\"\n                      }\n                    }\n                  ],\n                  \"request\": {\n                    \"method\": \"GET\",\n                    \"header\": [\n                      {\n                        \"key\": \"api\",\n                        \"value\": \"Xy4Gq51ixT3OYvZPqhb3D5OdemA3BYZ06ISg5Duo\",\n                        \"type\": \"text\"\n                      },\n                      {\n                        \"key\": \"Access\",\n                        \"value\": \"\",\n                        \"type\": \"text\"\n                      }\n                    ],\n                    \"url\": {\n                      \"raw\": \"{{pythonApiUrl}}/bcat/county_broadband_pending_rural_dev/geojson?state_abbr=TN\",\n                      \"host\": [\"{{pythonApiUrl}}\"],\n                      \"path\": [\n                        \"bcat\",\n                        \"county_broadband_pending_rural_dev\",\n                        \"geojson\"\n                      ],\n                      \"query\": [\n                        {\n                          \"key\": \"state_abbr\",\n                          \"value\": \"TN\"\n                        }\n                      ]\n                    }\n                  },\n                  \"response\": []\n                }\n              ]\n            },\n            {\n              \"name\": \"local\",\n              \"item\": [\n                {\n                  \"name\": \"county_broadband_pending_rural_dev/geojson\",\n                  \"event\": [\n                    {\n                      \"listen\": \"prerequest\",\n                      \"script\": {\n                        \"exec\": [\"\"],\n                        \"type\": \"text/javascript\"\n                      }\n                    }\n                  ],\n                  \"request\": {\n                    \"method\": \"GET\",\n                    \"header\": [\n                      {\n                        \"key\": \"api\",\n                        \"value\": \"Xy4Gq51ixT3OYvZPqhb3D5OdemA3BYZ06ISg5Duo\",\n                        \"type\": \"text\"\n                      },\n                      {\n                        \"key\": \"Access\",\n                        \"value\": \"\",\n                        \"type\": \"text\"\n                      }\n                    ],\n                    \"url\": {\n                      \"raw\": \"{{localApiUrl}}/bcat/county_broadband_pending_rural_dev/geojson?state_abbr=TN\",\n                      \"host\": [\"{{localApiUrl}}\"],\n                      \"path\": [\n                        \"bcat\",\n                        \"county_broadband_pending_rural_dev\",\n                        \"geojson\"\n                      ],\n                      \"query\": [\n                        {\n                          \"key\": \"state_abbr\",\n                          \"value\": \"TN\"\n                        }\n                      ]\n                    }\n                  },\n                  \"response\": []\n                }\n              ]\n            }\n          ]\n        },\n        {\n          \"name\": \"county_ilecs_geo\",\n          \"item\": [\n            {\n              \"name\": \"graphql\",\n              \"item\": [\n                {\n                  \"name\": \"county_ilecs_geo_geojson\",\n                  \"event\": [\n                    {\n                      \"listen\": \"prerequest\",\n                      \"script\": {\n                        \"exec\": [\n                          \"var clientId = pm.environment.get(\\\"cognitoClientId\\\");\",\n                          \"var username = pm.environment.get(\\\"cognitoUserName\\\");\",\n                          \"var password = pm.environment.get(\\\"cognitoUserPassword\\\");\",\n                          \"console.log(username)\",\n                          \"console.log(password)\",\n                          \"pm.sendRequest({\",\n                          \"           url: \\\"https://cognito-idp.us-east-1.amazonaws.com/\\\",\",\n                          \"           method: 'POST',\",\n                          \"           header: {\",\n                          \"                    'X-Amz-Target':   'AWSCognitoIdentityProviderService.InitiateAuth',\",\n                          \"                    'Content-Type': 'application/x-amz-json-1.1'\",\n                          \"                   },\",\n                          \"            body: {\",\n                          \"                   mode: 'raw',\",\n                          \"                   raw: JSON.stringify({\",\n                          \"                   \\\"AuthParameters\\\": {\",\n                          \"                   \\\"USERNAME\\\": username,\",\n                          \"                   \\\"PASSWORD\\\": password\",\n                          \"                   },\",\n                          \"                  \\\"AuthFlow\\\": \\\"USER_PASSWORD_AUTH\\\",\",\n                          \"                  \\\"ClientId\\\": clientId\",\n                          \"  }),\",\n                          \"options: {\",\n                          \"raw: {\",\n                          \"language: 'json'\",\n                          \"}\",\n                          \"}\",\n                          \"}\",\n                          \"}, function (error, response) {\",\n                          \"console.log(response.json());\",\n                          \"pm.environment.set(\\\"cognitoAccessToken\\\", response.json().AuthenticationResult.AccessToken);\",\n                          \"pm.environment.set(\\\"cognitoIdToken\\\", response.json().AuthenticationResult.IdToken);\",\n                          \"});\"\n                        ],\n                        \"type\": \"text/javascript\"\n                      }\n                    }\n                  ],\n                  \"request\": {\n                    \"auth\": {\n                      \"type\": \"oauth2\",\n                      \"oauth2\": [\n                        {\n                          \"key\": \"tokenType\",\n                          \"value\": \"\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"accessToken\",\n                          \"value\": \"{{cognitoIdToken}}\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"redirect_uri\",\n                          \"value\": \"https://example.com/\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"authUrl\",\n                          \"value\": \"https://mf-data-api-dev.auth.us-east-1.amazoncognito.com\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"scope\",\n                          \"value\": \"phone email openid profile aws.cognito.signin.user.admin\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"useBrowser\",\n                          \"value\": false,\n                          \"type\": \"boolean\"\n                        },\n                        {\n                          \"key\": \"clientId\",\n                          \"value\": \"id5voqulhfbc55ko0n59h6ae4\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"grant_type\",\n                          \"value\": \"implicit\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"tokenName\",\n                          \"value\": \"MF Cori Api Cognito Token\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"addTokenTo\",\n                          \"value\": \"header\",\n                          \"type\": \"string\"\n                        }\n                      ]\n                    },\n                    \"method\": \"POST\",\n                    \"header\": [],\n                    \"body\": {\n                      \"mode\": \"graphql\",\n                      \"graphql\": {\n                        \"query\": \"query ($state_abbr: String!, $skipCache: Boolean) {\\n    county_ilecs_geo_geojson (state_abbr: $state_abbr, skipCache: $skipCache) {\\n        type\\n        features {\\n            type\\n            id\\n            properties\\n            geometry {\\n                coordinates\\n                type\\n            }\\n        }\\n    }\\n}\",\n                        \"variables\": \"{\\n    \\\"state_abbr\\\": \\\"TN\\\",\\n    \\\"skipCache\\\": true\\n}\"\n                      }\n                    },\n                    \"url\": {\n                      \"raw\": \"{{apolloApiUrl}}/graphql\",\n                      \"host\": [\"{{apolloApiUrl}}\"],\n                      \"path\": [\"graphql\"]\n                    }\n                  },\n                  \"response\": []\n                }\n              ]\n            },\n            {\n              \"name\": \"restapi\",\n              \"item\": [\n                {\n                  \"name\": \"county_ilecs_geo/geojson\",\n                  \"event\": [\n                    {\n                      \"listen\": \"prerequest\",\n                      \"script\": {\n                        \"exec\": [\"\"],\n                        \"type\": \"text/javascript\"\n                      }\n                    }\n                  ],\n                  \"request\": {\n                    \"method\": \"GET\",\n                    \"header\": [\n                      {\n                        \"key\": \"api\",\n                        \"value\": \"Xy4Gq51ixT3OYvZPqhb3D5OdemA3BYZ06ISg5Duo\",\n                        \"type\": \"text\"\n                      },\n                      {\n                        \"key\": \"Access\",\n                        \"value\": \"\",\n                        \"type\": \"text\"\n                      }\n                    ],\n                    \"url\": {\n                      \"raw\": \"{{pythonApiUrl}}/bcat/county_ilecs_geo/geojson?state_abbr=TN\",\n                      \"host\": [\"{{pythonApiUrl}}\"],\n                      \"path\": [\"bcat\", \"county_ilecs_geo\", \"geojson\"],\n                      \"query\": [\n                        {\n                          \"key\": \"state_abbr\",\n                          \"value\": \"TN\"\n                        }\n                      ]\n                    }\n                  },\n                  \"response\": []\n                }\n              ]\n            },\n            {\n              \"name\": \"local\",\n              \"item\": [\n                {\n                  \"name\": \"county_ilecs_geo/geojson\",\n                  \"event\": [\n                    {\n                      \"listen\": \"prerequest\",\n                      \"script\": {\n                        \"exec\": [\"\"],\n                        \"type\": \"text/javascript\"\n                      }\n                    }\n                  ],\n                  \"request\": {\n                    \"method\": \"GET\",\n                    \"header\": [\n                      {\n                        \"key\": \"api\",\n                        \"value\": \"Xy4Gq51ixT3OYvZPqhb3D5OdemA3BYZ06ISg5Duo\",\n                        \"type\": \"text\"\n                      },\n                      {\n                        \"key\": \"Access\",\n                        \"value\": \"\",\n                        \"type\": \"text\"\n                      }\n                    ],\n                    \"url\": {\n                      \"raw\": \"{{localApiUrl}}/bcat/county_ilecs_geo/geojson?state_abbr=TN\",\n                      \"host\": [\"{{localApiUrl}}\"],\n                      \"path\": [\"bcat\", \"county_ilecs_geo\", \"geojson\"],\n                      \"query\": [\n                        {\n                          \"key\": \"state_abbr\",\n                          \"value\": \"TN\"\n                        }\n                      ]\n                    }\n                  },\n                  \"response\": []\n                }\n              ]\n            }\n          ]\n        },\n        {\n          \"name\": \"county_rural_dev_broadband_protected_borrowers\",\n          \"item\": [\n            {\n              \"name\": \"graphql\",\n              \"item\": [\n                {\n                  \"name\": \"county_rural_dev_broadband_protected_borrowers_geojson\",\n                  \"event\": [\n                    {\n                      \"listen\": \"prerequest\",\n                      \"script\": {\n                        \"exec\": [\n                          \"var clientId = pm.environment.get(\\\"cognitoClientId\\\");\",\n                          \"var username = pm.environment.get(\\\"cognitoUserName\\\");\",\n                          \"var password = pm.environment.get(\\\"cognitoUserPassword\\\");\",\n                          \"console.log(username)\",\n                          \"console.log(password)\",\n                          \"pm.sendRequest({\",\n                          \"           url: \\\"https://cognito-idp.us-east-1.amazonaws.com/\\\",\",\n                          \"           method: 'POST',\",\n                          \"           header: {\",\n                          \"                    'X-Amz-Target':   'AWSCognitoIdentityProviderService.InitiateAuth',\",\n                          \"                    'Content-Type': 'application/x-amz-json-1.1'\",\n                          \"                   },\",\n                          \"            body: {\",\n                          \"                   mode: 'raw',\",\n                          \"                   raw: JSON.stringify({\",\n                          \"                   \\\"AuthParameters\\\": {\",\n                          \"                   \\\"USERNAME\\\": username,\",\n                          \"                   \\\"PASSWORD\\\": password\",\n                          \"                   },\",\n                          \"                  \\\"AuthFlow\\\": \\\"USER_PASSWORD_AUTH\\\",\",\n                          \"                  \\\"ClientId\\\": clientId\",\n                          \"  }),\",\n                          \"options: {\",\n                          \"raw: {\",\n                          \"language: 'json'\",\n                          \"}\",\n                          \"}\",\n                          \"}\",\n                          \"}, function (error, response) {\",\n                          \"console.log(response.json());\",\n                          \"pm.environment.set(\\\"cognitoAccessToken\\\", response.json().AuthenticationResult.AccessToken);\",\n                          \"pm.environment.set(\\\"cognitoIdToken\\\", response.json().AuthenticationResult.IdToken);\",\n                          \"});\"\n                        ],\n                        \"type\": \"text/javascript\"\n                      }\n                    }\n                  ],\n                  \"request\": {\n                    \"auth\": {\n                      \"type\": \"oauth2\",\n                      \"oauth2\": [\n                        {\n                          \"key\": \"tokenType\",\n                          \"value\": \"\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"accessToken\",\n                          \"value\": \"{{cognitoIdToken}}\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"redirect_uri\",\n                          \"value\": \"https://example.com/\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"authUrl\",\n                          \"value\": \"https://mf-data-api-dev.auth.us-east-1.amazoncognito.com\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"scope\",\n                          \"value\": \"phone email openid profile aws.cognito.signin.user.admin\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"useBrowser\",\n                          \"value\": false,\n                          \"type\": \"boolean\"\n                        },\n                        {\n                          \"key\": \"clientId\",\n                          \"value\": \"id5voqulhfbc55ko0n59h6ae4\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"grant_type\",\n                          \"value\": \"implicit\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"tokenName\",\n                          \"value\": \"MF Cori Api Cognito Token\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"addTokenTo\",\n                          \"value\": \"header\",\n                          \"type\": \"string\"\n                        }\n                      ]\n                    },\n                    \"method\": \"POST\",\n                    \"header\": [],\n                    \"body\": {\n                      \"mode\": \"graphql\",\n                      \"graphql\": {\n                        \"query\": \"query ($state_abbr: String!, $skipCache: Boolean) {\\n    county_ilecs_geo_geojson (state_abbr: $state_abbr, skipCache: $skipCache) {\\n        type\\n        features {\\n            type\\n            id\\n            properties\\n            geometry {\\n                coordinates\\n                type\\n            }\\n        }\\n    }\\n}\",\n                        \"variables\": \"{\\n    \\\"state_abbr\\\": \\\"TN\\\",\\n    \\\"skipCache\\\": true\\n}\"\n                      }\n                    },\n                    \"url\": {\n                      \"raw\": \"{{apolloApiUrl}}/graphql\",\n                      \"host\": [\"{{apolloApiUrl}}\"],\n                      \"path\": [\"graphql\"]\n                    }\n                  },\n                  \"response\": []\n                }\n              ]\n            },\n            {\n              \"name\": \"restapi\",\n              \"item\": [\n                {\n                  \"name\": \"county_rural_dev_broadband_protected_borrowers/geojson\",\n                  \"event\": [\n                    {\n                      \"listen\": \"prerequest\",\n                      \"script\": {\n                        \"exec\": [\"\"],\n                        \"type\": \"text/javascript\"\n                      }\n                    }\n                  ],\n                  \"request\": {\n                    \"method\": \"GET\",\n                    \"header\": [\n                      {\n                        \"key\": \"api\",\n                        \"value\": \"Xy4Gq51ixT3OYvZPqhb3D5OdemA3BYZ06ISg5Duo\",\n                        \"type\": \"text\"\n                      },\n                      {\n                        \"key\": \"Access\",\n                        \"value\": \"\",\n                        \"type\": \"text\"\n                      }\n                    ],\n                    \"url\": {\n                      \"raw\": \"{{pythonApiUrl}}/bcat/county_rural_dev_broadband_protected_borrowers/geojson?stusps=TN\",\n                      \"host\": [\"{{pythonApiUrl}}\"],\n                      \"path\": [\n                        \"bcat\",\n                        \"county_rural_dev_broadband_protected_borrowers\",\n                        \"geojson\"\n                      ],\n                      \"query\": [\n                        {\n                          \"key\": \"stusps\",\n                          \"value\": \"TN\"\n                        }\n                      ]\n                    }\n                  },\n                  \"response\": []\n                }\n              ]\n            },\n            {\n              \"name\": \"local\",\n              \"item\": [\n                {\n                  \"name\": \"county_rural_dev_broadband_protected_borrowers/geojson\",\n                  \"event\": [\n                    {\n                      \"listen\": \"prerequest\",\n                      \"script\": {\n                        \"exec\": [\"\"],\n                        \"type\": \"text/javascript\"\n                      }\n                    }\n                  ],\n                  \"request\": {\n                    \"method\": \"GET\",\n                    \"header\": [\n                      {\n                        \"key\": \"api\",\n                        \"value\": \"Xy4Gq51ixT3OYvZPqhb3D5OdemA3BYZ06ISg5Duo\",\n                        \"type\": \"text\"\n                      },\n                      {\n                        \"key\": \"Access\",\n                        \"value\": \"\",\n                        \"type\": \"text\"\n                      }\n                    ],\n                    \"url\": {\n                      \"raw\": \"{{localApiUrl}}/bcat/county_rural_dev_broadband_protected_borrowers/geojson?stusps=TN\",\n                      \"host\": [\"{{localApiUrl}}\"],\n                      \"path\": [\n                        \"bcat\",\n                        \"county_rural_dev_broadband_protected_borrowers\",\n                        \"geojson\"\n                      ],\n                      \"query\": [\n                        {\n                          \"key\": \"stusps\",\n                          \"value\": \"TN\"\n                        }\n                      ]\n                    }\n                  },\n                  \"response\": []\n                }\n              ]\n            }\n          ]\n        },\n        {\n          \"name\": \"county_summary\",\n          \"item\": [\n            {\n              \"name\": \"graphql\",\n              \"item\": [\n                {\n                  \"name\": \"county_summary_geojson\",\n                  \"event\": [\n                    {\n                      \"listen\": \"prerequest\",\n                      \"script\": {\n                        \"exec\": [\"\"],\n                        \"type\": \"text/javascript\"\n                      }\n                    }\n                  ],\n                  \"request\": {\n                    \"method\": \"POST\",\n                    \"header\": [],\n                    \"body\": {\n                      \"mode\": \"graphql\",\n                      \"graphql\": {\n                        \"query\": \"query ($counties: [String]!, $skipCache: Boolean) {\\n    county_summary_geojson (counties: $counties, skipCache: $skipCache) {\\n        type\\n        features {\\n            type\\n            id\\n            properties\\n            geometry {\\n                coordinates\\n                type\\n            }\\n        }\\n    }\\n}\",\n                        \"variables\": \"{\\n    \\\"counties\\\": [\\\"47167\\\", \\\"47017\\\"],\\n    \\\"skipCache\\\": true\\n}\"\n                      }\n                    },\n                    \"url\": {\n                      \"raw\": \"{{apolloApiUrl}}/graphql\",\n                      \"host\": [\"{{apolloApiUrl}}\"],\n                      \"path\": [\"graphql\"]\n                    }\n                  },\n                  \"response\": []\n                },\n                {\n                  \"name\": \"county_summary_county_geojson\",\n                  \"event\": [\n                    {\n                      \"listen\": \"prerequest\",\n                      \"script\": {\n                        \"exec\": [\"\"],\n                        \"type\": \"text/javascript\"\n                      }\n                    }\n                  ],\n                  \"request\": {\n                    \"method\": \"POST\",\n                    \"header\": [],\n                    \"body\": {\n                      \"mode\": \"graphql\",\n                      \"graphql\": {\n                        \"query\": \"query ($county: String!, $skipCache: Boolean) {\\n    county_summary_county_geojson (county: $county, skipCache: $skipCache) {\\n        type\\n        features {\\n            type\\n            id\\n            properties\\n            geometry {\\n                coordinates\\n                type\\n            }\\n        }\\n    }\\n}\",\n                        \"variables\": \"{\\n    \\\"county\\\": \\\"47167\\\",\\n    \\\"skipCache\\\": true\\n}\"\n                      }\n                    },\n                    \"url\": {\n                      \"raw\": \"{{apolloApiUrl}}/graphql\",\n                      \"host\": [\"{{apolloApiUrl}}\"],\n                      \"path\": [\"graphql\"]\n                    }\n                  },\n                  \"response\": []\n                }\n              ]\n            },\n            {\n              \"name\": \"restapi\",\n              \"item\": [\n                {\n                  \"name\": \"county_summary/geojson\",\n                  \"event\": [\n                    {\n                      \"listen\": \"prerequest\",\n                      \"script\": {\n                        \"exec\": [\"\"],\n                        \"type\": \"text/javascript\"\n                      }\n                    }\n                  ],\n                  \"request\": {\n                    \"method\": \"GET\",\n                    \"header\": [\n                      {\n                        \"key\": \"api\",\n                        \"value\": \"Xy4Gq51ixT3OYvZPqhb3D5OdemA3BYZ06ISg5Duo\",\n                        \"type\": \"text\"\n                      },\n                      {\n                        \"key\": \"Access\",\n                        \"value\": \"\",\n                        \"type\": \"text\"\n                      }\n                    ],\n                    \"url\": {\n                      \"raw\": \"{{pythonApiUrl}}/bcat/county_summary/geojson?geoid_co=47001,47003\",\n                      \"host\": [\"{{pythonApiUrl}}\"],\n                      \"path\": [\"bcat\", \"county_summary\", \"geojson\"],\n                      \"query\": [\n                        {\n                          \"key\": \"geoid_co\",\n                          \"value\": \"47001,47003\"\n                        }\n                      ]\n                    }\n                  },\n                  \"response\": []\n                }\n              ]\n            },\n            {\n              \"name\": \"local\",\n              \"item\": [\n                {\n                  \"name\": \"county_summary/geojson\",\n                  \"event\": [\n                    {\n                      \"listen\": \"prerequest\",\n                      \"script\": {\n                        \"exec\": [\"\"],\n                        \"type\": \"text/javascript\"\n                      }\n                    }\n                  ],\n                  \"request\": {\n                    \"method\": \"GET\",\n                    \"header\": [\n                      {\n                        \"key\": \"api\",\n                        \"value\": \"Xy4Gq51ixT3OYvZPqhb3D5OdemA3BYZ06ISg5Duo\",\n                        \"type\": \"text\"\n                      },\n                      {\n                        \"key\": \"Access\",\n                        \"value\": \"\",\n                        \"type\": \"text\"\n                      }\n                    ],\n                    \"url\": {\n                      \"raw\": \"{{localApiUrl}}/bcat/county_summary/geojson?geoid_co=47001,47003\",\n                      \"host\": [\"{{localApiUrl}}\"],\n                      \"path\": [\"bcat\", \"county_summary\", \"geojson\"],\n                      \"query\": [\n                        {\n                          \"key\": \"geoid_co\",\n                          \"value\": \"47001,47003\"\n                        }\n                      ]\n                    }\n                  },\n                  \"response\": []\n                }\n              ]\n            }\n          ]\n        },\n        {\n          \"name\": \"fiber_cable_unserved_blocks\",\n          \"item\": [\n            {\n              \"name\": \"graphql\",\n              \"item\": [\n                {\n                  \"name\": \"fiber_cable_unserved_blocks_geojson\",\n                  \"event\": [\n                    {\n                      \"listen\": \"prerequest\",\n                      \"script\": {\n                        \"exec\": [\"\"],\n                        \"type\": \"text/javascript\"\n                      }\n                    }\n                  ],\n                  \"request\": {\n                    \"method\": \"POST\",\n                    \"header\": [],\n                    \"body\": {\n                      \"mode\": \"graphql\",\n                      \"graphql\": {\n                        \"query\": \"query ($counties: [String]!, $skipCache: Boolean) {\\n    fiber_cable_unserved_blocks_geojson (counties: $counties, skipCache: $skipCache) {\\n        type\\n        features {\\n            type\\n            id\\n            properties\\n            geometry {\\n                coordinates\\n                type\\n            }\\n        }\\n    }\\n}\",\n                        \"variables\": \"{\\n    \\\"counties\\\": [\\\"47167\\\", \\\"47017\\\"],\\n    \\\"skipCache\\\": true\\n}\"\n                      }\n                    },\n                    \"url\": {\n                      \"raw\": \"{{apolloApiUrl}}/graphql\",\n                      \"host\": [\"{{apolloApiUrl}}\"],\n                      \"path\": [\"graphql\"]\n                    }\n                  },\n                  \"response\": []\n                },\n                {\n                  \"name\": \"fiber_cable_unserved_blocks_county_geojson\",\n                  \"event\": [\n                    {\n                      \"listen\": \"prerequest\",\n                      \"script\": {\n                        \"exec\": [\"\"],\n                        \"type\": \"text/javascript\"\n                      }\n                    }\n                  ],\n                  \"request\": {\n                    \"method\": \"POST\",\n                    \"header\": [],\n                    \"body\": {\n                      \"mode\": \"graphql\",\n                      \"graphql\": {\n                        \"query\": \"query ($county: String!, $skipCache: Boolean) {\\n    fiber_cable_unserved_blocks_county_geojson (county: $county, skipCache: $skipCache) {\\n        type\\n        features {\\n            type\\n            id\\n            properties\\n            geometry {\\n                coordinates\\n                type\\n            }\\n        }\\n    }\\n}\",\n                        \"variables\": \"{\\n    \\\"county\\\": \\\"47167\\\",\\n    \\\"skipCache\\\": true\\n}\"\n                      }\n                    },\n                    \"url\": {\n                      \"raw\": \"{{apolloApiUrl}}/graphql\",\n                      \"host\": [\"{{apolloApiUrl}}\"],\n                      \"path\": [\"graphql\"]\n                    }\n                  },\n                  \"response\": []\n                }\n              ]\n            },\n            {\n              \"name\": \"restapi\",\n              \"item\": [\n                {\n                  \"name\": \"fiber_cable_unserved_blocks/geojson\",\n                  \"event\": [\n                    {\n                      \"listen\": \"prerequest\",\n                      \"script\": {\n                        \"exec\": [\"\"],\n                        \"type\": \"text/javascript\"\n                      }\n                    }\n                  ],\n                  \"request\": {\n                    \"method\": \"GET\",\n                    \"header\": [\n                      {\n                        \"key\": \"api\",\n                        \"value\": \"Xy4Gq51ixT3OYvZPqhb3D5OdemA3BYZ06ISg5Duo\",\n                        \"type\": \"text\"\n                      },\n                      {\n                        \"key\": \"Access\",\n                        \"value\": \"\",\n                        \"type\": \"text\"\n                      }\n                    ],\n                    \"url\": {\n                      \"raw\": \"{{pythonApiUrl}}/bcat/fiber_cable_unserved_blocks/geojson?geoid_co=47001,47003\",\n                      \"host\": [\"{{pythonApiUrl}}\"],\n                      \"path\": [\n                        \"bcat\",\n                        \"fiber_cable_unserved_blocks\",\n                        \"geojson\"\n                      ],\n                      \"query\": [\n                        {\n                          \"key\": \"geoid_co\",\n                          \"value\": \"47001,47003\"\n                        }\n                      ]\n                    }\n                  },\n                  \"response\": []\n                }\n              ]\n            },\n            {\n              \"name\": \"local\",\n              \"item\": [\n                {\n                  \"name\": \"fiber_cable_unserved_blocks/geojson\",\n                  \"event\": [\n                    {\n                      \"listen\": \"prerequest\",\n                      \"script\": {\n                        \"exec\": [\"\"],\n                        \"type\": \"text/javascript\"\n                      }\n                    }\n                  ],\n                  \"request\": {\n                    \"method\": \"GET\",\n                    \"header\": [\n                      {\n                        \"key\": \"api\",\n                        \"value\": \"Xy4Gq51ixT3OYvZPqhb3D5OdemA3BYZ06ISg5Duo\",\n                        \"type\": \"text\"\n                      },\n                      {\n                        \"key\": \"Access\",\n                        \"value\": \"\",\n                        \"type\": \"text\"\n                      }\n                    ],\n                    \"url\": {\n                      \"raw\": \"{{localApiUrl}}/bcat/fiber_cable_unserved_blocks/geojson?geoid_co=47001,47003\",\n                      \"host\": [\"{{localApiUrl}}\"],\n                      \"path\": [\n                        \"bcat\",\n                        \"fiber_cable_unserved_blocks\",\n                        \"geojson\"\n                      ],\n                      \"query\": [\n                        {\n                          \"key\": \"geoid_co\",\n                          \"value\": \"47001,47003\"\n                        }\n                      ]\n                    }\n                  },\n                  \"response\": []\n                }\n              ]\n            }\n          ]\n        },\n        {\n          \"name\": \"incumbent_electric_providers_geo\",\n          \"item\": [\n            {\n              \"name\": \"graphql\",\n              \"item\": [\n                {\n                  \"name\": \"incumbent_electric_providers_geo_geojson\",\n                  \"event\": [\n                    {\n                      \"listen\": \"prerequest\",\n                      \"script\": {\n                        \"exec\": [\n                          \"var clientId = pm.environment.get(\\\"cognitoClientId\\\");\",\n                          \"var username = pm.environment.get(\\\"cognitoUserName\\\");\",\n                          \"var password = pm.environment.get(\\\"cognitoUserPassword\\\");\",\n                          \"console.log(username)\",\n                          \"console.log(password)\",\n                          \"pm.sendRequest({\",\n                          \"           url: \\\"https://cognito-idp.us-east-1.amazonaws.com/\\\",\",\n                          \"           method: 'POST',\",\n                          \"           header: {\",\n                          \"                    'X-Amz-Target':   'AWSCognitoIdentityProviderService.InitiateAuth',\",\n                          \"                    'Content-Type': 'application/x-amz-json-1.1'\",\n                          \"                   },\",\n                          \"            body: {\",\n                          \"                   mode: 'raw',\",\n                          \"                   raw: JSON.stringify({\",\n                          \"                   \\\"AuthParameters\\\": {\",\n                          \"                   \\\"USERNAME\\\": username,\",\n                          \"                   \\\"PASSWORD\\\": password\",\n                          \"                   },\",\n                          \"                  \\\"AuthFlow\\\": \\\"USER_PASSWORD_AUTH\\\",\",\n                          \"                  \\\"ClientId\\\": clientId\",\n                          \"  }),\",\n                          \"options: {\",\n                          \"raw: {\",\n                          \"language: 'json'\",\n                          \"}\",\n                          \"}\",\n                          \"}\",\n                          \"}, function (error, response) {\",\n                          \"console.log(response.json());\",\n                          \"pm.environment.set(\\\"cognitoAccessToken\\\", response.json().AuthenticationResult.AccessToken);\",\n                          \"pm.environment.set(\\\"cognitoIdToken\\\", response.json().AuthenticationResult.IdToken);\",\n                          \"});\"\n                        ],\n                        \"type\": \"text/javascript\"\n                      }\n                    }\n                  ],\n                  \"request\": {\n                    \"auth\": {\n                      \"type\": \"oauth2\",\n                      \"oauth2\": [\n                        {\n                          \"key\": \"tokenType\",\n                          \"value\": \"\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"accessToken\",\n                          \"value\": \"{{cognitoIdToken}}\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"redirect_uri\",\n                          \"value\": \"https://example.com/\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"authUrl\",\n                          \"value\": \"https://mf-data-api-dev.auth.us-east-1.amazoncognito.com\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"scope\",\n                          \"value\": \"phone email openid profile aws.cognito.signin.user.admin\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"useBrowser\",\n                          \"value\": false,\n                          \"type\": \"boolean\"\n                        },\n                        {\n                          \"key\": \"clientId\",\n                          \"value\": \"id5voqulhfbc55ko0n59h6ae4\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"grant_type\",\n                          \"value\": \"implicit\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"tokenName\",\n                          \"value\": \"MF Cori Api Cognito Token\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"addTokenTo\",\n                          \"value\": \"header\",\n                          \"type\": \"string\"\n                        }\n                      ]\n                    },\n                    \"method\": \"POST\",\n                    \"header\": [],\n                    \"body\": {\n                      \"mode\": \"graphql\",\n                      \"graphql\": {\n                        \"query\": \"query ($state_abbr: String!, $skipCache: Boolean) {\\n    incumbent_electric_providers_geo_geojson (state_abbr: $state_abbr, skipCache: $skipCache) {\\n        type\\n        features {\\n            type\\n            id\\n            properties\\n            geometry {\\n                coordinates\\n                type\\n            }\\n        }\\n    }\\n}\",\n                        \"variables\": \"{\\n    \\\"state_abbr\\\": \\\"TN\\\",\\n    \\\"skipCache\\\": true\\n}\"\n                      }\n                    },\n                    \"url\": {\n                      \"raw\": \"{{apolloApiUrl}}/graphql\",\n                      \"host\": [\"{{apolloApiUrl}}\"],\n                      \"path\": [\"graphql\"]\n                    }\n                  },\n                  \"response\": []\n                }\n              ]\n            },\n            {\n              \"name\": \"restapi\",\n              \"item\": [\n                {\n                  \"name\": \"incumbent_electric_providers_geo/geojson\",\n                  \"event\": [\n                    {\n                      \"listen\": \"prerequest\",\n                      \"script\": {\n                        \"exec\": [\"\"],\n                        \"type\": \"text/javascript\"\n                      }\n                    }\n                  ],\n                  \"request\": {\n                    \"method\": \"GET\",\n                    \"header\": [\n                      {\n                        \"key\": \"api\",\n                        \"value\": \"Xy4Gq51ixT3OYvZPqhb3D5OdemA3BYZ06ISg5Duo\",\n                        \"type\": \"text\"\n                      },\n                      {\n                        \"key\": \"Access\",\n                        \"value\": \"\",\n                        \"type\": \"text\"\n                      }\n                    ],\n                    \"url\": {\n                      \"raw\": \"{{pythonApiUrl}}/bcat/incumbent_electric_providers_geo/geojson?state_abbr=TN\",\n                      \"host\": [\"{{pythonApiUrl}}\"],\n                      \"path\": [\n                        \"bcat\",\n                        \"incumbent_electric_providers_geo\",\n                        \"geojson\"\n                      ],\n                      \"query\": [\n                        {\n                          \"key\": \"state_abbr\",\n                          \"value\": \"TN\"\n                        }\n                      ]\n                    }\n                  },\n                  \"response\": []\n                }\n              ]\n            },\n            {\n              \"name\": \"local\",\n              \"item\": [\n                {\n                  \"name\": \"incumbent_electric_providers_geo/geojson\",\n                  \"event\": [\n                    {\n                      \"listen\": \"prerequest\",\n                      \"script\": {\n                        \"exec\": [\"\"],\n                        \"type\": \"text/javascript\"\n                      }\n                    }\n                  ],\n                  \"request\": {\n                    \"method\": \"GET\",\n                    \"header\": [\n                      {\n                        \"key\": \"api\",\n                        \"value\": \"Xy4Gq51ixT3OYvZPqhb3D5OdemA3BYZ06ISg5Duo\",\n                        \"type\": \"text\"\n                      },\n                      {\n                        \"key\": \"Access\",\n                        \"value\": \"\",\n                        \"type\": \"text\"\n                      }\n                    ],\n                    \"url\": {\n                      \"raw\": \"{{localApiUrl}}/bcat/incumbent_electric_providers_geo/geojson?state_abbr=TN\",\n                      \"host\": [\"{{localApiUrl}}\"],\n                      \"path\": [\n                        \"bcat\",\n                        \"incumbent_electric_providers_geo\",\n                        \"geojson\"\n                      ],\n                      \"query\": [\n                        {\n                          \"key\": \"state_abbr\",\n                          \"value\": \"TN\"\n                        }\n                      ]\n                    }\n                  },\n                  \"response\": []\n                }\n              ]\n            }\n          ]\n        },\n        {\n          \"name\": \"county_adjacency_crosswalk\",\n          \"item\": [\n            {\n              \"name\": \"graphql\",\n              \"item\": [\n                {\n                  \"name\": \"county_adjacency_crosswalk_geojson\",\n                  \"event\": [\n                    {\n                      \"listen\": \"prerequest\",\n                      \"script\": {\n                        \"exec\": [\n                          \"var clientId = pm.environment.get(\\\"cognitoClientId\\\");\",\n                          \"var username = pm.environment.get(\\\"cognitoUserName\\\");\",\n                          \"var password = pm.environment.get(\\\"cognitoUserPassword\\\");\",\n                          \"console.log(username)\",\n                          \"console.log(password)\",\n                          \"pm.sendRequest({\",\n                          \"           url: \\\"https://cognito-idp.us-east-1.amazonaws.com/\\\",\",\n                          \"           method: 'POST',\",\n                          \"           header: {\",\n                          \"                    'X-Amz-Target':   'AWSCognitoIdentityProviderService.InitiateAuth',\",\n                          \"                    'Content-Type': 'application/x-amz-json-1.1'\",\n                          \"                   },\",\n                          \"            body: {\",\n                          \"                   mode: 'raw',\",\n                          \"                   raw: JSON.stringify({\",\n                          \"                   \\\"AuthParameters\\\": {\",\n                          \"                   \\\"USERNAME\\\": username,\",\n                          \"                   \\\"PASSWORD\\\": password\",\n                          \"                   },\",\n                          \"                  \\\"AuthFlow\\\": \\\"USER_PASSWORD_AUTH\\\",\",\n                          \"                  \\\"ClientId\\\": clientId\",\n                          \"  }),\",\n                          \"options: {\",\n                          \"raw: {\",\n                          \"language: 'json'\",\n                          \"}\",\n                          \"}\",\n                          \"}\",\n                          \"}, function (error, response) {\",\n                          \"console.log(response.json());\",\n                          \"pm.environment.set(\\\"cognitoAccessToken\\\", response.json().AuthenticationResult.AccessToken);\",\n                          \"pm.environment.set(\\\"cognitoIdToken\\\", response.json().AuthenticationResult.IdToken);\",\n                          \"});\"\n                        ],\n                        \"type\": \"text/javascript\"\n                      }\n                    }\n                  ],\n                  \"request\": {\n                    \"auth\": {\n                      \"type\": \"oauth2\",\n                      \"oauth2\": [\n                        {\n                          \"key\": \"tokenType\",\n                          \"value\": \"\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"accessToken\",\n                          \"value\": \"{{cognitoIdToken}}\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"redirect_uri\",\n                          \"value\": \"https://example.com/\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"authUrl\",\n                          \"value\": \"https://mf-data-api-dev.auth.us-east-1.amazoncognito.com\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"scope\",\n                          \"value\": \"phone email openid profile aws.cognito.signin.user.admin\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"useBrowser\",\n                          \"value\": false,\n                          \"type\": \"boolean\"\n                        },\n                        {\n                          \"key\": \"clientId\",\n                          \"value\": \"id5voqulhfbc55ko0n59h6ae4\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"grant_type\",\n                          \"value\": \"implicit\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"tokenName\",\n                          \"value\": \"MF Cori Api Cognito Token\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"addTokenTo\",\n                          \"value\": \"header\",\n                          \"type\": \"string\"\n                        }\n                      ]\n                    },\n                    \"method\": \"POST\",\n                    \"header\": [],\n                    \"body\": {\n                      \"mode\": \"graphql\",\n                      \"graphql\": {\n                        \"query\": \"query ($counties: [String], $skipCache: Boolean) {\\n    county_adjacency_crosswalk_geojson (counties: $counties, skipCache: $skipCache) {\\n        type\\n        features {\\n            type\\n            id\\n            properties\\n            geometry {\\n                coordinates\\n                type\\n            }\\n        }\\n    }\\n}\",\n                        \"variables\": \"{\\n    \\\"state_abbr\\\": \\\"TN\\\",\\n    \\\"skipCache\\\": true\\n}\"\n                      }\n                    },\n                    \"url\": {\n                      \"raw\": \"{{apolloApiUrl}}/graphql\",\n                      \"host\": [\"{{apolloApiUrl}}\"],\n                      \"path\": [\"graphql\"]\n                    }\n                  },\n                  \"response\": []\n                },\n                {\n                  \"name\": \"county_adjacency_crosswalk_county_geojson\",\n                  \"event\": [\n                    {\n                      \"listen\": \"prerequest\",\n                      \"script\": {\n                        \"exec\": [\n                          \"var clientId = pm.environment.get(\\\"cognitoClientId\\\");\",\n                          \"var username = pm.environment.get(\\\"cognitoUserName\\\");\",\n                          \"var password = pm.environment.get(\\\"cognitoUserPassword\\\");\",\n                          \"console.log(username)\",\n                          \"console.log(password)\",\n                          \"pm.sendRequest({\",\n                          \"           url: \\\"https://cognito-idp.us-east-1.amazonaws.com/\\\",\",\n                          \"           method: 'POST',\",\n                          \"           header: {\",\n                          \"                    'X-Amz-Target':   'AWSCognitoIdentityProviderService.InitiateAuth',\",\n                          \"                    'Content-Type': 'application/x-amz-json-1.1'\",\n                          \"                   },\",\n                          \"            body: {\",\n                          \"                   mode: 'raw',\",\n                          \"                   raw: JSON.stringify({\",\n                          \"                   \\\"AuthParameters\\\": {\",\n                          \"                   \\\"USERNAME\\\": username,\",\n                          \"                   \\\"PASSWORD\\\": password\",\n                          \"                   },\",\n                          \"                  \\\"AuthFlow\\\": \\\"USER_PASSWORD_AUTH\\\",\",\n                          \"                  \\\"ClientId\\\": clientId\",\n                          \"  }),\",\n                          \"options: {\",\n                          \"raw: {\",\n                          \"language: 'json'\",\n                          \"}\",\n                          \"}\",\n                          \"}\",\n                          \"}, function (error, response) {\",\n                          \"console.log(response.json());\",\n                          \"pm.environment.set(\\\"cognitoAccessToken\\\", response.json().AuthenticationResult.AccessToken);\",\n                          \"pm.environment.set(\\\"cognitoIdToken\\\", response.json().AuthenticationResult.IdToken);\",\n                          \"});\"\n                        ],\n                        \"type\": \"text/javascript\"\n                      }\n                    }\n                  ],\n                  \"request\": {\n                    \"auth\": {\n                      \"type\": \"oauth2\",\n                      \"oauth2\": [\n                        {\n                          \"key\": \"tokenType\",\n                          \"value\": \"\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"accessToken\",\n                          \"value\": \"{{cognitoIdToken}}\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"redirect_uri\",\n                          \"value\": \"https://example.com/\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"authUrl\",\n                          \"value\": \"https://mf-data-api-dev.auth.us-east-1.amazoncognito.com\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"scope\",\n                          \"value\": \"phone email openid profile aws.cognito.signin.user.admin\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"useBrowser\",\n                          \"value\": false,\n                          \"type\": \"boolean\"\n                        },\n                        {\n                          \"key\": \"clientId\",\n                          \"value\": \"id5voqulhfbc55ko0n59h6ae4\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"grant_type\",\n                          \"value\": \"implicit\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"tokenName\",\n                          \"value\": \"MF Cori Api Cognito Token\",\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"key\": \"addTokenTo\",\n                          \"value\": \"header\",\n                          \"type\": \"string\"\n                        }\n                      ]\n                    },\n                    \"method\": \"POST\",\n                    \"header\": [],\n                    \"body\": {\n                      \"mode\": \"graphql\",\n                      \"graphql\": {\n                        \"query\": \"query ($county: String!, $skipCache: Boolean) {\\n    county_adjacency_crosswalk_county_geojson (county: $county, skipCache: $skipCache) {\\n        type\\n        features {\\n            type\\n            id\\n            properties\\n            geometry {\\n                coordinates\\n                type\\n            }\\n        }\\n    }\\n}\",\n                        \"variables\": \"{\\n    \\\"state_abbr\\\": \\\"TN\\\",\\n    \\\"skipCache\\\": true\\n}\"\n                      }\n                    },\n                    \"url\": {\n                      \"raw\": \"{{apolloApiUrl}}/graphql\",\n                      \"host\": [\"{{apolloApiUrl}}\"],\n                      \"path\": [\"graphql\"]\n                    }\n                  },\n                  \"response\": []\n                }\n              ]\n            },\n            {\n              \"name\": \"restapi\",\n              \"item\": [\n                {\n                  \"name\": \"county_adjacency_crosswalk/geojson\",\n                  \"event\": [\n                    {\n                      \"listen\": \"prerequest\",\n                      \"script\": {\n                        \"exec\": [\"\"],\n                        \"type\": \"text/javascript\"\n                      }\n                    }\n                  ],\n                  \"request\": {\n                    \"method\": \"GET\",\n                    \"header\": [\n                      {\n                        \"key\": \"api\",\n                        \"value\": \"Xy4Gq51ixT3OYvZPqhb3D5OdemA3BYZ06ISg5Duo\",\n                        \"type\": \"text\"\n                      },\n                      {\n                        \"key\": \"Access\",\n                        \"value\": \"\",\n                        \"type\": \"text\"\n                      }\n                    ],\n                    \"url\": {\n                      \"raw\": \"{{pythonApiUrl}}/bcat/county_adjacency_crosswalk/geojson?geoid_co=47001,47003\",\n                      \"host\": [\"{{pythonApiUrl}}\"],\n                      \"path\": [\"bcat\", \"county_adjacency_crosswalk\", \"geojson\"],\n                      \"query\": [\n                        {\n                          \"key\": \"geoid_co\",\n                          \"value\": \"47001,47003\"\n                        }\n                      ]\n                    }\n                  },\n                  \"response\": []\n                }\n              ]\n            },\n            {\n              \"name\": \"local\",\n              \"item\": [\n                {\n                  \"name\": \"county_adjacency_crosswalk/geojson\",\n                  \"event\": [\n                    {\n                      \"listen\": \"prerequest\",\n                      \"script\": {\n                        \"exec\": [\"\"],\n                        \"type\": \"text/javascript\"\n                      }\n                    }\n                  ],\n                  \"request\": {\n                    \"method\": \"GET\",\n                    \"header\": [\n                      {\n                        \"key\": \"api\",\n                        \"value\": \"Xy4Gq51ixT3OYvZPqhb3D5OdemA3BYZ06ISg5Duo\",\n                        \"type\": \"text\"\n                      },\n                      {\n                        \"key\": \"Access\",\n                        \"value\": \"\",\n                        \"type\": \"text\"\n                      }\n                    ],\n                    \"url\": {\n                      \"raw\": \"{{localApiUrl}}/bcat/county_adjacency_crosswalk/geojson?geoid_co=47001,47003\",\n                      \"host\": [\"{{localApiUrl}}\"],\n                      \"path\": [\"bcat\", \"county_adjacency_crosswalk\", \"geojson\"],\n                      \"query\": [\n                        {\n                          \"key\": \"geoid_co\",\n                          \"value\": \"47001,47003\"\n                        }\n                      ]\n                    }\n                  },\n                  \"response\": []\n                }\n              ]\n            }\n          ]\n        },\n        {\n          \"name\": \"general_queries\",\n          \"item\": [\n            {\n              \"name\": \"feature_collection\",\n              \"event\": [\n                {\n                  \"listen\": \"prerequest\",\n                  \"script\": {\n                    \"exec\": [\"\"],\n                    \"type\": \"text/javascript\"\n                  }\n                }\n              ],\n              \"request\": {\n                \"method\": \"POST\",\n                \"header\": [],\n                \"body\": {\n                  \"mode\": \"graphql\",\n                  \"graphql\": {\n                    \"query\": \"query ($table: String!, $counties: [String]) {\\n    feature_collection (table: $table, counties: $counties) {\\n        type\\n        features {\\n            type\\n            id\\n            properties\\n            geometry {\\n                coordinates\\n                type\\n            }\\n        }\\n    }\\n}\",\n                    \"variables\": \"{\\n    \\\"table\\\": \\\"auction_904_subsidy_awards\\\",\\n    \\\"counties\\\": [\\\"47167\\\", \\\"47017\\\"]\\n}\"\n                  }\n                },\n                \"url\": {\n                  \"raw\": \"{{apolloApiUrl}}/graphql\",\n                  \"host\": [\"{{apolloApiUrl}}\"],\n                  \"path\": [\"graphql\"]\n                }\n              },\n              \"response\": []\n            },\n            {\n              \"name\": \"county_feature_collection\",\n              \"event\": [\n                {\n                  \"listen\": \"prerequest\",\n                  \"script\": {\n                    \"exec\": [\"\"],\n                    \"type\": \"text/javascript\"\n                  }\n                }\n              ],\n              \"request\": {\n                \"method\": \"POST\",\n                \"header\": [],\n                \"body\": {\n                  \"mode\": \"graphql\",\n                  \"graphql\": {\n                    \"query\": \"query ($table: String!, $county: String!) {\\n    county_feature (table: $table, county: $county) {\\n        type\\n        features {\\n            type\\n            id\\n            properties\\n            geometry {\\n                coordinates\\n                type\\n            }\\n        }\\n    }\\n}\",\n                    \"variables\": \"{\\n    \\\"table\\\": \\\"auction_904_subsidy_awards\\\",\\n    \\\"county\\\": \\\"47167\\\"\\n}\"\n                  }\n                },\n                \"url\": {\n                  \"raw\": \"{{apolloApiUrl}}/graphql\",\n                  \"host\": [\"{{apolloApiUrl}}\"],\n                  \"path\": [\"graphql\"]\n                }\n              },\n              \"response\": []\n            }\n          ]\n        }\n      ]\n    }\n  ],\n  \"auth\": {\n    \"type\": \"oauth2\",\n    \"oauth2\": [\n      {\n        \"key\": \"tokenType\",\n        \"value\": \"\",\n        \"type\": \"string\"\n      },\n      {\n        \"key\": \"accessToken\",\n        \"value\": \"{{cognitoIdToken}}\",\n        \"type\": \"string\"\n      },\n      {\n        \"key\": \"addTokenTo\",\n        \"value\": \"header\",\n        \"type\": \"string\"\n      },\n      {\n        \"key\": \"scope\",\n        \"value\": \"\",\n        \"type\": \"string\"\n      },\n      {\n        \"key\": \"clientId\",\n        \"value\": \"\",\n        \"type\": \"string\"\n      },\n      {\n        \"key\": \"authUrl\",\n        \"value\": \"\",\n        \"type\": \"string\"\n      },\n      {\n        \"key\": \"redirect_uri\",\n        \"value\": \"\",\n        \"type\": \"string\"\n      },\n      {\n        \"key\": \"tokenName\",\n        \"value\": \"\",\n        \"type\": \"string\"\n      },\n      {\n        \"key\": \"challengeAlgorithm\",\n        \"value\": \"S256\",\n        \"type\": \"string\"\n      },\n      {\n        \"key\": \"grant_type\",\n        \"value\": \"implicit\",\n        \"type\": \"string\"\n      },\n      {\n        \"key\": \"client_authentication\",\n        \"value\": \"header\",\n        \"type\": \"string\"\n      }\n    ]\n  },\n  \"event\": [\n    {\n      \"listen\": \"prerequest\",\n      \"script\": {\n        \"type\": \"text/javascript\",\n        \"exec\": [\n          \"var clientId = pm.environment.get(\\\"cognitoClientId\\\");\",\n          \"var username = pm.environment.get(\\\"cognitoUserName\\\");\",\n          \"var password = pm.environment.get(\\\"cognitoUserPassword\\\");\",\n          \"console.log(username)\",\n          \"console.log(password)\",\n          \"console.log(clientId)\",\n          \"pm.sendRequest({\",\n          \"           url: \\\"https://cognito-idp.us-east-1.amazonaws.com/\\\",\",\n          \"           method: 'POST',\",\n          \"           header: {\",\n          \"                    'X-Amz-Target':   'AWSCognitoIdentityProviderService.InitiateAuth',\",\n          \"                    'Content-Type': 'application/x-amz-json-1.1'\",\n          \"                   },\",\n          \"            body: {\",\n          \"                   mode: 'raw',\",\n          \"                   raw: JSON.stringify({\",\n          \"                   \\\"AuthParameters\\\": {\",\n          \"                   \\\"USERNAME\\\": username,\",\n          \"                   \\\"PASSWORD\\\": password\",\n          \"                   },\",\n          \"                  \\\"AuthFlow\\\": \\\"USER_PASSWORD_AUTH\\\",\",\n          \"                  \\\"ClientId\\\": clientId\",\n          \"  }),\",\n          \"options: {\",\n          \"raw: {\",\n          \"language: 'json'\",\n          \"}\",\n          \"}\",\n          \"}\",\n          \"}, function (error, response) {\",\n          \"console.log(response.json());\",\n          \"pm.environment.set(\\\"cognitoAccessToken\\\", response.json().AuthenticationResult.AccessToken);\",\n          \"pm.environment.set(\\\"cognitoIdToken\\\", response.json().AuthenticationResult.IdToken);\",\n          \"});\"\n        ]\n      }\n    },\n    {\n      \"listen\": \"test\",\n      \"script\": {\n        \"type\": \"text/javascript\",\n        \"exec\": [\"\"]\n      }\n    }\n  ],\n  \"variable\": []\n}"
  },
  {
    "objectID": "cori-dev.postman_environment.json.html",
    "href": "cori-dev.postman_environment.json.html",
    "title": "CORI/RISI Mapping & Data Analysis (MDA)",
    "section": "",
    "text": "{\n    \"id\": \"716f2986-451d-4b82-9058-93b82880f37b\",\n    \"name\": \"cori-dev\",\n    \"values\": [\n        {\n            \"key\": \"cognitoClientId\",\n            \"value\": \"70o6i77h1orcnvonb9ua3fh58e\",\n            \"type\": \"default\",\n            \"enabled\": true\n        },\n        {\n            \"key\": \"cognitoAccessToken\",\n            \"value\": \"\",\n            \"type\": \"secret\",\n            \"enabled\": true\n        },\n        {\n            \"key\": \"cognitoIdToken\",\n            \"value\": \"\",\n            \"type\": \"secret\",\n            \"enabled\": true\n        },\n        {\n            \"key\": \"cognitoUserName\",\n            \"value\": \"nahum@mergingfutures.com\",\n            \"type\": \"default\",\n            \"enabled\": true\n        },\n        {\n            \"key\": \"cognitoUserPassword\",\n            \"value\": \"Temporary54321!\",\n            \"type\": \"secret\",\n            \"enabled\": true\n        },\n        {\n            \"key\": \"apolloApiUrl\",\n            \"value\": \"https://d6q5pgqgx5oy5.cloudfront.net\",\n            \"type\": \"default\",\n            \"enabled\": true\n        },\n        {\n            \"key\": \"pythonApiUrl\",\n            \"value\": \"https://d6q5pgqgx5oy5.cloudfront.net\",\n            \"type\": \"default\",\n            \"enabled\": true\n        },\n        {\n            \"key\": \"localApiUrl\",\n            \"value\": \"http://localhost:2000/local\",\n            \"type\": \"default\",\n            \"enabled\": true\n        }\n    ],\n    \"_postman_variable_scope\": \"environment\",\n    \"_postman_exported_at\": \"2022-07-23T14:37:17.627Z\",\n    \"_postman_exported_using\": \"Postman/9.25.1\"\n}"
  },
  {
    "objectID": "Migrating-from-CDK-V1-=--V2.html",
    "href": "Migrating-from-CDK-V1-=--V2.html",
    "title": "CORI/RISI Mapping & Data Analysis (MDA)",
    "section": "",
    "text": "The main changes from AWS CDK v1 to CDK v2 are as follows. - AWS CDK v2 consolidates the stable parts of the AWS Construct Library, including the core library, into a single package, aws-cdk-lib. Developers no longer need to install additional packages for the individual AWS services they use. This single-package approach also means that you don‚Äôt have to synchronize the versions of the various CDK library packages. L1 (CfnXXXX) constructs, which represent the exact resources available in AWS CloudFormation, are always considered stable and so are included in aws-cdk-lib. - Experimental modules, where we‚Äôre still working with the community to develop new L2 or L3 constructs, are not included in aws-cdk-lib. Instead, they‚Äôre distributed as individual packages. Experimental packages are named with an alpha suffix and a semantic version number. The semantic version number matches the first version of the AWS Construct Library that they‚Äôre compatible with, also with an alpha suffix. Constructs move into aws-cdk-lib after being designated stable, permitting the main Construct Library to adhere to strict semantic versioning. Stability is specified at the service level. For example, if we begin creating one or more L2 constructs for Amazon AppFlow, which at this writing has only L1 constructs, they first appear in a module named @aws-cdk/aws-appflow-alpha. Then, they move to aws-cdk-lib when we feel that the new constructs meet the fundamental needs of customers. Once a module has been designated stable and incorporated into aws-cdk-lib, new APIs are added using the ‚ÄúBetaN‚Äù convention described in the next bullet. A new version of each experimental module is released with every release of the AWS CDK. For the most part, however, they needn‚Äôt be kept in sync. You can upgrade aws-cdk-lib or the experimental module whenever you want. The exception is that when two or more related experimental modules depend on each other, they must be the same version. - For stable modules to which new functionality is being added, new APIs (whether entirely new constructs or new methods or properties on an existing construct) receive a Beta1 suffix while work is in progress. (Followed by Beta2, Beta3, and so on when breaking changes are needed.) A version of the API without the suffix is added when the API is designated stable. All methods except the latest (whether beta or final) are then deprecated. For example, if we add a new method grantPower() to a construct, it initially appears as grantPowerBeta1(). If breaking changes are needed (for example, a new required parameter or property), the next version of the method would be named grantPowerBeta2(), and so on. When work is complete and the API is finalized, the method grantPower() (with no suffix) is added, and the BetaN methods are deprecated. All the beta APIs remain in the Construct Library until the next major version (3.0) release, and their signatures will not change. You‚Äôll see deprecation warnings if you use them, so you should move to the final version of the API at your earliest convenience. However, no future AWS CDK 2.x releases will break your application. - The Construct class has been extracted from the AWS CDK into a separate library, along with related types. This is done to support efforts to apply the Construct Programming Model to other domains. If you are writing your own constructs or using related APIs, you must declare the constructs module as a dependency and make minor changes to your imports. If you are using advanced features, such as hooking into the CDK app lifecycle, more changes may be needed. For full details, see the RFC. - Deprecated properties, methods, and types in AWS CDK v1.x and its Construct Library have been removed completely from the CDK v2 API. In most supported languages, these APIs produce warnings under v1.x, so you may have already migrated to the replacement APIs. A complete list of deprecated APIs in CDK v1.x is available on GitHub. - Behavior that was gated by feature flags in AWS CDK v1.x is enabled by default in CDK v2. The earlier feature flags are no longer needed, and in most cases they‚Äôre not supported. A few are still available to let you revert to CDK v1 behavior in very specific circumstances. For more information, see Updating feature flags. - With CDK v2, the environments you deploy into must be bootstrapped using the modern bootstrap stack. The legacy bootstrap stack (the default under v1) is no longer supported. CDK v2 furthermore requires a new version of the modern stack. To upgrade your existing environments, re-bootstrap them. It is no longer necessary to set any feature flags or environment variables to use the modern bootstrap stack.\n## Migration Steps\n### Update feature flags in cdk.json tp false - @aws-cdk/core:enableStackNameDuplicates - aws-cdk:enableDiffNoFail - @aws-cdk/aws-ecr-assets:dockerIgnoreSupport - @aws-cdk/aws-secretsmanager:parseOwnedSecretName - @aws-cdk/aws-kms:defaultKeyPolicies - @aws-cdk/aws-s3:grantWriteWithoutAcl - @aws-cdk/aws-efs:defaultEncryptionAtRest\nBefore {    \"app\": \"npx ts-node --prefer-ts-exts bin/aws-helper.ts\",    \"context\": {      \"@aws-cdk/core:newStyleStackSynthesis\": false,      \"@aws-cdk/core:stackRelativeExports\": false,      \"@aws-cdk/aws-rds:lowercaseDbIdentifier\": false,      \"@aws-cdk/aws-apigateway:usagePlanKeyOrderInsensitiveId\": false,      \"@aws-cdk/aws-lambda:recognizeVersionProps\": false,      \"@aws-cdk/aws-cloudfront:defaultSecurityPolicyTLSv1.2_2021\": false    }  } After {    \"app\": \"npx ts-node --prefer-ts-exts bin/aws-helper.ts\",    \"context\": {      \"@aws-cdk/core:newStyleStackSynthesis\": false,      \"@aws-cdk/core:stackRelativeExports\": false,      \"@aws-cdk/aws-rds:lowercaseDbIdentifier\": false,      \"@aws-cdk/aws-apigateway:usagePlanKeyOrderInsensitiveId\": false,      \"@aws-cdk/aws-lambda:recognizeVersionProps\": false,      \"@aws-cdk/aws-cloudfront:defaultSecurityPolicyTLSv1.2_2021\": false    }  } ### Bump to latest version of V1 as a precaution npm i @aws-cdk@1.204.0 --save\n### Update your app‚Äôs dependencies and imports as necessary for the programming language that it‚Äôs written in. Should look something like the following\n\nNOTE: Some libs could be experimental in CDK V2, which will mean we have to import those alpha packages in separately, like V1.\n\"dependencies\": {\n\"@aws-cdk/aws-lambda-python-alpha\": \"^2.0.0-alpha.11\",\n\"aws-cdk-lib\": \"^2.0.0\",\n\"constructs\": \"^10.0.0\"\n}\n\nnpm install -g aws-cdk@latest\n\ncdk bootstrap aws://ACCOUNT-NUMBER-1/REGION-1 ‚Äìprofile dev,uat,prod\n\n\n\nWindow OS Simply download and install the Python 3.9 installer from the Python download page. And you are not sure which version is for you. I would recommend going for Windows x86‚Äì64 executable installer version or you can simply click this link.\nRecommended Python Installer (Screenshot from the Python Download page) You may check on the ‚ÄúAdd Python to Path‚Äù option while installing to register Python3.9 as the main python on your system\n$ python --version Python 3.9.0"
  },
  {
    "objectID": "Migrating-from-CDK-V1-=--V2.html#main-changes",
    "href": "Migrating-from-CDK-V1-=--V2.html#main-changes",
    "title": "CORI/RISI Mapping & Data Analysis (MDA)",
    "section": "",
    "text": "The main changes from AWS CDK v1 to CDK v2 are as follows. - AWS CDK v2 consolidates the stable parts of the AWS Construct Library, including the core library, into a single package, aws-cdk-lib. Developers no longer need to install additional packages for the individual AWS services they use. This single-package approach also means that you don‚Äôt have to synchronize the versions of the various CDK library packages. L1 (CfnXXXX) constructs, which represent the exact resources available in AWS CloudFormation, are always considered stable and so are included in aws-cdk-lib. - Experimental modules, where we‚Äôre still working with the community to develop new L2 or L3 constructs, are not included in aws-cdk-lib. Instead, they‚Äôre distributed as individual packages. Experimental packages are named with an alpha suffix and a semantic version number. The semantic version number matches the first version of the AWS Construct Library that they‚Äôre compatible with, also with an alpha suffix. Constructs move into aws-cdk-lib after being designated stable, permitting the main Construct Library to adhere to strict semantic versioning. Stability is specified at the service level. For example, if we begin creating one or more L2 constructs for Amazon AppFlow, which at this writing has only L1 constructs, they first appear in a module named @aws-cdk/aws-appflow-alpha. Then, they move to aws-cdk-lib when we feel that the new constructs meet the fundamental needs of customers. Once a module has been designated stable and incorporated into aws-cdk-lib, new APIs are added using the ‚ÄúBetaN‚Äù convention described in the next bullet. A new version of each experimental module is released with every release of the AWS CDK. For the most part, however, they needn‚Äôt be kept in sync. You can upgrade aws-cdk-lib or the experimental module whenever you want. The exception is that when two or more related experimental modules depend on each other, they must be the same version. - For stable modules to which new functionality is being added, new APIs (whether entirely new constructs or new methods or properties on an existing construct) receive a Beta1 suffix while work is in progress. (Followed by Beta2, Beta3, and so on when breaking changes are needed.) A version of the API without the suffix is added when the API is designated stable. All methods except the latest (whether beta or final) are then deprecated. For example, if we add a new method grantPower() to a construct, it initially appears as grantPowerBeta1(). If breaking changes are needed (for example, a new required parameter or property), the next version of the method would be named grantPowerBeta2(), and so on. When work is complete and the API is finalized, the method grantPower() (with no suffix) is added, and the BetaN methods are deprecated. All the beta APIs remain in the Construct Library until the next major version (3.0) release, and their signatures will not change. You‚Äôll see deprecation warnings if you use them, so you should move to the final version of the API at your earliest convenience. However, no future AWS CDK 2.x releases will break your application. - The Construct class has been extracted from the AWS CDK into a separate library, along with related types. This is done to support efforts to apply the Construct Programming Model to other domains. If you are writing your own constructs or using related APIs, you must declare the constructs module as a dependency and make minor changes to your imports. If you are using advanced features, such as hooking into the CDK app lifecycle, more changes may be needed. For full details, see the RFC. - Deprecated properties, methods, and types in AWS CDK v1.x and its Construct Library have been removed completely from the CDK v2 API. In most supported languages, these APIs produce warnings under v1.x, so you may have already migrated to the replacement APIs. A complete list of deprecated APIs in CDK v1.x is available on GitHub. - Behavior that was gated by feature flags in AWS CDK v1.x is enabled by default in CDK v2. The earlier feature flags are no longer needed, and in most cases they‚Äôre not supported. A few are still available to let you revert to CDK v1 behavior in very specific circumstances. For more information, see Updating feature flags. - With CDK v2, the environments you deploy into must be bootstrapped using the modern bootstrap stack. The legacy bootstrap stack (the default under v1) is no longer supported. CDK v2 furthermore requires a new version of the modern stack. To upgrade your existing environments, re-bootstrap them. It is no longer necessary to set any feature flags or environment variables to use the modern bootstrap stack.\n## Migration Steps\n### Update feature flags in cdk.json tp false - @aws-cdk/core:enableStackNameDuplicates - aws-cdk:enableDiffNoFail - @aws-cdk/aws-ecr-assets:dockerIgnoreSupport - @aws-cdk/aws-secretsmanager:parseOwnedSecretName - @aws-cdk/aws-kms:defaultKeyPolicies - @aws-cdk/aws-s3:grantWriteWithoutAcl - @aws-cdk/aws-efs:defaultEncryptionAtRest\nBefore {    \"app\": \"npx ts-node --prefer-ts-exts bin/aws-helper.ts\",    \"context\": {      \"@aws-cdk/core:newStyleStackSynthesis\": false,      \"@aws-cdk/core:stackRelativeExports\": false,      \"@aws-cdk/aws-rds:lowercaseDbIdentifier\": false,      \"@aws-cdk/aws-apigateway:usagePlanKeyOrderInsensitiveId\": false,      \"@aws-cdk/aws-lambda:recognizeVersionProps\": false,      \"@aws-cdk/aws-cloudfront:defaultSecurityPolicyTLSv1.2_2021\": false    }  } After {    \"app\": \"npx ts-node --prefer-ts-exts bin/aws-helper.ts\",    \"context\": {      \"@aws-cdk/core:newStyleStackSynthesis\": false,      \"@aws-cdk/core:stackRelativeExports\": false,      \"@aws-cdk/aws-rds:lowercaseDbIdentifier\": false,      \"@aws-cdk/aws-apigateway:usagePlanKeyOrderInsensitiveId\": false,      \"@aws-cdk/aws-lambda:recognizeVersionProps\": false,      \"@aws-cdk/aws-cloudfront:defaultSecurityPolicyTLSv1.2_2021\": false    }  } ### Bump to latest version of V1 as a precaution npm i @aws-cdk@1.204.0 --save\n### Update your app‚Äôs dependencies and imports as necessary for the programming language that it‚Äôs written in. Should look something like the following\n\nNOTE: Some libs could be experimental in CDK V2, which will mean we have to import those alpha packages in separately, like V1.\n\"dependencies\": {\n\"@aws-cdk/aws-lambda-python-alpha\": \"^2.0.0-alpha.11\",\n\"aws-cdk-lib\": \"^2.0.0\",\n\"constructs\": \"^10.0.0\"\n}\n\nnpm install -g aws-cdk@latest\n\ncdk bootstrap aws://ACCOUNT-NUMBER-1/REGION-1 ‚Äìprofile dev,uat,prod\n\n\n\nWindow OS Simply download and install the Python 3.9 installer from the Python download page. And you are not sure which version is for you. I would recommend going for Windows x86‚Äì64 executable installer version or you can simply click this link.\nRecommended Python Installer (Screenshot from the Python Download page) You may check on the ‚ÄúAdd Python to Path‚Äù option while installing to register Python3.9 as the main python on your system\n$ python --version Python 3.9.0"
  },
  {
    "objectID": "Suggested-Code-Snippets.html",
    "href": "Suggested-Code-Snippets.html",
    "title": "Suggested Code Snippets",
    "section": "",
    "text": "Working with SQL\nRead Data\nWrite Data\nDisconnect from DB\nValidate join\nExplore Data\nVisualize data on a map\nCompare 2 dataframes",
    "crumbs": [
      "Home",
      "Team Onboarding",
      "Suggested Code Snippets"
    ]
  },
  {
    "objectID": "Suggested-Code-Snippets.html#toc",
    "href": "Suggested-Code-Snippets.html#toc",
    "title": "Suggested Code Snippets",
    "section": "",
    "text": "Working with SQL\nRead Data\nWrite Data\nDisconnect from DB\nValidate join\nExplore Data\nVisualize data on a map\nCompare 2 dataframes",
    "crumbs": [
      "Home",
      "Team Onboarding",
      "Suggested Code Snippets"
    ]
  },
  {
    "objectID": "G-Drive-Practice.html",
    "href": "G-Drive-Practice.html",
    "title": "Google Drive Practice",
    "section": "",
    "text": "MDA Shared Drive\nDrew recommends ‚Ä¶",
    "crumbs": [
      "Home",
      "Team Onboarding",
      "Google Drive Practice"
    ]
  },
  {
    "objectID": "help_I_have_a_data_team.html",
    "href": "help_I_have_a_data_team.html",
    "title": "Help I Have Data Team",
    "section": "",
    "text": "Help I Have Data Team\n\n\nüèóÔ∏è This page is under construction! üèóÔ∏è\n\nWhen I want to share data with them it is better to do it in .csv!\ncsv stand for comma separated value: it is a plain text file (with usually the first row containing the header) that divide every fields with a comma (‚Äúa separator‚Äù).\nSometimes you can also get ;, | or even ^ as separator!\nWhat matter here is that a csv file are a plain text file format, unlike .xls, were what you get is not what you see (when you share the files)!\n\n\nfiles name should not have white space and weird character!\nExample: final % 16 21.txt is a bad name!\nA lot of tools we are using are command line based (docker as an example) and here white spaces are used to separate stuff (arguments).\nNow, weird characters (#, $, &) are somewhat ‚Äúfine‚Äù. The ‚Äúsomewhat‚Äù is hard to predict, sometimes this character are used for other purpose and sometimes the tools we are using (or a dependency of the tool we are using) is old enough to only use ASCI characters!\nNaming is hard but some conventions can greatly help.\nWe can take one FCC files as an example:\nbdc_40_Other_fixed_broadband_J23_14nov2023.csv\nWe have nearly everything we need nicely separated by _:\n- ‚Äú40‚Äù is the state fips\n- ‚ÄúOther‚Äù is the technology category\n- ‚Äúfixed broadband‚Äù is what FCC is calling ‚Äúdata type‚Äù\n- ‚ÄúJ23‚Äù is teh release (June 2023)\n- ‚Äú14nov2023‚Äù is the date of release\nWe could improve it by by changing a _ to - between ‚Äúfixed‚Äù and ‚Äúbroadband‚Äù (because they are part of the category):\nbdc_40_Other_fixed-broadband_J23_14nov2023.csv",
    "crumbs": [
      "Home",
      "Team Onboarding",
      "Help I Have Data Team"
    ]
  },
  {
    "objectID": "Repository-Classification.html",
    "href": "Repository-Classification.html",
    "title": "Repository-Classification",
    "section": "",
    "text": "Repository-Classification\nMDA repositories run the gamut from code under active development to stale code that is no longer useful. To document the status of repositories, use one of the five following tags.\n\n\nStable\n\n\n\nlifecycle\n\n\nThis should be the standard tag for repositories that are not under active development. Stable code should have no anticipated breaking changes and have received some QA. To mark a repository as stable:\n\nPaste ![lifecycle](https://img.shields.io/badge/lifecycle-stable-green.svg) into the readme and commit changes\nAdd ‚Äòstable‚Äô as a topic for the repository\n\n\n\n\nMaturing\n\n\n\nlifecycle\n\n\nMaturing repositories are under active development. This tag should not be used long-term. It indicates that breaking changes are expected in the code. To be labeled ‚ÄòMaturing‚Äô, the existing code should have received some level of QA but is subject to change. To mark a repository as maturing:\n\nPaste ![lifecycle](https://img.shields.io/badge/lifecycle-maturing-blue.svg) into the readme and commit changes\nAdd ‚Äòmaturing‚Äô as a topic for the repository\n\n\n\n\nExperimental\n\n\n\nlifecycle\n\n\nThis tag should be used sparingly ‚Äì ideally we should have few experimental repos. Code in an experimental repo may not have received any QA. Nothing generated from an experimental repo should be used in a deliverable or as the foundation for other code without serious external review. To mark a repository as experimental:\n\nPaste ![lifecycle](https://img.shields.io/badge/lifecycle-experimental-orange.svg) into the readme and commit changes\nAdd ‚Äòexperimental‚Äô as a topic for the repository\n\n\n\n\nDeprecated\n\n\n\nlifecycle\n\n\nWe may wish to preserve legacy code in some repositories. These should be marked as deprecated and link to the current state of the art. To mark a repository as deprecated:\n\nPaste ![lifecycle](https://img.shields.io/badge/lifecycle-deprecated-red.svg) into the readme and commit changes\nAdd ‚Äòdeprecated‚Äô as a topic for the repository\n\n\n\n\nUnsupported\n\n\n\nlifecycle\n\n\nSome repositories contain code that is being used but should not receive additional development. This tag is intended to denote for ourselves and for anyone who comes after that if additional development on the tool or project associated with the repo is desired, there is a strong recommendation to rewrite the code entirely and not to treat the existing code as a useful or reliable foundation for future work.\nIn essence, this tag indicates the presence of catastrophic technical debt.\nTo mark a repository as unsupported:\n\nPaste ![lifecycle](https://www.repostatus.org/badges/latest/unsupported.svg) into the readme and commit changes\nAdd ‚Äòunsupported‚Äô as a topic for the repository",
    "crumbs": [
      "Home",
      "Infrastructure",
      "Repository-Classification"
    ]
  },
  {
    "objectID": "Research-Consulting-Tech-Support.html",
    "href": "Research-Consulting-Tech-Support.html",
    "title": "Research Consulting Tech Support",
    "section": "",
    "text": "Note: What follows if from the start of an MDA discussion about the ways we can/cannot provide access to data (for research)‚Ä¶\nWe approve access to the following datasets, in the following ways:\n\nacs\n\nGoogle Sheets (?)\nR (YES, BUT we cannot QC or guarantee the resulting work product)\nTableau (YES,‚Ä¶)\n\n..."
  },
  {
    "objectID": "Broadband-Consulting-Tech-Requirements.html",
    "href": "Broadband-Consulting-Tech-Requirements.html",
    "title": "Broadband Consulting Tech Requirement",
    "section": "",
    "text": "Note: These are key takeaways from discussions with the Broadband team about the process and scope of their day-to-day work‚Ä¶\nOur tradable broadband knowledge set is focused on how to pull people together to plan/build broadband infrastructure in strategic ways.\nTo do this we need:\n\nCollaboration and Share-ability (!)\n\nWeb-based map views that can be shared and accessed by URL\n\nCurrently, this capability is primarily supported by Carto\nPotential enablement with ArcGIS Online:\n\n‚Äúit seemed like a better version of Carto‚Äù - Myles\n\nFelt is also nice https://felt.com/ and work with QGIS\n\n\nUnderstanding of telecommunications infrastructure and landscape:\n\nWhere is/is not\n\nOperating footprint\nGrant funded areas (past awards?)\n\nWhat (tech + service level)\n\nISP presence\nAdvertised speeds\n\nCategorized by grant criteria (e.g.¬†BEAD)\n\n\nWho (ownership)\n\nDBA mapped back to single entity (source-of-truth)\nFunding history (previous grant recipient?)\n\nDensity of (potential) subscribers\nSocial/Demographic analysis of (potential) subscribers\n\nCalculate route miles needed for potential deployments in more sophisticated manner\n\n\nat which scale/granularity (resoltion): what is the best resolution to provide valuable insight without glutting everythings?\n\nUnderstanding actual performance (average speeds) of broadband networks\nUnderstanding physical topology (layout) of existing network\n\nAbility to model and compare models of potential (future) physical topology of networks\n\nDetermine/calculate feasibility by:\n\nCosts\nRoute Miles\nTechnology\n\n\n\nDynamically assess multiple geographic contexts for potential infrastructure planning\n\ncompare these contexts\nnon-standard geographies driven by real world requirements\nLowest level of geographic context may be neighborhood\nMay approximate mapping of block-level data to non-standard geography; when possible (this is hard)\nSome geography selection is by name\n\nAbility to map names to geospatial representations\n\nWould block-selector tool be helpful?\n\n+support for automatically suggesting block set mappings for non-standard geographic features as well as mapping to geographically-relevant names (within limited/scoped contexts)\n\n\n\n\nNote: What follows are additional key takeaways from an MDA discussion about the ways we can/cannot provide access to data‚Ä¶\nWe approve access to the following datasets, in the following ways:\n\nsch_broadband\n\nBEAD/BCAT (We stand-by the outputs/reports/etc.)\nBroadband Risk (We stand-by the outputs/reports/etc.)\nMDA produced data products (codebooks, maps, reports, sheets, etc.)\npgAdmin (YES, BUT we cannot QC or guarantee the resulting work product)\nQGIS (YES, BUT we cannot QC or guarantee the resulting work product)\nR (YES, BUT we cannot QC or guarantee the resulting work product)\n\nacs\n\npgAdmin (should we support wrangling a tidy dataset in SQL? NO)\nQGIS (should we support wrangling a tidy dataset in SQL? NO)\nR (YES, BUT we cannot QC or guarantee the resulting work product)\n\n\n\n\nNOTE: Fixed Broadband Availability Data comes from the FCC‚Äôs National Broadband Map  See the FCC exploratory data analysis notebook that Olivier‚Äôs maintains and updates as each additional data set is ingested: https://ruralinnovation.github.io/proj-fcc-report/  To perform place-based analysis on broadband access, service availability, and relative affordability, including broadband serviceable location data, we utilize the BDC Public Data Downloads repository and BDC Public Data API served by the FCC National Broadband Map. To date, we are not seeking, nor have we been granted access to the Broadband Serviceable Location Fabric dataset (sometimes referred to as the FCC Broadband Fabric, FCC Fabric, or Location Fabric) produced by CQA (CostQuest Associates).",
    "crumbs": [
      "Home",
      "Capabilities & Knowledge",
      "Broadband Consulting Tech Requirement"
    ]
  },
  {
    "objectID": "CORI-Data-API.html#requirements",
    "href": "CORI-Data-API.html#requirements",
    "title": "CORI Data API",
    "section": "Requirements",
    "text": "Requirements\n\nNodeJS 16.x+ - Installing NodeJS\nnpm 8.x+ - (needed for NPM Workspaces) - (should be installed as part of NodeJS installation)\nAWS CLI - Installing AWS CLI\nAWS SAM CLI Installing SAM CLI\nAWS CDK V2 - Installing AWS CDK V2\nPython 3.9+ - Installing Python\n\n\nNodeJS 16.x+ and NPM 8.x+\n\nThis project uses NPM Workspaces to managing multiple packages from your local file system from within a singular top-level, root package. NPM workspaces requires NPM version 8+. This should be installed as part of the installation of NodeJS 16.x+\n\nInstalling NodeJS\n\n\nAWS CLI\nThe AWS Command Line Interface (AWS CLI) is an open source tool that enables you to interact with AWS services using commands in your command-line shell. With minimal configuration, the AWS CLI enables you to start running commands that implement functionality equivalent to that provided by the browser-based AWS Management Console from the command prompt in your terminal program:\nInstalling AWS CLI\n\n\nAWS SAM CLI\nAWS SAM provides you with a command line tool, the AWS SAM CLI, that makes it easy for you to create and manage serverless applications. You need to install and configure a few things in order to use the AWS SAM CLI.\nInstalling SAM CLI\n\n\nAWS CDK V2\nThe AWS CDK lets you build reliable, scalable, cost-effective applications in the cloud with the considerable expressive power of a programming language. This approach yields many benefits, including:\n\nBuild with high-level constructs that automatically provide sensible, secure defaults for your AWS resources, defining more infrastructure with less code.\nUse programming idioms like parameters, conditionals, loops, composition, and inheritance to model your system design from building blocks provided by AWS and others.\nPut your infrastructure, application code, and configuration all in one place, ensuring that at every milestone you have a complete, cloud-deployable system.\nEmploy software engineering practices such as code reviews, unit tests, and source control to make your infrastructure more robust.\nConnect your AWS resources together (even across stacks) and grant permissions using simple, intent-oriented APIs.\nImport existing AWS CloudFormation templates to give your resources a CDK API.\nUse the power of AWS CloudFormation to perform infrastructure deployments predictably and repeatedly, with rollback on error.\nEasily share infrastructure design patterns among teams within your organization or even with the public.\n\nInstalling AWS CDK V2\n\n\nPython 3.9+\nInstalling Python\n\n\nAdditional Suggested Environment Setup\n\nA Node Version Manager Installing NVM or Installing n",
    "crumbs": [
      "Home",
      "Infrastructure",
      "CORI Data API"
    ]
  },
  {
    "objectID": "CORI-Data-API.html#installation-and-development",
    "href": "CORI-Data-API.html#installation-and-development",
    "title": "CORI Data API",
    "section": "Installation and Development",
    "text": "Installation and Development\n\nGetting started\n\nClone the repo\ngit clone https://github.com/ruralinnovation/cori-data-api.git\nChange into project directory\ncd cori-data-api\nInstall libraries for all packages\nnpm install\nSet local environment (shell) varibles:\n\n    $ export INTEGRATION_TESTING_USERNAME=&lt;aws-cognito-username&gt;\n    $ export INTEGRATION_TESTING_PASSWORD=&lt;aws-cognito-pasword&gt;\nAt this point, you should be able to build the api and run the test suite:\n$ npm run build\n$ npm run test",
    "crumbs": [
      "Home",
      "Infrastructure",
      "CORI Data API"
    ]
  },
  {
    "objectID": "CORI-Data-API.html#working-with-npm-workspaces",
    "href": "CORI-Data-API.html#working-with-npm-workspaces",
    "title": "CORI Data API",
    "section": "Working with NPM Workspaces",
    "text": "Working with NPM Workspaces\n\nWe suggest reading through the NPM Workspaces documentation before attempting to install any new packages or work with this repository.\n\nNPM Workspaces Documentation\nWorkspaces allows you to organize your code in a mono-repo with multiple packages (projects). There is a shared dependency tree to reduce build time and redundant packages.",
    "crumbs": [
      "Home",
      "Infrastructure",
      "CORI Data API"
    ]
  },
  {
    "objectID": "CORI-Data-API.html#prerequisites",
    "href": "CORI-Data-API.html#prerequisites",
    "title": "CORI Data API",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nUser/Role Setup\n\nCreate a READ_ONLY user\n\nLog in to database as admin with psql\nCreate read only role and new user with:\n\nCREATE ROLE read_only_access;\n\nGRANT CONNECT ON DATABASE (DB_NAME} TO read_only_access;\n\nGRANT USAGE ON SCHEMA public TO read_only_access;\n\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO read_only_access;\n\nGRANT SELECT ON ALL TABLES IN SCHEMA bcat TO read_only_access;\n\nCREATE USER read_only_user WITH PASSWORD  ________________;\n\nGRANT read_only_access TO read_only_user;\n\nKeep note of password for next step\n\n\n\nSave database credentials in AWS Parameter Store\n\nLog into AWS Console\nGo to Systems Manager/Parameter Store\nSave the database password in a parameter with a prefix e.g.¬†/cori/api/password\nKeep note of the parameter name",
    "crumbs": [
      "Home",
      "Infrastructure",
      "CORI Data API"
    ]
  },
  {
    "objectID": "CORI-Data-API.html#update-api-configuration-file",
    "href": "CORI-Data-API.html#update-api-configuration-file",
    "title": "CORI Data API",
    "section": "Update Api Configuration File",
    "text": "Update Api Configuration File\nThe ApiStackProps has an databaseConfig attribute with the following schema:\n\ninterface DatabaseConfig {\n  vpcId: string;\n  databaseSecurityGroupId: string;\n  host: string;\n  dbname: string;\n  dbuser: string;\n  parameterName: string;\n}\nOpen the main configuration file for your environment in config/config.ts. Update the attributes values with your database information.\nThe dbuser attribute value should be read_only_user if that is how you configured the previous section.\nThe paramaterName attribute value should be the name of the password parameter you stores in AWS Parameter Store in the previous step. (e.g.¬†/cori/api/password)",
    "crumbs": [
      "Home",
      "Infrastructure",
      "CORI Data API"
    ]
  },
  {
    "objectID": "CORI-Data-API.html#prerequisites-1",
    "href": "CORI-Data-API.html#prerequisites-1",
    "title": "CORI Data API",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nUser/Role Setup\ntodo",
    "crumbs": [
      "Home",
      "Infrastructure",
      "CORI Data API"
    ]
  },
  {
    "objectID": "CORI-Data-API.html#update-api-configuration",
    "href": "CORI-Data-API.html#update-api-configuration",
    "title": "CORI Data API",
    "section": "Update Api Configuration",
    "text": "Update Api Configuration\ntodo",
    "crumbs": [
      "Home",
      "Infrastructure",
      "CORI Data API"
    ]
  },
  {
    "objectID": "CORI-Data-API.html#starting-the-local-server",
    "href": "CORI-Data-API.html#starting-the-local-server",
    "title": "CORI Data API",
    "section": "Starting the Local Server",
    "text": "Starting the Local Server\n\nnpm run start",
    "crumbs": [
      "Home",
      "Infrastructure",
      "CORI Data API"
    ]
  },
  {
    "objectID": "CORI-Data-API.html#cicd-setup",
    "href": "CORI-Data-API.html#cicd-setup",
    "title": "CORI Data API",
    "section": "CICD Setup",
    "text": "CICD Setup\n\nGithub Setup\n\nCreate a new user in Github for CICD\nCreate a Personal Access Token for this user\nStore the Personal Access Token in AWS Secrets Manager with the name github-token",
    "crumbs": [
      "Home",
      "Infrastructure",
      "CORI Data API"
    ]
  },
  {
    "objectID": "CORI-Data-API.html#pipeline-infrastructure",
    "href": "CORI-Data-API.html#pipeline-infrastructure",
    "title": "CORI Data API",
    "section": "Pipeline Infrastructure",
    "text": "Pipeline Infrastructure\nThe Pipeline is configured using CDK Pipeline construct.\n\nCDK Pipelines is an opinionated construct library. It is purpose-built to deploy one or more copies of your CDK applications using CloudFormation with a minimal amount of effort on your part. It is not intended to support arbitrary deployment pipelines, and very specifically it is not built to use CodeDeploy to applications to instances, or deploy your custom-built ECR images to an ECS cluster directly: use CDK file assets with CloudFormation Init for instances, or CDK container assets for ECS clusters instead.\n\nCDK Pipelines Module\n\nPipeline code\nThe Pipeline code is located in the packages/infrastructure/src/stacks/PipelineStack.ts file.\nIn order to be deployed this stack requires configuration parameters for connecting to Github as well as deploying the ApiStack.\n\ninterface PipelineStackProps {\n  /**\n   * GitHub source configuration\n   */\n  source: {\n    /**\n     * Case-sensitive GitHub repo name\n     *  i.e. mergingfutures/cori-data-api\n     */\n    repo: string;\n    /**\n     * Which branch to listen on\n     * When changes are committed, the pipeline will trigger\n     */\n    branch: string;\n\n    /**\n     * Personal access token for authentication\n     *  i.e. cdk.SecretValue.secretsManager('mergingfutures-pat')\n     */\n    authentication: SecretValue;\n\n    /**\n     * How to trigger the pipeline.\n     *  Must have admin access on repo to use WEBHOOK.\n     *  Only read access is required for POLL\n     */\n    trigger?: GitHubTrigger;\n  };\n\n  /**\n   * Use this to re-use an existing S3 bucket.\n   */\n  artifactBucketName?: string;\n\n  /**\n   * Configures the api to be deployed by the pipeline\n   */\n  ApiConfig: ApiStackProps;\n\n  /**\n   * Credentials for Integration Testing\n   */\n  integrationConfig: {\n    userName: string;\n    password: string;\n  };\n}\n\n\nPipeline deployment\nEach pipeline is associated with a branch and an environment.\n\nWe have setup a DEV and PROD Pipeline for you, but you can have other pipelines connected to other branches as well.\n\n\nCheck out the associated branch.\nEnsure the branch code is pushed to the remote repo\nCreate a entry for the branch in config/configs\n\n\nBootstrapping\n\nIn the main pipeline account\nnpm run bootstrap:pipeline -- aws://{ACCOUNT-NUMBER}/{REGION} [--profile {PROFILE}]\nIn any other accounts (if using cross-account deploy)\nnpm run bootstrap:pipeline -- aws://{ACCOUNT-NUMBER}/{REGION} --trust {PIPELINE-ACCOUNT-NUMBER} [--profile {PROFILE}]\n\n\n\nDeploy the Pipeline\nEach pipeline is associated with a branch and an environment.\n\nCheck out the associated branch.\nCreate a entry for the branch in config/configs\nDeploy the pipeline\ncd packages/infrastructure\nnpm run deploy:pipeline -- [--profile {PROFILE}]\n\nOnce deployed, the pipeline will trigger on new commits to the associated branch.",
    "crumbs": [
      "Home",
      "Infrastructure",
      "CORI Data API"
    ]
  },
  {
    "objectID": "CORI-Data-API.html#directory-structure-1",
    "href": "CORI-Data-API.html#directory-structure-1",
    "title": "CORI Data API",
    "section": "Directory Structure",
    "text": "Directory Structure\n\n‚îú‚îÄ‚îÄ dependency-layer                        # Shared dependency/libraries layer\n‚îú‚îÄ‚îÄ bcat                                    # BCAT Service\n‚îú‚îÄ‚îÄ local                                   # Local Development service\n‚îî‚îÄ‚îÄ scaffolding                             # Scaffolding for new service (See Creating New Service Section)",
    "crumbs": [
      "Home",
      "Infrastructure",
      "CORI Data API"
    ]
  },
  {
    "objectID": "CORI-Data-API.html#python-dependency-layer",
    "href": "CORI-Data-API.html#python-dependency-layer",
    "title": "CORI Data API",
    "section": "Python Dependency Layer",
    "text": "Python Dependency Layer\nLambda layers provide a convenient way to package libraries and other dependencies that you can use with your Lambda functions. Using layers reduces the size of uploaded deployment archives and makes it faster to deploy your code.\nA layer is a .zip file archive that can contain additional code or data. A layer can contain libraries, a custom runtime, data, or configuration files. Layers promote code sharing and separation of responsibilities so that you can iterate faster on writing business logic.\nYou can use layers only with Lambda functions deployed as a .zip file archive. For functions defined as a container image, you package your preferred runtime and all code dependencies when you create the container image. For more information, see Working with Lambda layers and extensions in container images on the AWS Compute Blog.\nYou can create layers using the Lambda console, the Lambda API, AWS CloudFormation, or the AWS Serverless Application Model (AWS SAM). For more information about creating layers with AWS SAM, see Working with layers in the AWS Serverless Application Model Developer Guide.\nCreating and sharing Lambda Layers\nIt is import that you only include packages in your layer that a majority of the lambdas will use, as redundant libraries will increase container start time and reduce performance.\n\nYou are not limited to using a single layer in a lambda and can include up to 5 layers for each individual lambda. As the API grows you may find creating an assortment of dependency layers specific to certain typological functions is necessary. The total unzipped size of the function and all layers cannot exceed the unzipped deployment package size quota of 250 MB.\n\nThe dependencies for the Python Microservices are packaged and zipped in the packages/python-lambdas/dependency-layer directory.\nThis zipped filed is then deployed as a lambda layer dependency for all python lambdas. This will cut down on container start time by sharing these resources across many lambdas.\n\nIncluded Packages\n\npsycopg = ‚Äú^3.0.14‚Äù\npsycopg-binary = ‚Äú^3.0.14‚Äù\naws-lambda-powertools = ‚Äú^1.26.0‚Äù\n\n\nPsycopg & Psycopg-Binary\nPsycopg is the most popular PostgreSQL adapter for the Python programming language. Its core is a complete implementation of the Python DB API 2.0 specifications. Several extensions allow access to many of the features offered by PostgreSQL.\nDocumentation\n\n\nAWS Python Lambda Powertools\nWe leverage AWS Lambda Powertools library, which is a suite of utilities for AWS Lambda functions to ease adopting best practices such as tracing, structured logging, custom metrics, idempotency, batching, and more.\nCheck out this detailed blog post with a practical example.\nIn our experience it has a developer friendly (Flask-like) Api. It makes it very easy to configure routing/endpoints in each Python Microservice.\nFor more information READ THE DOCS\n\n\n\nAdding New Dependencies to the Layer\n# Change into the Python Microservices directory\ncd packages/python-lambdas\n\n# Activate the Python environment\nsource .env/bin/activate\n\n# Add new packages\npip install package1, package2\n\n# Copy Packages directory into the Dist directory\ncp -r ./.env/lib/python3.8/site-packages ./dist/python\n\n# Zip up the dist directory packages\n...",
    "crumbs": [
      "Home",
      "Infrastructure",
      "CORI Data API"
    ]
  },
  {
    "objectID": "CORI-Data-API.html#creating-new-services",
    "href": "CORI-Data-API.html#creating-new-services",
    "title": "CORI Data API",
    "section": "Creating New Services",
    "text": "Creating New Services\n\nCopy/paste scaffolding directory\nRename service directory and the name in the pyproject.toml file\nUpdate index.py with custom endpoints and logic.\n\n\nCreate a new ApiEndpoint in the ApiStack with the new service as handler for the endpoints. (see Creating a new Api Endpoint)\nPush changes to current branch to re-deploy with pipeline.\nCheck Integration Tests Step in CodePipeline interface.",
    "crumbs": [
      "Home",
      "Infrastructure",
      "CORI Data API"
    ]
  },
  {
    "objectID": "CORI-Data-API.html#core-apistack",
    "href": "CORI-Data-API.html#core-apistack",
    "title": "CORI Data API",
    "section": "Core ApiStack",
    "text": "Core ApiStack\nThe Root Stack for the entire API project is the ApiStack.\nThis stack is located at: packages/infrastructure/src/stacks/ApiStack.ts\nFamiliarize yourself with the ApiStackProps specified at the top of the file. These props are all the required and optional parameters that drive the configuration and deployment of the two APis and supporting lambdas, the networking, the hosting and the authentication.\n\nexport interface DatabaseConfig {\n  vpcId: string;\n  databaseSecurityGroupId: string;\n  host: string;\n  dbname: string;\n  dbuser: string;\n  parameterName: string;\n}\nexport interface CacheConfig {\n  host: string;\n  port: number;\n  username: string;\n  parameterName: string;\n  globalTTL: string;\n}\n\ninterface AppSyncUserPoolConfig {\n  userPoolId: string;\n}\n\nexport interface ServiceConfig {\n  /**\n   * The Logical Name of the service (NO SPACES) e.g. BCATService\n   */\n  logicalName: string;\n  /**\n   * The Core path to trigger the Microservice e.g. /bcat\n   */\n  corePath: string;\n  /**\n   * The name of the directory this service is located.  e.g. bcat\n   */\n  directoryName: string;\n}\n\ninterface AppSyncConfig {\n  /**\n   * Optional: When provided will configure additional user pools in the app sync authorization configuration\n   */\n  additionalUserPools: AppSyncUserPoolConfig[];\n}\n\nexport interface ApiStackProps extends StackProps {\n  env: {\n    account: string;\n    region: string;\n  };\n  client: string;\n  stage: string;\n  project: string;\n\n  loggingLevel: string;\n\n  /**\n   * Retain Dynamo Table and UserPool on delete\n   */\n  retain: boolean;\n\n  /**\n   * Database integration configuration\n   * Puts lambdas in VPC. Expecting VPC to be in another stack or deployed already.\n   * DB creds are accessed through parameter store and deployed as part of the lambda service environment.\n   *\n   */\n  databaseConfig: DatabaseConfig;\n\n  /**\n   *\n   */\n  cacheEnabled: boolean;\n  cacheConfig: CacheConfig;\n\n  /**\n   * Optional. When provided, will attach to existing Cognito for authentication.\n   */\n  existingCognito?: ExistingCognitoConfig;\n\n  microservicesConfig: ServiceConfig[];\n}",
    "crumbs": [
      "Home",
      "Infrastructure",
      "CORI Data API"
    ]
  },
  {
    "objectID": "CORI-Data-API.html#custom-constructs",
    "href": "CORI-Data-API.html#custom-constructs",
    "title": "CORI Data API",
    "section": "Custom Constructs",
    "text": "Custom Constructs\n\nNetworking Construct\nThe Networking Construct is responsible for creating a new Security Group for all of the Python Lambdas, and enabling communication between this new Lambda Security Group and the existing Database Security Group. This construct accepts the DatabaseConfig in order to instantiate the required resources.\n\nAll Python Lambdas are placed within your existing VPC and have no connection to the public internet.\nThe only INGRESS communication allowed to these lambdas are from Api Gateway on Core AWS Network.\nThe only EGRESS communication allowed from these lambdas is to the PostgreSQL Database and this is opened from the link between the two security groups.\nThis eliminates any security risks from the public internet.\n\n\nAWS CDK V2 Constructs Used\n\nVPC Construct\nSecurity Group Construct\n\n\n\nMore Information\n\nAmazon VPC\nSecurity Groups\n\n\n\n\nAuthentication (Cognito) Construct\nThe Authentication (Cognito) Construct is responsible for one of two things:\n\nIf you have an existing Cognito UserPool you want to use as a directory for controlling access to the APIs, this construct imports a reference to that UserPool and adds a new Postman client for development testing.\nOR\nIf you don‚Äôt have an existing UserPool, this construct creates a new UserPool, adds a new Authentication domain, and then adds the Postman client for development testing.\n\n\nThis construct is then passed into the ApiGateways in both servers (PythonDataServer & ApolloServer) for attaching CognitoAuthorizers as access controls.\n\n\nAWS CDK V2 Constructs Used\n\nUserPool Construct\nCfnUserPoolDomain Construct\nUserPoolClient Construct\n\n\n\nMore Information\n\nCognito User Pools\nUser Pool Domains\nUser Pool App Clients\n\n\n\n\nPython Data Server Construct\nThe Python Data Server Construct is responsible for:\n\nCreate the ApiGateway for your Python Microservices.\nDeploy the Dependency Layer\nDeploy new Python Microservices.\nCreate Endpoints on the ApiGateway to trigger new Microservices.\n\n\nCustom CDK V2 Constructs Used\n\nApiGw Construct\n\nCreates Api Gateway with Cognito Authorization. Has supporting methods for adding new endpoints with lambda triggers.\n\nPython Lambda Construct\n\nCreate a new Python Lambda (microservice) with an associated Log Group\n\n\n\n\n\nAWS CDK V2 Constructs Used\n\nRestApi\nCognitoUserPoolsAuthorizer.\nPythonFunction.\nLogGroup.\n\n\n\nMore Information\n\nWorking With Rest Apis\nControl access to a REST API using Amazon Cognito user pools as authorizer\nBuilding lambda functions with python\nWhat is Amazon CloudWatch Logs\n\n\n\n\nApollo (GraphQL) Server Construct\nThe Apollo GraphQL Server Construct is responsible for:\n\nCreate the ApiGateway for your GraphQL Server.\nDeploy a single NodeJS Lambda function to respond to GraphQL requests.\nCreate Endpoint on the ApiGateway to trigger lambda.\n\n\nCustom CDK V2 Constructs Used\n\nApiGw Construct\n\nCreates Api Gateway with Cognito Authorization. Has supporting methods for adding new endpoints with lambda triggers.\n\n\nAWS CDK V2 Constructs Used\n\nRestApi\nCognitoUserPoolsAuthorizer.\nNodejsFunction.\nLogGroup.\n\n\n\nMore Information\n\nWorking With Rest Apis\nControl access to a REST API using Amazon Cognito user pools as authorizer\nBuilding lambda functions with Typescript\nBuilding lambda functions with Node.js\nWhat is Amazon CloudWatch Logs\n\n\n\n\nHosting (CloudFront) Construct\nThe Hosting Construct is responsible for:\n\nCreate a Cloudfront Distribution.\nCreate origin on / path for Python RestApi\nCreate origin on /graphql path for Apollo Server.\nCreate bucket for access logs.\n\n\nAWS CDK V2 Constructs Used\n\nCloudFrontWebDistribution\nBucket\n\n\n\nMore Information\n\nWhat is Amazon Cloudfront\nOverview of distributions\nWorking with distributions\nUsing various origins with CloudFront distributions\nCloudFront logging\n\n\n\n\nResources\nCDK Day 2020 - Building Real-time Back Ends on AWS with AppSync and CDK\nNPM Workspaces\nGraphQL Tools\nAWS CDK V2\nGithub Actions\nSharing DB Snapshot between Accounts Sharing KMS KEY\nAmazon RDS PostgreSQL verses Amazon Aurora PostgreSQL\nTypescript / ESLint\nTypescript",
    "crumbs": [
      "Home",
      "Infrastructure",
      "CORI Data API"
    ]
  },
  {
    "objectID": "Configuring-our-AWS-CLI-(CDK-and-SAM)-Credentials.html",
    "href": "Configuring-our-AWS-CLI-(CDK-and-SAM)-Credentials.html",
    "title": "Prerequisite Steps",
    "section": "",
    "text": "The following has been extracted from AWS Docs (with added info for more clarity and context): CLI Configure profiles"
  },
  {
    "objectID": "Configuring-our-AWS-CLI-(CDK-and-SAM)-Credentials.html#new-user",
    "href": "Configuring-our-AWS-CLI-(CDK-and-SAM)-Credentials.html#new-user",
    "title": "Prerequisite Steps",
    "section": "New User",
    "text": "New User\n\nCreate an IAM user with Programmatic Access\n\n\n\n\nimage\n\n\n\nSpecify permissions and tags (keep in mind the permissions will control the CLI commands you can run)\nDownload the CSV file with the User Credentials and store it locally.\n\n\n\n\nimage\n\n\n\nYou will use the Access Key ID and Secret Access Key in the following sections."
  },
  {
    "objectID": "Configuring-our-AWS-CLI-(CDK-and-SAM)-Credentials.html#existing-user",
    "href": "Configuring-our-AWS-CLI-(CDK-and-SAM)-Credentials.html#existing-user",
    "title": "Prerequisite Steps",
    "section": "Existing User",
    "text": "Existing User\n\nOpen the AWS Console and navigate to the IAM service.\nClick Users and select the existing User\nOn the User screen, click Security credentials\n\n\n\n\nimage\n\n\n\nClick Create Access Key button in the Access Keys section\n\n\n\n\nimage\n\n\n\nDownload the CSV file with the User credentials and store it locally\n\n\n\n\nimage\n\n\n\nYou will use the Access Key ID and Secret Access Key in the following sections."
  },
  {
    "objectID": "Configuring-our-AWS-CLI-(CDK-and-SAM)-Credentials.html#credentials-profile",
    "href": "Configuring-our-AWS-CLI-(CDK-and-SAM)-Credentials.html#credentials-profile",
    "title": "Prerequisite Steps",
    "section": "Credentials Profile",
    "text": "Credentials Profile\nThe following example shows a credentials file with two profiles. The first [default] is used when you run a AWS CLI command with no profile. The second is used when you run a AWS CLI command with the ‚Äìprofile user1 parameter.\nThe credentials file uses a different naming format than the AWS CLI config file for named profiles. Do not use the word profile when creating an entry in the credentials file.\n~/.aws/credentials (Linux & Mac) or %USERPROFILE%\\.aws\\credentials (Windows)\n[default]\naws_access_key_id=AKIAIOSFODNN7EXAMPLE\naws_secret_access_key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n\n[user1]\naws_access_key_id=AKIAI44QH8DHBEXAMPLE\naws_secret_access_key=je7MtGbClwBF/2Zp9Utk/h3yCo8nvbEXAMPLEKEY"
  },
  {
    "objectID": "Configuring-our-AWS-CLI-(CDK-and-SAM)-Credentials.html#config-profile",
    "href": "Configuring-our-AWS-CLI-(CDK-and-SAM)-Credentials.html#config-profile",
    "title": "Prerequisite Steps",
    "section": "Config Profile",
    "text": "Config Profile\nEach profile can specify different credentials‚Äîperhaps from different IAM users‚Äîand can also specify different AWS Regions and output formats. When naming the profile in a config file, include the prefix word ‚Äúprofile‚Äù.\nThe following example specifies Region and output information for the default and user1 profiles.\n~/.aws/config (Linux & Mac) or %USERPROFILE%\\.aws\\config (Windows)\n[default]\nregion=us-west-2\noutput=json\n\n[profile user1]\nregion=us-east-1\noutput=text"
  },
  {
    "objectID": "Session-2-‚Äê-2023‚Äê10‚Äê23.html",
    "href": "Session-2-‚Äê-2023‚Äê10‚Äê23.html",
    "title": "Notes from Session 2",
    "section": "",
    "text": "Notes from Session 2"
  },
  {
    "objectID": "Example-React-Map-Container-with-REACT-MAP-GL-Package.html",
    "href": "Example-React-Map-Container-with-REACT-MAP-GL-Package.html",
    "title": "React Map Container Leveraging react-map-gl Package",
    "section": "",
    "text": "React Map Container Leveraging react-map-gl Package\nimport { ApolloProvider, useQuery } from '@apollo/client';\nimport { Box, Container } from '@mui/material';\nimport { useEffect, useRef, useState } from 'react';\nimport mapboxgl from 'mapbox-gl';\n\nimport {\n  broadband_unserved_blocks_geojson,\n  county_broadband_farm_bill_eligibility_geojson,\n  incumbent_electric_providers_geo_geojson,\n} from '../services/bcatQueries';\nimport Map, { Layer, Source, FillLayer, LineLayer } from 'react-map-gl';\n\n// For more information on data-driven styles, see https://www.mapbox.com/help/gl-dds-ref/\nexport const dataLayer1: LineLayer = {\n  id: 'county_broadband_farm_bill_eligibility_line_layer',\n  source: 'test',\n  type: 'line',\n  paint: {\n    'line-color': 'black',\n  },\n};\nexport const dataLayer2: FillLayer = {\n  id: 'county_broadband_farm_bill_eligibility_fill_layer',\n  source: 'test',\n  type: 'fill',\n  paint: {\n    'fill-color': '#0080ff', // blue color fill\n    'fill-opacity': 0.5,\n  },\n};\n\nfunction MapContainer() {\n  \n  const [viewport, setViewport] = useState();\n  \n  const { loading, error, data } = useQuery(county_broadband_farm_bill_eligibility_geojson, {\n    fetchPolicy: 'no-cache',\n    variables: {\n      state_abbr: 'TN',\n      skipCache: true,\n    },\n  });\n\n  useEffect(() =&gt; {\n    console.log('ERROR ', error);\n  }, [error]);\n\n  if (loading) return &lt;div&gt;Loading Data&lt;/div&gt;;\n\n\n  return (\n    &lt;Container maxWidth=\"xl\"&gt;\n      &lt;Box sx={{ bgcolor: '#cfe8fc', height: '100vh' }}&gt;\n        {data && (\n          &lt;Map\n            initialViewState={{\n              longitude: -86.503,\n              latitude: 35.562,\n              zoom: 7,\n            }}\n            style={{ width: '100%', height: '100vh' }}\n            mapStyle=\"mapbox://styles/mergingfutures/ckyn2t9jv0una14prs29fkgy2\"&gt;\n            &lt;Source type=\"geojson\" id=\"test\" data={data.county_broadband_farm_bill_eligibility_geojson}&gt;\n              &lt;Layer {...dataLayer1} /&gt;\n              &lt;Layer {...dataLayer2} /&gt;\n            &lt;/Source&gt;\n          &lt;/Map&gt;\n        )}\n      &lt;/Box&gt;\n    &lt;/Container&gt;\n  );\n}\n\nexport default MapContainer;\n\n\nExample GraphQL Query File for Apollo-Client\n\nNote: These queries are ALL available in the Postman Environment.\n\nimport { gql } from '@apollo/client';\n\nexport const broadband_unserved_blocks_geojson = gql`\n  query ($counties: [String], $skipCache: Boolean) {\n    broadband_unserved_blocks_geojson(counties: $counties, skipCache: $skipCache) {\n      type\n      features {\n        type\n        id\n        properties\n        geometry {\n          coordinates\n          type\n        }\n      }\n    }\n  }\n`;\n\nexport const broadband_unserved_blocks_county_geojson = gql`\n  query ($county: String!, $skipCache: Boolean) {\n    broadband_unserved_blocks_county_geojson(county: $county, skipCache: $skipCache) {\n      type\n      features {\n        type\n        id\n        properties\n        geometry {\n          coordinates\n          type\n        }\n      }\n    }\n  }\n`;\n\nexport const county_broadband_farm_bill_eligibility_geojson = gql`\n  query ($state_abbr: String!, $skipCache: Boolean) {\n    county_broadband_farm_bill_eligibility_geojson(state_abbr: $state_abbr, skipCache: $skipCache) {\n      type\n      features {\n        type\n        id\n        properties\n        geometry {\n          coordinates\n          type\n        }\n      }\n    }\n  }\n`;\n\nexport const incumbent_electric_providers_geo_geojson = gql`\n  query ($state_abbr: String!, $skipCache: Boolean) {\n    incumbent_electric_providers_geo_geojson(state_abbr: $state_abbr, skipCache: $skipCache) {\n      type\n      features {\n        type\n        id\n        properties\n        geometry {\n          coordinates\n          type\n        }\n      }\n    }\n  }\n`;"
  },
  {
    "objectID": "Metadata.html",
    "href": "Metadata.html",
    "title": "CORI/RISI Mapping & Data Analysis (MDA)",
    "section": "",
    "text": "The MDA tracks a variety of metadata on high-traffic tables. Metadata is information about the data that we are sourcing and/or creating, which may include: * Where the data came from (source) * When the data was acquired * What the table names mean * What the field names mean * Any information about how the data was created/generated/derived * and more‚Ä¶",
    "crumbs": [
      "Home",
      "Infrastructure",
      "What is Metadata?"
    ]
  },
  {
    "objectID": "Metadata.html#what-is-metadata",
    "href": "Metadata.html#what-is-metadata",
    "title": "CORI/RISI Mapping & Data Analysis (MDA)",
    "section": "",
    "text": "The MDA tracks a variety of metadata on high-traffic tables. Metadata is information about the data that we are sourcing and/or creating, which may include: * Where the data came from (source) * When the data was acquired * What the table names mean * What the field names mean * Any information about how the data was created/generated/derived * and more‚Ä¶",
    "crumbs": [
      "Home",
      "Infrastructure",
      "What is Metadata?"
    ]
  },
  {
    "objectID": "Metadata.html#when-should-metadata-be-updated",
    "href": "Metadata.html#when-should-metadata-be-updated",
    "title": "CORI/RISI Mapping & Data Analysis (MDA)",
    "section": "When should Metadata be updated?",
    "text": "When should Metadata be updated?\nMetadata should always be created for: * All active source data sets (data stored in database after ETL, not necessarily raw source data). * All product data sets (data sets exposed in a tool or MDA-derived data sets used across projects, i.e.¬†Climate Resiliency data) * Data produced for a specific (terminal) project does not necessarily require metadata, this is left to the discretion of the analyst.\nAdditionally, if you plan to expose the data set (schema + table within the PostgreSQL RDS instance) that you are working on to anyone else in the organization, you need to add Metadata.",
    "crumbs": [
      "Home",
      "Infrastructure",
      "What is Metadata?"
    ]
  },
  {
    "objectID": "Metadata.html#when-should-metadata-be-removed",
    "href": "Metadata.html#when-should-metadata-be-removed",
    "title": "CORI/RISI Mapping & Data Analysis (MDA)",
    "section": "When should Metadata be removed?",
    "text": "When should Metadata be removed?\nWhen source data is removed from the database.",
    "crumbs": [
      "Home",
      "Infrastructure",
      "What is Metadata?"
    ]
  },
  {
    "objectID": "Metadata.html#who-is-responsible-for-creating-and-updating-metadata",
    "href": "Metadata.html#who-is-responsible-for-creating-and-updating-metadata",
    "title": "CORI/RISI Mapping & Data Analysis (MDA)",
    "section": "Who is responsible for creating and updating metadata?",
    "text": "Who is responsible for creating and updating metadata?\nData is typically either source data or generated for specific products or projects. The MDA Data Engineer has primary responsibility for ensuring the existence of source metadata. For project or product specific data, the project or product owner has ultimate responsibility for creating metadata.",
    "crumbs": [
      "Home",
      "Infrastructure",
      "What is Metadata?"
    ]
  },
  {
    "objectID": "Metadata.html#how-to-update-metadata-via-coriverse",
    "href": "Metadata.html#how-to-update-metadata-via-coriverse",
    "title": "CORI/RISI Mapping & Data Analysis (MDA)",
    "section": "How to update Metadata via coriverse",
    "text": "How to update Metadata via coriverse\nFor instruction on creating and accessing MDA metadata, visit the cori_db wiki.",
    "crumbs": [
      "Home",
      "Infrastructure",
      "What is Metadata?"
    ]
  },
  {
    "objectID": "Metadata.html#works-on-metadata",
    "href": "Metadata.html#works-on-metadata",
    "title": "CORI/RISI Mapping & Data Analysis (MDA)",
    "section": "works on Metadata",
    "text": "works on Metadata\n\nglobal level view\nwe have: - tables in DB (data-prod / data ?), tables can support products and/or analysis or be temporary - a schema (multiple tables) that are referencing those information (metadata) - an R package (cori.db) that are helping updating Metadata - someone need to either fill a csv and/or write the description for the metadata to be complete - metadata can be a dependency on other tools - it has a ‚Äúvisualizer‚Äù: CORI explorer &lt;- dependencies on metadata function\nList of specific tables:\n# [1] \"acs_metadata\"         \"field_metadata\"      \n# [3] \"source_metadata\"      \"product_metadata\"    \n# [5] \"table_metadata\"       \"pipeline_diagnostics\"\n\n\ncori.db functions and a yaml dealing with metadata\nyaml (nst/params/package_params.yml) with the structure of 3 tables : - source_metadata - field_metadata - table_metadata:\nie only those 3 tables are supported by cori.db - create_metadata_tables() is the first function to run for new data set it has the yaml as dependency, can be run with a df or without, by default write 3 .csv, if a df is provided it will return one of the csv with the column name already filled\n\nupdate_metadata() take con, and need field_meta, table_meta, source_meta to exist and be valid df or exit the cols of the 3 must be same as yaml or stop if the data are already in metadata it will ask the user if he want to override it\n\n(Pot. improvement: it will break in a non interactive sessions)\nrepeat the process for the 3 tables\nglue produce multiple queries hence the need to Vectorize\nthen it deletes and append (times 3)\n\nread_db() (using pull_metadata) by default it will check if the table is present in 3 tables and save a 3 csv for them with select row\nacs metadata is updated here https://github.com/ruralinnovation/data-acs/blob/master/99_update_metadata.R it uses prev function update_metadata and DBI::dbWriteTable(con, ‚Äúacs_metadata‚Äù, codebook, overwrite = TRUE) be careful update_metadata is commented out in main branch\n\n\n\nTODO product to metadata\n\nmetadata in MVP state\nupdate our tools in production: BEAD / CH\ncori.utils check\nside not unsure if dplyr::add_rows should be added here",
    "crumbs": [
      "Home",
      "Infrastructure",
      "What is Metadata?"
    ]
  },
  {
    "objectID": "CORI-Ontology.html",
    "href": "CORI-Ontology.html",
    "title": "CORI/RISI Ontology",
    "section": "",
    "text": "is an ongoing initiative within the United States Census Bureau\nreleases a variety of data tables every year covering a wide range of social, economic, housing, and demographic data\n\n\n\n\n\nis the smallest geographic area for which the Bureau of the Census collects and tabulates decennial census data\nis the smallest statistical unit of analysis of a single Community‚Äôs economic participation, social impact, and inventory of tech ecosystem resources (?)\nis the smallest geospatial unit at which we can geographically place, filter or group Broadband Serviceable Location data\nis a collection of Broadband Serviceable Location(s)\ncan not be statistically described by data that is attributed to the block group, tract census boundaries that forming the set of blocks to which it belongs\nmay be a constituent component of one and only one block group\nis a constituent component of one and only one tract\n\n\n\n\n\nis ‚Ä¶\n\n\n\n\n\nStarted being used in 1910 (NY city), at first should follow rivers and roads and contains around 400 peoples.\nif divided between two new tracts and should follow this kind of pattern1: Census Tract O3 (parent) is divided in Census Tract 03.01 (child 1) and Census Tract 03.02.\n\n1¬†https://www.youtube.com/watch?v=DOe4alPmjss\nNote Olivier 26-04-2024: still unclear how that translate in geoid_tr and how do they deal when we move from two ‚Äúparents‚Äù to one ‚Äúchild‚Äù\n\n\nis a union of the set of blocks and/or block groups that are located within a specified geographical boundary (unsure about that do they start from block and build tract or tract and divide in block)\nis often arbitrary, except for coinciding with political lines, in unincorporated areas of the United States\nfor some metrics, can be statistically described by the aggregation of statistics for each of its constituent blocks\n\n\n\n\n\nis a region that is not governed by a local municipality, see also Census Designated Space (CDP)2\n\n2¬†https://www.census.gov/programs-surveys/bas/information/cdp.html\n\n\n\nis ‚Ä¶\n\n\n\n\n\nis ‚Ä¶\n\n\n\n\n\nis ‚Ä¶\n\n\n\n\nZIP (‚ÄúZone Improvment Plan‚Äù) code were made to help deliver mails (‚Äúdelivery routes‚Äù), they usually follow roads that were affected to a postman. At their smallest level they are tied to address (point). The Census Bureau is provisding a Zip Code Tabulation Area (ZCTA) that allow to link which is the dominant ZIP code in a census tabulation blocks3.\n3¬†https://www.census.gov/programs-surveys/geography/guidance/geo-areas/zctas.html\n\n\n\nis ‚Äúa business or residential location in the United States at which mass-market fixed broadband Internet access service is, or can be, installed.‚Äù4\n\n4¬†source: https://help.bdc.fcc.gov/hc/en-us/articles/16842264428059-About-the-Fabric-What-a-Broadband-Serviceable-Location-BSL-Is-and-Is-Not\nis a location that can have access to multiple ISPs, each offering different services (technologies, upload, download speed).\ncan be split into residential and business locations but this distinction does not seem very effective in the National Broadband Map dataset.\nis not geographically represented with exact latitude and longitude, but only by census block geoid and H3 index in the NBM dataset (MDA references this data as the National Broadband Map; officially the dataset is released as BDC (Broadband Data Collection) Public Data).\n\n\n\n\n\nis an ongoing initiative within the Federal Communications Commission\nprovides information about the internet services available to individual locations across the country, along with new maps of mobile coverage, as reported by Internet Service Providers (ISPs) in the FCC‚Äôs ongoing Broadband Data Collection.\n\n\n\n\n\nis a FCC provider (with a provider ID)?\n\nIn the FCC Broadband Map they are defined by a frn, provider_id and brand_name. We are mostly working with the one who are providing ‚Äúfixed‚Äù services.\n\n\n\n\nis ‚Ä¶\n\n\n\n\n\nis ‚Ä¶\n\n\n\n\n\nis ‚Ä¶\n\n\n\n\n\nis ‚Ä¶\n\n\n\n\n\nis ‚Ä¶\n\n\n\n\n\nis ‚Ä¶\n\n\n\n\n\nis ‚Ä¶\n\n\n\n\n\nis a remote store for one or more immutable artifacts of source data or outputs of product data\nis a system for storing, documenting, and indexing versions of data artifacts\n\n\n\n\n\nis a remote store for one or more mutable representations or views of a data product\n\n\nRural ‚Ä¶ ‚ÄúRural‚Äù is an adjective for the purpose of this list; it should be applied to other terms (i.e.¬†A Rural County ‚Ä¶)",
    "crumbs": [
      "Home",
      "Capabilities & Knowledge",
      "CORI/RISI Ontology"
    ]
  },
  {
    "objectID": "CORI-Ontology.html#general-terms",
    "href": "CORI-Ontology.html#general-terms",
    "title": "CORI/RISI Ontology",
    "section": "",
    "text": "is an ongoing initiative within the United States Census Bureau\nreleases a variety of data tables every year covering a wide range of social, economic, housing, and demographic data\n\n\n\n\n\nis the smallest geographic area for which the Bureau of the Census collects and tabulates decennial census data\nis the smallest statistical unit of analysis of a single Community‚Äôs economic participation, social impact, and inventory of tech ecosystem resources (?)\nis the smallest geospatial unit at which we can geographically place, filter or group Broadband Serviceable Location data\nis a collection of Broadband Serviceable Location(s)\ncan not be statistically described by data that is attributed to the block group, tract census boundaries that forming the set of blocks to which it belongs\nmay be a constituent component of one and only one block group\nis a constituent component of one and only one tract\n\n\n\n\n\nis ‚Ä¶\n\n\n\n\n\nStarted being used in 1910 (NY city), at first should follow rivers and roads and contains around 400 peoples.\nif divided between two new tracts and should follow this kind of pattern1: Census Tract O3 (parent) is divided in Census Tract 03.01 (child 1) and Census Tract 03.02.\n\n1¬†https://www.youtube.com/watch?v=DOe4alPmjss\nNote Olivier 26-04-2024: still unclear how that translate in geoid_tr and how do they deal when we move from two ‚Äúparents‚Äù to one ‚Äúchild‚Äù\n\n\nis a union of the set of blocks and/or block groups that are located within a specified geographical boundary (unsure about that do they start from block and build tract or tract and divide in block)\nis often arbitrary, except for coinciding with political lines, in unincorporated areas of the United States\nfor some metrics, can be statistically described by the aggregation of statistics for each of its constituent blocks\n\n\n\n\n\nis a region that is not governed by a local municipality, see also Census Designated Space (CDP)2\n\n2¬†https://www.census.gov/programs-surveys/bas/information/cdp.html\n\n\n\nis ‚Ä¶\n\n\n\n\n\nis ‚Ä¶\n\n\n\n\n\nis ‚Ä¶\n\n\n\n\nZIP (‚ÄúZone Improvment Plan‚Äù) code were made to help deliver mails (‚Äúdelivery routes‚Äù), they usually follow roads that were affected to a postman. At their smallest level they are tied to address (point). The Census Bureau is provisding a Zip Code Tabulation Area (ZCTA) that allow to link which is the dominant ZIP code in a census tabulation blocks3.\n3¬†https://www.census.gov/programs-surveys/geography/guidance/geo-areas/zctas.html\n\n\n\nis ‚Äúa business or residential location in the United States at which mass-market fixed broadband Internet access service is, or can be, installed.‚Äù4\n\n4¬†source: https://help.bdc.fcc.gov/hc/en-us/articles/16842264428059-About-the-Fabric-What-a-Broadband-Serviceable-Location-BSL-Is-and-Is-Not\nis a location that can have access to multiple ISPs, each offering different services (technologies, upload, download speed).\ncan be split into residential and business locations but this distinction does not seem very effective in the National Broadband Map dataset.\nis not geographically represented with exact latitude and longitude, but only by census block geoid and H3 index in the NBM dataset (MDA references this data as the National Broadband Map; officially the dataset is released as BDC (Broadband Data Collection) Public Data).\n\n\n\n\n\nis an ongoing initiative within the Federal Communications Commission\nprovides information about the internet services available to individual locations across the country, along with new maps of mobile coverage, as reported by Internet Service Providers (ISPs) in the FCC‚Äôs ongoing Broadband Data Collection.\n\n\n\n\n\nis a FCC provider (with a provider ID)?\n\nIn the FCC Broadband Map they are defined by a frn, provider_id and brand_name. We are mostly working with the one who are providing ‚Äúfixed‚Äù services.\n\n\n\n\nis ‚Ä¶\n\n\n\n\n\nis ‚Ä¶\n\n\n\n\n\nis ‚Ä¶\n\n\n\n\n\nis ‚Ä¶\n\n\n\n\n\nis ‚Ä¶\n\n\n\n\n\nis ‚Ä¶\n\n\n\n\n\nis ‚Ä¶\n\n\n\n\n\nis a remote store for one or more immutable artifacts of source data or outputs of product data\nis a system for storing, documenting, and indexing versions of data artifacts\n\n\n\n\n\nis a remote store for one or more mutable representations or views of a data product\n\n\nRural ‚Ä¶ ‚ÄúRural‚Äù is an adjective for the purpose of this list; it should be applied to other terms (i.e.¬†A Rural County ‚Ä¶)",
    "crumbs": [
      "Home",
      "Capabilities & Knowledge",
      "CORI/RISI Ontology"
    ]
  },
  {
    "objectID": "profile/about.html",
    "href": "profile/about.html",
    "title": "The Center on Rural Innovation",
    "section": "",
    "text": "The Center on Rural Innovation (CORI) is a 501(c)(3) nonprofit organization partnering with rural leaders across the country to build tech economies that support scalable entrepreneurship and lead to more tech jobs in rural America.\nThese repositories are maintained by the Mapping and Data Anaylytics (MDA) team at CORI/RISI.\nOur team provides data, analytics and visualizations to support rural participation in the digital economy through scalable entrepreneurship and tech job growth in rural America.\nWe strive towards serving as the primary center of excellence advancing sustainable, economic opportunity and equity in rural America. We do this by providing expert spatial and statistical analysis, modernized visualization and tool development, credible technical subject matter expertise, and transparent documentation.\nLearn more about the MDA approach to Knowledge here: Research, mapping, and data analytics"
  },
  {
    "objectID": "Support-Sessions.html",
    "href": "Support-Sessions.html",
    "title": "CORI/RISI Mapping & Data Analysis (MDA)",
    "section": "",
    "text": "Notes from support sessions"
  },
  {
    "objectID": "MDA-Style-Guide.html",
    "href": "MDA-Style-Guide.html",
    "title": "CORI/RISI Mapping & Data Analysis (MDA)",
    "section": "",
    "text": "This is the CORI/RISI MDA team style guide.\nThis is an evolving set of best practices for R programming on the MDA team. A template GitHub repo is available for new projects and provides suggestions for base structure and minimal documentation.",
    "crumbs": [
      "Home",
      "Team Onboarding",
      "Naming Conventions"
    ]
  },
  {
    "objectID": "MDA-Style-Guide.html#naming-conventions",
    "href": "MDA-Style-Guide.html#naming-conventions",
    "title": "CORI/RISI Mapping & Data Analysis (MDA)",
    "section": "Naming Conventions",
    "text": "Naming Conventions\n\nVariable and table names, both in code and on the database, should always be in snake_case, i.e.¬†all lower case with underscore separation of words.\nGithub repositories dedicated to a new project should begin with the prefix proj-. Similarly, repositories dedicated to ETL of a specific data set should begin with the prefix data-. All repositories should have clear, descriptive names.\nFunctions are verbs. Whenever possible use a descriptive verb or action phrase to name your function (e.g.¬†add_column(), rather than column_adder()).\nScripts that need to be run in a specific order should be prefixed with two digits and and an underscore, e.g.¬†01_, 02_ ‚Ä¶ 10_. Scripts numbered with a single digit will be displayed out of order in most file explorers when there are 10 or more scripts.",
    "crumbs": [
      "Home",
      "Team Onboarding",
      "Naming Conventions"
    ]
  },
  {
    "objectID": "MDA-Style-Guide.html#code",
    "href": "MDA-Style-Guide.html#code",
    "title": "CORI/RISI Mapping & Data Analysis (MDA)",
    "section": "Code",
    "text": "Code\n\nAlways use library() to load packages. require() is used in a lot of MDA legacy code, and should be replaced with library() when it is encountered.\nNeither library() nor require() should ever appear inside of functions.\n\nState package dependencies in your scripts explicitly at the top level using library().\n\nWhen creating functions that call tidyverse functions, use the embracing operator, e.g.¬†{ var }, instead of !!as.name(var) or similar.\n\nVariables inside the embracing operator should always be padded with a space on either side.\n\nThere is no such thing as over-commented code. To the extent possible, a script should be self-documenting. Be kind to your reviewers (and six-months-from-now you) and write the comments even when you‚Äôre in a hurry.\nAvoid while loops and nested loops of any kind.\n\nWhile both have applications, they are rare and there are almost always cleaner solutions that are easier to understand and reason about in the long run.\n\nAlways use a project-oriented workflow.\nA great deal of legacy MDA code prefixes all variables with x.. This practice should be avoided moving forward.\nSeparate tasks, such as downloading a source file and then processing that data into a usable table, should be separated into numbered scripts.\nWhitespace makes code more readable. Always pad assignment operators and equals signs (including within function calls!) with a single space on both sides. Separate commands with one or more new lines.\nThe assignment operator (&lt;-) is preferred to = for object creation, consistent with the tidyverse style guide.\nWhen in doubt about good formatting, defer to the tidyverse style guide",
    "crumbs": [
      "Home",
      "Team Onboarding",
      "Naming Conventions"
    ]
  },
  {
    "objectID": "MDA-Style-Guide.html#tools",
    "href": "MDA-Style-Guide.html#tools",
    "title": "CORI/RISI Mapping & Data Analysis (MDA)",
    "section": "Tools",
    "text": "Tools\n\nIf using RStudio, ensure that under Tools &gt;&gt; Global Options the option ‚ÄòRestore .RData into workspace at startup‚Äô is not checked and the option ‚ÄòSave workspace to .RData on exit‚Äô is set to Never. Using .RData to save presets and variables trades a minor convenience for major code portability headaches. Always start from a fresh session.",
    "crumbs": [
      "Home",
      "Team Onboarding",
      "Naming Conventions"
    ]
  },
  {
    "objectID": "MDA-Style-Guide.html#the-zen-of-functional-programming-the-zen-of-python",
    "href": "MDA-Style-Guide.html#the-zen-of-functional-programming-the-zen-of-python",
    "title": "CORI/RISI Mapping & Data Analysis (MDA)",
    "section": "The Zen of Functional Programming (The Zen of Python +)",
    "text": "The Zen of Functional Programming (The Zen of Python +)\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases aren‚Äôt special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity, refuse the temptation to guess.\nThere should be one‚Äì and preferably only one ‚Äìobvious way to do it.\nIf you find the one obvious way to do it, document it here for the sake of your coworkers and yourself in six months.\nNow is better than never.\nAlthough never is often better than right now.\nIf the implementation is hard to explain, it‚Äôs a bad idea.\nIf the implementation is easy to explain, it may be a good idea.\nCopy and paste is the root of 42% of all evil.\nIf you would copy and paste code a third time, instead write a function.\nIf you would copy and paste a function for the third time in a script, instead use a functional.\nAlthough a loop is better than nothing.\nIf you would copy a function into a new script even once, document it and add it to the coriverse.",
    "crumbs": [
      "Home",
      "Team Onboarding",
      "Naming Conventions"
    ]
  },
  {
    "objectID": "Ansible.html",
    "href": "Ansible.html",
    "title": "Ansible in the coriverse",
    "section": "",
    "text": "Ansible is an essential tool for managing our data infrastructure. Initially it was only employed to create new linux user accounts during MDA onboarding (see legacy playbooks), but ever since the great RDS Migration of 2023, ansible is the main way that we provision, save and restore data in bulk to the various databases (see PostgreSQL-RDS-Managment section in the wiki).\nAnsible playbooks are used to manage users on the team‚Äôs shared EC2 instance and databases instance within our RDS cluster.\ninventory/: stores our ansible hosts (CORI/RISI R server) see 1) for shell env. variable.\nplaybooks/: contains various playbooks:\nplaybooks/legacy/: playbooks for managing EC2 instance and adding users. Adding new users worked in January 2023. The old readme was also added here.\nplaybooks/basic/: provides a playbook to test Ansible and seeing if the connection to some target host works.\nplaybooks/cori-risi-ad-postgresql/: playbooks to connect to the Active-Directory-secured RDS cluster and manage database instances and users.\nplaybooks/cori-risi-old-db/: playbooks that were used to connect to the now defunct ‚Äúold‚Äù database (in a terminated RDS cluster).\nplaybooks/kerberos/: playbook to verify the ability to authenticate with Active Directory\nplaybooks/postgresql/: before running playbooks against the database instances on the new RDS cluster we tested them on the local PostgreSQL instance running on our shared EC2 instance.\nplaybooks/queries/: SQL helper/utility scripts.\npackage.json: npm is also used here as a way to store some procedures and save some time and brain space (see as an example: npm run clean).\n\n\nSo far, this works (October 2023):\n\nAssign appropriate values for the ssh user and private key file to the shell environment variables, ANSIBLE_SSH_USER and ANSIBLE_SSH_PRIVATE_KEY_FILE:\n\n# export your name from EC2 instance\nexport ANSIBLE_SSH_USER=&lt;USER_NAME&gt;\n# check ls ~/.ssh to know which key use\nexport ANSIBLE_SSH_PRIVATE_KEY_FILE=&lt;~/.ssh/SSH_PRIVATE_KEY_FILE&gt;\n\nRun npm install or manually write a simple, local hosts file (hosts):\n\n   [cori_risi_r_server]\n   18.235.239.47\n\nRun the basic ansible ping command with command line arguments for the name of the ssh user and the path to the ssh private key file:\n\nansible cori_risi_r_server -m ping -i hosts --extra-vars \"ansible_ssh_user=$ANSIBLE_SSH_USER\" --extra-vars \"ansible_ssh_private_key_file=$ANSIBLE_SSH_PRIVATE_KEY_FILE\"\nYou should be prompted with:\n18.235.239.47 | SUCCESS =&gt; {\n    \"ansible_facts\": {\n        \"discovered_interpreter_python\": \"/usr/bin/python3\"\n    },\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\n\n\nAlternatively, this repo also includes a hosts.yml file in the inventory subdirectory, which can be supplied to the ansible commands with -i inventory/hosts.yml:\n\n$ ansible cori_risi_r_server -i inventory/hosts.yml -a \"/bin/echo hello\"\n\nNotes: To run ansible playbooks on the R server with nohup you can use this sequence of shell commands (from the ansible project directory):\nread -s password\n******\necho $password &gt; p.temp\nnohup bash -c 'ansible-playbook -i inventory/hosts.yml playbooks/cori-risi-ad-postgresql/main.yml --extra-vars \"db_database=data\" --extra-vars \"password_file=$(pwd)/p.temp\"' &gt; cori-risi-ad-postgresql.log &\nRemember to delete the p.temp file afterwards.",
    "crumbs": [
      "Home",
      "Infrastructure",
      "Ansible in the coriverse"
    ]
  },
  {
    "objectID": "Ansible.html#set-up-ansible",
    "href": "Ansible.html#set-up-ansible",
    "title": "Ansible in the coriverse",
    "section": "",
    "text": "So far, this works (October 2023):\n\nAssign appropriate values for the ssh user and private key file to the shell environment variables, ANSIBLE_SSH_USER and ANSIBLE_SSH_PRIVATE_KEY_FILE:\n\n# export your name from EC2 instance\nexport ANSIBLE_SSH_USER=&lt;USER_NAME&gt;\n# check ls ~/.ssh to know which key use\nexport ANSIBLE_SSH_PRIVATE_KEY_FILE=&lt;~/.ssh/SSH_PRIVATE_KEY_FILE&gt;\n\nRun npm install or manually write a simple, local hosts file (hosts):\n\n   [cori_risi_r_server]\n   18.235.239.47\n\nRun the basic ansible ping command with command line arguments for the name of the ssh user and the path to the ssh private key file:\n\nansible cori_risi_r_server -m ping -i hosts --extra-vars \"ansible_ssh_user=$ANSIBLE_SSH_USER\" --extra-vars \"ansible_ssh_private_key_file=$ANSIBLE_SSH_PRIVATE_KEY_FILE\"\nYou should be prompted with:\n18.235.239.47 | SUCCESS =&gt; {\n    \"ansible_facts\": {\n        \"discovered_interpreter_python\": \"/usr/bin/python3\"\n    },\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\n\n\nAlternatively, this repo also includes a hosts.yml file in the inventory subdirectory, which can be supplied to the ansible commands with -i inventory/hosts.yml:\n\n$ ansible cori_risi_r_server -i inventory/hosts.yml -a \"/bin/echo hello\"\n\nNotes: To run ansible playbooks on the R server with nohup you can use this sequence of shell commands (from the ansible project directory):\nread -s password\n******\necho $password &gt; p.temp\nnohup bash -c 'ansible-playbook -i inventory/hosts.yml playbooks/cori-risi-ad-postgresql/main.yml --extra-vars \"db_database=data\" --extra-vars \"password_file=$(pwd)/p.temp\"' &gt; cori-risi-ad-postgresql.log &\nRemember to delete the p.temp file afterwards.",
    "crumbs": [
      "Home",
      "Infrastructure",
      "Ansible in the coriverse"
    ]
  },
  {
    "objectID": "React,-AWS-Amplify-and-API-Usage.html",
    "href": "React,-AWS-Amplify-and-API-Usage.html",
    "title": "Configuring Amplify for different envs",
    "section": "",
    "text": "You will need to create a config file in the public directory of your hosted application. This file will house attributes from your deployed API (e.g.¬†UserPoolId and the UserPool domain). When a user navigates to your application the Configuration Service will read this config as a static file and configure the Amplify Library to authenticate your application\n{\n  \"environment\": \"dev\",\n  \"region\": \"us-east-1\",\n  \"apiUrl\": \"https://d25ssrwsq4u9bu.cloudfront.net\",\n  \"cognito\": {\n    \"clientId\": \"6um99fv2qtb6f7ise3i037vna\",\n    \"domain\": \"mf-data-api-dev.auth.us-east-1.amazoncognito.com\",\n    \"userPoolId\": \"us-east-1_NE91zaapX\"\n  }\n}\n\n\n\nYou can create a ConfigurationService, which is responsible for reading the config file, storing attributes in state, and setting up Amplify Auth Library. You can import this service in an ApiContext.\nimport { Auth, Logger } from 'aws-amplify';\n\ninterface ICognitoConfig {\n  userPoolId: string;\n  clientId: string;\n  identityPoolId: string;\n  domain: string;\n  hostedAuthenticationUrl: string;\n  logoutUrl: string;\n}\n\nexport default class ConfigurationService {\n  private logger: Logger;\n  loaded = false;\n\n  // These properties are assigned from config.json\n  environment: string = '';\n  region: string = '';\n  apiUrl: string = '';\n  cognito: ICognitoConfig = {\n    userPoolId: '',\n    clientId: '',\n    identityPoolId: '',\n    domain: '',\n    hostedAuthenticationUrl: '',\n    logoutUrl: '',\n  };\n  version: string = '';\n\n  constructor() {\n    this.logger = new Logger('ConfigurationService');\n  }\n\n  public async load() {\n    const response = await fetch('/config.json');\n    const cfg = await response.json();\n    Object.assign(this, cfg);\n    console.log('Config Loaded');\n    this.configureAmplify();\n    this.loaded = true;\n  }\n\n  get loginUrl(): string {\n    return window.location.origin;\n  }\n\n  isLocal(): boolean {\n    return this.environment === 'local';\n  }\n  isDev(): boolean {\n    return this.environment === 'dev';\n  }\n\n  private configureAmplify(): void {\n    Logger.LOG_LEVEL = this.isLocal() || this.isDev() ? 'DEBUG' : 'INFO';\n    const options = {\n      Analytics: {\n        disabled: true,\n      },\n      Auth: {\n        region: this.region,\n        userPoolId: this.cognito.userPoolId,\n        userPoolWebClientId: this.cognito.clientId,\n        oauth: {\n          domain: this.cognito.domain,\n          scope: ['email', 'openid', 'profile'],\n          redirectSignIn: this.loginUrl,\n          redirectSignOut: this.cognito.logoutUrl ? this.cognito.logoutUrl : this.loginUrl + '/logout/',\n          responseType: 'code',\n          mandatorySignIn: true,\n        },\n      },\n    };\n\n    Auth.configure(options);\n  }\n}"
  },
  {
    "objectID": "React,-AWS-Amplify-and-API-Usage.html#config.json",
    "href": "React,-AWS-Amplify-and-API-Usage.html#config.json",
    "title": "Configuring Amplify for different envs",
    "section": "",
    "text": "You will need to create a config file in the public directory of your hosted application. This file will house attributes from your deployed API (e.g.¬†UserPoolId and the UserPool domain). When a user navigates to your application the Configuration Service will read this config as a static file and configure the Amplify Library to authenticate your application\n{\n  \"environment\": \"dev\",\n  \"region\": \"us-east-1\",\n  \"apiUrl\": \"https://d25ssrwsq4u9bu.cloudfront.net\",\n  \"cognito\": {\n    \"clientId\": \"6um99fv2qtb6f7ise3i037vna\",\n    \"domain\": \"mf-data-api-dev.auth.us-east-1.amazoncognito.com\",\n    \"userPoolId\": \"us-east-1_NE91zaapX\"\n  }\n}"
  },
  {
    "objectID": "React,-AWS-Amplify-and-API-Usage.html#sample-config-service",
    "href": "React,-AWS-Amplify-and-API-Usage.html#sample-config-service",
    "title": "Configuring Amplify for different envs",
    "section": "",
    "text": "You can create a ConfigurationService, which is responsible for reading the config file, storing attributes in state, and setting up Amplify Auth Library. You can import this service in an ApiContext.\nimport { Auth, Logger } from 'aws-amplify';\n\ninterface ICognitoConfig {\n  userPoolId: string;\n  clientId: string;\n  identityPoolId: string;\n  domain: string;\n  hostedAuthenticationUrl: string;\n  logoutUrl: string;\n}\n\nexport default class ConfigurationService {\n  private logger: Logger;\n  loaded = false;\n\n  // These properties are assigned from config.json\n  environment: string = '';\n  region: string = '';\n  apiUrl: string = '';\n  cognito: ICognitoConfig = {\n    userPoolId: '',\n    clientId: '',\n    identityPoolId: '',\n    domain: '',\n    hostedAuthenticationUrl: '',\n    logoutUrl: '',\n  };\n  version: string = '';\n\n  constructor() {\n    this.logger = new Logger('ConfigurationService');\n  }\n\n  public async load() {\n    const response = await fetch('/config.json');\n    const cfg = await response.json();\n    Object.assign(this, cfg);\n    console.log('Config Loaded');\n    this.configureAmplify();\n    this.loaded = true;\n  }\n\n  get loginUrl(): string {\n    return window.location.origin;\n  }\n\n  isLocal(): boolean {\n    return this.environment === 'local';\n  }\n  isDev(): boolean {\n    return this.environment === 'dev';\n  }\n\n  private configureAmplify(): void {\n    Logger.LOG_LEVEL = this.isLocal() || this.isDev() ? 'DEBUG' : 'INFO';\n    const options = {\n      Analytics: {\n        disabled: true,\n      },\n      Auth: {\n        region: this.region,\n        userPoolId: this.cognito.userPoolId,\n        userPoolWebClientId: this.cognito.clientId,\n        oauth: {\n          domain: this.cognito.domain,\n          scope: ['email', 'openid', 'profile'],\n          redirectSignIn: this.loginUrl,\n          redirectSignOut: this.cognito.logoutUrl ? this.cognito.logoutUrl : this.loginUrl + '/logout/',\n          responseType: 'code',\n          mandatorySignIn: true,\n        },\n      },\n    };\n\n    Auth.configure(options);\n  }\n}"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Enter the coriverse!",
    "section": "",
    "text": "We are building a one-of-a-kind, data-driven platform that can provide the whole-systems, integrated model that is necessary to describe and explain the place that Rural inhabits within the broader social and economic fabric of the United States. The ultimate goal of those efforts is to energize and guide local ingenuity and leadership within rural communities as they confront the overwhelming organizational challenges involved in the development of an emerging local tech economy. This synthesis has the potential to substantially shift the cultural, intellectual and material/capital contributions of rural people living in rural places.\nAs the Mapping and Data Analytics team, we are responsible for the foundational data and methodology that underlies any explanatory model that is produced or data product that is delivered and/or published by the Center on Rural Innovation and Rural Innovation Strategies, Inc., respectively. Inherit in engaging with this responsibility is an ongoing need to map the knowledge that is generated and/or captured by our work to the basic terms of which that knowledge is composed. The collective and individual professional experience of the MDA team provides a wide breadth of analytical insights into the data that we use, but much of this value is implicit in the final output of our work product, rather than intentionally and continuously integrated into the system on which our work is occurring. As much as possible, we should aim to build a system or a platform for our work that maximizes our ability to sustain and improve our models, as well as the organization‚Äôs ability to intelligently utilize our models in pursuit of our mission.\nNote: DO NOT EDIT THIS README IN THE coriverse REPO. It is derived from README.qmd in the wiki repo."
  },
  {
    "objectID": "index.html#initial-setup",
    "href": "index.html#initial-setup",
    "title": "Enter the coriverse!",
    "section": "Initial Setup",
    "text": "Initial Setup\nThis package can be installed by using devtools or remotes.\n\nSetup for Development\nOnce you have all the dependencies installed, to build and install this package from the local project directory, run:\npkgbuild::clean_dll(); pkgbuild::compile_dll(); devtools::document(); devtools::check(); devtools::install();\n\n\nWhy R\nR is used for several reasons. The first one is probably historical: at the beginning the team had a lot of R users. Now R is used as our main ‚Äúglue‚Äù language. It fills the gap between the data analysis requirements and the data engineering and software development requirements of our various projects.\nWe do not have a full IT department and a lot of our solutions are ‚Äúhand made‚Äù we cannot afford to spend times managing conda/venv setup. R, thanks to CRAN, offers a good selection of packages without a need for too much investment.\nR also offers a mature geospatial data ecosystem, wrappers around API(s) for our data sources (e.g., US census data), literate programming experience with Rmarkdown and Quarto, great data visualizations packages (i.e., ggplot2, shiny), and is probably on top for statistical analysis and modeling.\nRstudio is a good and free IDE that can be run on a server or desktop, but R also integrates with VScode or emacs.\nFinally, the strong focus on producing good documentation and a lively community make it a fun ecosystem to interact with!\n\n\nInstall R on Mac OS X\nIf you are installing the R language on your system for the first time, we recommend that you do not use the homebrew client to manage your installation. You should begin by ensuring that the command-line component for Xcode is available on your system and that GNU Fortran has been installed from this source:\n\ngfortran-12.2-universal.pkg\nXcode\n\nOnce these prerequisites are installed, head over to CRAN and download the latest version of R for macOS 11 (and higher; R version 4.3.2 as of the last update to these instructions).\n\n\nEnvironment variables\nAfter installing R, you need to first set up local environment variables, either through your shell profile (preferably) or with an .Renviron file.\n~/.profile:\nexport GITHUB_USER='&lt;your-github-user-name&gt;'\nexport GITHUB_PAT='&lt;your-github-personal-authentication-token&gt;'\n~/.Renviron:\nGITHUB_USER=\"&lt;your-github-user-name&gt;\"   \nGITHUB_PAT=\"&lt;your-github-personal-authentication-token&gt;\"\n\n\nInstallation\ncoriverse is an R metapackage, allowing us to conveniently install component R packages that address different pieces of the MDA workflow. To install, use the following steps:\n\nEnsure you have :package: remotes 2.4.0 or greater installed (current version as of June 2021) and usethis. Use install.packages('remotes'); install.packages('usethis') to get the latest version.\nCreate a GitHub token:\n\n    ## (optional, if not previously done) set your user name and email:\n    # usethis::use_git_config(user.name = \"YourName\", user.email = \"your@mail.com\")\n    \n    ## create a personal access token for authentication:\n    usethis::create_github_token() \n    ## in case usethis version &lt; 2.0.0: usethis::browse_github_token() (or even better: update usethis!)\n    ## 2023-01-25: it opens the default web browser at github PAT web page see 1.\n\n    ## set personal access token:\n    credentials::set_github_pat(\"ghp_...\")\n\nSet an environment variable called GITHUB_PAT by running Sys.setenv(GITHUB_PAT = 'MY_TOKEN_HERE'), replacing MY_TOKEN_HERE with the valid GitHub personal access token you previously created. Instructions for creating a Personal Access Token are available in GitHub‚Äôs documentation. Your PAT only needs repo permissions (the first section of options when creating a PAT).\n\n\n\n\nscope\n\n\n\nUse the install_github() function to install the coriverse package(s), which will look for the environment variable GITHUB_PAT and will allow you to install packages from private repos. Call:\n\n    remotes::install_github('ruralinnovation/coriverse')\nFor instructions on programmatically connecting to the database using the coriverse, see cori.db"
  },
  {
    "objectID": "index.html#setup-for-development-1",
    "href": "index.html#setup-for-development-1",
    "title": "Enter the coriverse!",
    "section": "Setup for Development",
    "text": "Setup for Development\nOnce you have all the dependencies installed, to build and install this package from the local project directory, run:\npkgbuild::clean_dll(); pkgbuild::compile_dll(); devtools::document(); devtools::check(); devtools::install();"
  },
  {
    "objectID": "onboarding_team_db.html",
    "href": "onboarding_team_db.html",
    "title": "CORI/RISI Mapping & Data Analysis (MDA)",
    "section": "",
    "text": "Software requirement: pgAdmin 4 &gt; v7\n\nMost CORI Computer are Mac: https://www.postgresql.org/ftp/pgadmin/pgadmin4/v7.8/macos/\n\nYour IP address should have been whitelisted, if not contact John or Olivier.\n\n\n\n\nRight clic on Server -&gt; Register -&gt; Server\nNew windows pop up:\n\n\nName: cori-ad\nIn the Connection tab Host name/address: cori-risi-ad-postgresql.c6zaibvi9wyg.us-east-1.rds.amazonaws.com\nUsername: your username (your email)\nPassword: your password\nWhat do you have in Parameters‚Äôs tab? if not add SSL mode as prefer\n\n\nOpen the ‚ÄúQuery tool‚Äù and enter:\n\nThe ‚ÄúQuery tool‚Äù look like a small silo:\n\nTo turn the ‚ÄúQuery Tool‚Äù from ‚Äúgrey‚Äù to ‚Äúblack‚Äù select the postgres DB.\nThen enter:\nALTER USER \"your_username\" WITH PASSWORD 'my_secret_pwd';\nYour username need to be double quoted.\nWhen it displays ‚ÄúQuery returned successfully‚Äù you should be able to save this password.\nThen you can disconnect from the server and when you reconnect it will ask for your password and you can save it from here:\n\n\n\n\nRefer to the coriverse wiki and cori.db readme for more details instructions on installing this package.\nremotes::install_github(\"ruralinnovation/cori.db\")\npackageVersion(\"cori.db\")\n# [1] ‚Äò0.2.0‚Äô\ncori.db::set_db_credentials(\"your_username\", \"my_secret_pwd\")\n# Restart the R session \n\n\n\nmda_team users do not have permission to create schema.\nWhen creating a new schema to allow everyone in a team to access it the ownership need to be changed.\nALTER SCHEMA \"my_schema\" OWNER TO mda_team;",
    "crumbs": [
      "Home",
      "Team Onboarding",
      "Database Onboarding"
    ]
  },
  {
    "objectID": "onboarding_team_db.html#database-onboarding",
    "href": "onboarding_team_db.html#database-onboarding",
    "title": "CORI/RISI Mapping & Data Analysis (MDA)",
    "section": "",
    "text": "Software requirement: pgAdmin 4 &gt; v7\n\nMost CORI Computer are Mac: https://www.postgresql.org/ftp/pgadmin/pgadmin4/v7.8/macos/\n\nYour IP address should have been whitelisted, if not contact John or Olivier.\n\n\n\n\nRight clic on Server -&gt; Register -&gt; Server\nNew windows pop up:\n\n\nName: cori-ad\nIn the Connection tab Host name/address: cori-risi-ad-postgresql.c6zaibvi9wyg.us-east-1.rds.amazonaws.com\nUsername: your username (your email)\nPassword: your password\nWhat do you have in Parameters‚Äôs tab? if not add SSL mode as prefer\n\n\nOpen the ‚ÄúQuery tool‚Äù and enter:\n\nThe ‚ÄúQuery tool‚Äù look like a small silo:\n\nTo turn the ‚ÄúQuery Tool‚Äù from ‚Äúgrey‚Äù to ‚Äúblack‚Äù select the postgres DB.\nThen enter:\nALTER USER \"your_username\" WITH PASSWORD 'my_secret_pwd';\nYour username need to be double quoted.\nWhen it displays ‚ÄúQuery returned successfully‚Äù you should be able to save this password.\nThen you can disconnect from the server and when you reconnect it will ask for your password and you can save it from here:\n\n\n\n\nRefer to the coriverse wiki and cori.db readme for more details instructions on installing this package.\nremotes::install_github(\"ruralinnovation/cori.db\")\npackageVersion(\"cori.db\")\n# [1] ‚Äò0.2.0‚Äô\ncori.db::set_db_credentials(\"your_username\", \"my_secret_pwd\")\n# Restart the R session \n\n\n\nmda_team users do not have permission to create schema.\nWhen creating a new schema to allow everyone in a team to access it the ownership need to be changed.\nALTER SCHEMA \"my_schema\" OWNER TO mda_team;",
    "crumbs": [
      "Home",
      "Team Onboarding",
      "Database Onboarding"
    ]
  },
  {
    "objectID": "R-and-Python-API-Usage.html",
    "href": "R-and-Python-API-Usage.html",
    "title": "Summary",
    "section": "",
    "text": "The purpose of this document is to describe the general functionality of the API for data science purposes, with attention to integration with geospatial libraries in R and Python. Comments on general performance and suggested best practices for using the API resources are below.\n\n\n\nTesting the default queries from R using httr::GET() returns GeoJSON that can be readily accessed by R‚Äôs spatial libraries. We‚Äôll first make a request to the API and check its performance:\nlibrary(httr)\n\nheaders = c(\n    'api' = 'Xy4Gq51ixT3OYvZPqhb3D5OdemA3BYZ06ISg5Duo',\n    'Access' = '',\n    'Authorization' = 'Bearer\neyJraWQiOiJJRzhDcXFYenEzR1dGWTk5bUdwK0VkZzVNbUNLa1RCSXpJT3dJdFdQc\nmZzPSIsImFsZyI6IlJTMjU2In0.eyJzdWIiOiJjYmMyNWY4ZS01MmE3LTQwZDUtYm\nJlMS0wYmYwYTI5N2MxMGIiLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZSwiaXNzIjoiaHR\n0cHM6XC9cL2NvZ25pdG8taWRwLnVzLWVhc3QtMS5hbWF6b25hd3MuY29tXC91cy1l\nYXN0LTFfTkU5MXphYXBYIiwiY29nbml0bzp1c2VybmFtZSI6Im1mLWludC10ZXN0Q\nHlvcG1haWwuY29tIiwib3JpZ2luX2p0aSI6IjcxMWI4NWZkLTdkZmUtNDQ4Zi1hMD\nk0LTBlMjdjNzIzYjc0YSIsImF1ZCI6IjZ1bTk5ZnYycXRiNmY3aXNlM2kwMzd2bmE\niLCJldmVudF9pZCI6IjZjYWNkYzQ4LTE2YTEtNDcyNi04YTYyLWJmZmVlZGJjZjE2\nNiIsInRva2VuX3VzZSI6ImlkIiwiYXV0aF90aW1lIjoxNjU3MzI1MzAyLCJleHAiO\njE2NTczMjg5MDIsImlhdCI6MTY1NzMyNTMwMiwianRpIjoiMjZlMzUzMGYtMTc3Mi\n00NzUzLTk1ZDgtNDM5NjE5N2UxM2YzIiwiZW1haWwiOiJtZi1pbnQtdGVzdEB5b3B\ntYWlsLmNvbSJ9.ZNnTbwnBTQfC57G9zQnVQO3gFZh9IPh9Z4hJ85vCmfCpody0bBj\nyAGKWH8at26TscM6OZJzs52HFAGxhGUf6chCIqQUlXNB2q3UwuHd7qh9imA0EESfMt70V7nxRTS17BXXAwiMZhPgngk29mTQm9M4psL6n7z\nDr0heEwS7bOGH9Nzp55hdNWWEHKPGQr81mFloXJnhQiyMAHC3BkJx_\ncBiTSAQcFcqqPJW1iH7lyC9thmsboxOlLG9IK6R5J\nWR2uz05CJvp49L3XneAG-IWw3OSQuVi81UFIcv5RnAgi3MuLG6J8uS9sIa6QNSHxxLYicURRZC7ak0mVBvlaVA'\n)\n\ntictoc::tic()\nrequest &lt;- GET(url = \"https://d25ssrwsq4u9bu.cloudfront.net/bcat/\ncounty_broadband_farm_bill_eligibility/geojson?state_abbr=TN\",\nadd_headers(headers))\ntictoc::toc()\n\n10.996 sec elapsed \nIn loading into R, the farm bill eligibility data tends to load in between 10 and 16 seconds. This is likely fine for data science purposes, though you may prefer to use the MVT endpoint for dynamic querying of tiles in an interactive app context. The county-specific datasets load in a second or less, even when a few counties are requested.\nThe request can be read into R as a simple features object and mapped for review:\nlibrary(sf)\nlibrary(mapview)\n\nrequest_geo &lt;- request %&gt;%\n   content(as = \"text\") %&gt;%\n   st_read(quiet = TRUE)\n\nmapview(request_geo)\nThe request can be converted to a simple features object and mapped quite quickly. The Feature ID ‚Äúvalid_raw‚Äù value will need context from the CORI team.\n\nThe API is configured for wrapping in a general access function. Storing in the authentication would be done through an environment variable, but would work something like this:\n\nget_cori &lt;- function(dataset, geoids) {\n  # Some sort of function like `get_headers()` that grabs the stored tokens\n  # headers &lt;- get_headers()\n  endpoint &lt;-\nglue::glue(\"https://d25ssrwsq4u9bu.cloudfront.net/bcat/{dataset}/\ngeojson\")\n\n  req &lt;- httr::GET(url = endpoint,\n                         query = list(geoid_co = paste0(geoids, collapse = \",\")),\n                         httr::add_headers(headers))\n\nif (req$status_code == 200) {\n  # If successful, return the spatial object\n  req_sf &lt;- req %&gt;%\n    httr::content(as = \"text\") %&gt;%\n    sf::st_read(quiet = TRUE)\n\n  return(req_sf)\n} else {\n  # Otherwise, return the error message\n  msg &lt;- httr::content(req, as = \"text\")\n  rlang::abort(message = glue::glue(\"Your request failed. The error message is {msg}\"))\n}\n}\nNow, this will only work for endpoints that have the geoid_co parameter so broader adjustments would need to be made. Let‚Äôs try it out:\nawards &lt;- get_cori(\n  dataset = \"auction_904_subsidy_awards\",\n  geoids = c(\"48113\", \"48439\")\n)\nmapview(awards)\n\n\n\n\n\nPython access to the API will work much the same way, and quick mapping of the data is possible if the appropriate tools are installed. The workflow outlined below fetches the data using requests, then reads in the result with geopandas and maps the data with leafmap.\nimport requests\n\nurl =\n\"https://d25ssrwsq4u9bu.cloudfront.net/bcat/auction_904_subsidy_awards/geojson?geoid_co=47001,47003,47011\"\n\npayload={}\nheaders = {\n  'api': 'Xy4Gq51ixT3OYvZPqhb3D5OdemA3BYZ06ISg5Duo',\n  'Access': '',\n  'Authorization': 'Bearer\neyJraWQiOiJJRzhDcXFYenEzR1dGWTk5bUdwK0VkZzVNbUNLa1RCSXpJT3dJdFdQc\nmZzPSIsImFsZyI6IlJTMjU2In0.eyJzdWIiOiJjYmMyNWY4ZS01MmE3LTQwZDUtYm\nJlMS0wYmYwYTI5N2MxMGIiLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZSwiaXNzIjoiaHR\n0cHM6XC9cL2NvZ25pdG8taWRwLnVzLWVhc3QtMS5hbWF6b25hd3MuY29tXC91cy1l\nYXN0LTFfTkU5MXphYXBYIiwiY29nbml0bzp1c2VybmFtZSI6Im1mLWludC10ZXN0Q\nHlvcG1haWwuY29tIiwib3JpZ2luX2p0aSI6IjcxMWI4NWZkLTdkZmUtNDQ4Zi1hMD\nk0LTBlMjdjNzIzYjc0YSIsImF1ZCI6IjZ1bTk5ZnYycXRiNmY3aXNlM2kwMzd2bmE\niLCJldmVudF9pZCI6IjZjYWNkYzQ4LTE2YTEtNDcyNi04YTYyLWJmZmVlZGJjZjE2\nNiIsInRva2VuX3VzZSI6ImlkIiwiYXV0aF90aW1lIjoxNjU3MzI1MzAyLCJleHAiO\njE2NTczMjg5MDIsImlhdCI6MTY1NzMyNTMwMiwianRpIjoiMjZlMzUzMGYtMTc3Mi\n00NzUzLTk1ZDgtNDM5NjE5N2UxM2YzIiwiZW1haWwiOiJtZi1pbnQtdGVzdEB5b3B\ntYWlsLmNvbSJ9.ZNnTbwnBTQfC57G9zQnVQO3gFZh9IPh9Z4hJ85vCmfCpody0bBj\nyAGKWH8at26TscM6OZJzs52HFAGxhGUf6chCIqQUlXNB2q3UwuHd7qh9imA0EESfMt70V7nxRTS17BXXAwiMZhPgngk29mTQm9M4psL6n7z\nDr0heEwS7bOGH9Nzp55hdNWWEHKPGQr81mFloXJnhQiyMAHC3BkJx_\ncBiTSAQcFcqqPJW1iH7lyC9thmsboxOlLG9IK6R5J\nWR2uz05CJvp49L3XneAG-IWw3OSQuVi81UFIcv5RnAgi3MuLG6J8uS9sIa6QNSHxxLYicURRZC7ak0mVBvlaVA'\n}\n\nresponse = requests.request(\"GET\", url, headers=headers, data=payload)\n\nresponse.status_code\n\n200\nWith the data in hand, we can read into Python as a GeoDataFrame.\nimport geopandas as gp\n\ngeo = gp.read_file(response.text)\n\ngeo.plot()\n\nAnd interactively with leafmap:\n\nimport leafmap.leafmap as leafmap\n\nm = leafmap.Map(center = [86.9066448, 35.8682], zoom = 7)\n\nm.add_gdf(geo, \"BCAT\")\n\nm\n\nPutting this all together, a start at a function would look like this:\n\ndef get_cori(dataset, geoids):\n  # headers = get_headers()\n  payload={}\n\n  sep = \",\"\n\n  geoid_sep = sep.join(geoids)\n\n  url = \"https://d25ssrwsq4u9bu.cloudfront.net/bcat/\" + dataset + \"/geojson\"\n\n  qry = sep.join(geoids)\n\n  req = requests.get(url, headers=headers, data=payload, params={'geoid_co':geoid_sep})\n\n  if (req.status_code == 200):\n    json = req.text\n    geo = gp.read_file(json)\n    return geo\n  else:\n    msg = req.text\n    raise SystemExit(msg\nThe result works as below:\nawards = get_cori(\"auction_904_subsidy_awards\", geoids = [\"48113\", \"48439\"])\nawards.plot()"
  },
  {
    "objectID": "R-and-Python-API-Usage.html#using-the-api-from-r",
    "href": "R-and-Python-API-Usage.html#using-the-api-from-r",
    "title": "Summary",
    "section": "",
    "text": "Testing the default queries from R using httr::GET() returns GeoJSON that can be readily accessed by R‚Äôs spatial libraries. We‚Äôll first make a request to the API and check its performance:\nlibrary(httr)\n\nheaders = c(\n    'api' = 'Xy4Gq51ixT3OYvZPqhb3D5OdemA3BYZ06ISg5Duo',\n    'Access' = '',\n    'Authorization' = 'Bearer\neyJraWQiOiJJRzhDcXFYenEzR1dGWTk5bUdwK0VkZzVNbUNLa1RCSXpJT3dJdFdQc\nmZzPSIsImFsZyI6IlJTMjU2In0.eyJzdWIiOiJjYmMyNWY4ZS01MmE3LTQwZDUtYm\nJlMS0wYmYwYTI5N2MxMGIiLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZSwiaXNzIjoiaHR\n0cHM6XC9cL2NvZ25pdG8taWRwLnVzLWVhc3QtMS5hbWF6b25hd3MuY29tXC91cy1l\nYXN0LTFfTkU5MXphYXBYIiwiY29nbml0bzp1c2VybmFtZSI6Im1mLWludC10ZXN0Q\nHlvcG1haWwuY29tIiwib3JpZ2luX2p0aSI6IjcxMWI4NWZkLTdkZmUtNDQ4Zi1hMD\nk0LTBlMjdjNzIzYjc0YSIsImF1ZCI6IjZ1bTk5ZnYycXRiNmY3aXNlM2kwMzd2bmE\niLCJldmVudF9pZCI6IjZjYWNkYzQ4LTE2YTEtNDcyNi04YTYyLWJmZmVlZGJjZjE2\nNiIsInRva2VuX3VzZSI6ImlkIiwiYXV0aF90aW1lIjoxNjU3MzI1MzAyLCJleHAiO\njE2NTczMjg5MDIsImlhdCI6MTY1NzMyNTMwMiwianRpIjoiMjZlMzUzMGYtMTc3Mi\n00NzUzLTk1ZDgtNDM5NjE5N2UxM2YzIiwiZW1haWwiOiJtZi1pbnQtdGVzdEB5b3B\ntYWlsLmNvbSJ9.ZNnTbwnBTQfC57G9zQnVQO3gFZh9IPh9Z4hJ85vCmfCpody0bBj\nyAGKWH8at26TscM6OZJzs52HFAGxhGUf6chCIqQUlXNB2q3UwuHd7qh9imA0EESfMt70V7nxRTS17BXXAwiMZhPgngk29mTQm9M4psL6n7z\nDr0heEwS7bOGH9Nzp55hdNWWEHKPGQr81mFloXJnhQiyMAHC3BkJx_\ncBiTSAQcFcqqPJW1iH7lyC9thmsboxOlLG9IK6R5J\nWR2uz05CJvp49L3XneAG-IWw3OSQuVi81UFIcv5RnAgi3MuLG6J8uS9sIa6QNSHxxLYicURRZC7ak0mVBvlaVA'\n)\n\ntictoc::tic()\nrequest &lt;- GET(url = \"https://d25ssrwsq4u9bu.cloudfront.net/bcat/\ncounty_broadband_farm_bill_eligibility/geojson?state_abbr=TN\",\nadd_headers(headers))\ntictoc::toc()\n\n10.996 sec elapsed \nIn loading into R, the farm bill eligibility data tends to load in between 10 and 16 seconds. This is likely fine for data science purposes, though you may prefer to use the MVT endpoint for dynamic querying of tiles in an interactive app context. The county-specific datasets load in a second or less, even when a few counties are requested.\nThe request can be read into R as a simple features object and mapped for review:\nlibrary(sf)\nlibrary(mapview)\n\nrequest_geo &lt;- request %&gt;%\n   content(as = \"text\") %&gt;%\n   st_read(quiet = TRUE)\n\nmapview(request_geo)\nThe request can be converted to a simple features object and mapped quite quickly. The Feature ID ‚Äúvalid_raw‚Äù value will need context from the CORI team.\n\nThe API is configured for wrapping in a general access function. Storing in the authentication would be done through an environment variable, but would work something like this:\n\nget_cori &lt;- function(dataset, geoids) {\n  # Some sort of function like `get_headers()` that grabs the stored tokens\n  # headers &lt;- get_headers()\n  endpoint &lt;-\nglue::glue(\"https://d25ssrwsq4u9bu.cloudfront.net/bcat/{dataset}/\ngeojson\")\n\n  req &lt;- httr::GET(url = endpoint,\n                         query = list(geoid_co = paste0(geoids, collapse = \",\")),\n                         httr::add_headers(headers))\n\nif (req$status_code == 200) {\n  # If successful, return the spatial object\n  req_sf &lt;- req %&gt;%\n    httr::content(as = \"text\") %&gt;%\n    sf::st_read(quiet = TRUE)\n\n  return(req_sf)\n} else {\n  # Otherwise, return the error message\n  msg &lt;- httr::content(req, as = \"text\")\n  rlang::abort(message = glue::glue(\"Your request failed. The error message is {msg}\"))\n}\n}\nNow, this will only work for endpoints that have the geoid_co parameter so broader adjustments would need to be made. Let‚Äôs try it out:\nawards &lt;- get_cori(\n  dataset = \"auction_904_subsidy_awards\",\n  geoids = c(\"48113\", \"48439\")\n)\nmapview(awards)"
  },
  {
    "objectID": "R-and-Python-API-Usage.html#using-the-api-from-python",
    "href": "R-and-Python-API-Usage.html#using-the-api-from-python",
    "title": "Summary",
    "section": "",
    "text": "Python access to the API will work much the same way, and quick mapping of the data is possible if the appropriate tools are installed. The workflow outlined below fetches the data using requests, then reads in the result with geopandas and maps the data with leafmap.\nimport requests\n\nurl =\n\"https://d25ssrwsq4u9bu.cloudfront.net/bcat/auction_904_subsidy_awards/geojson?geoid_co=47001,47003,47011\"\n\npayload={}\nheaders = {\n  'api': 'Xy4Gq51ixT3OYvZPqhb3D5OdemA3BYZ06ISg5Duo',\n  'Access': '',\n  'Authorization': 'Bearer\neyJraWQiOiJJRzhDcXFYenEzR1dGWTk5bUdwK0VkZzVNbUNLa1RCSXpJT3dJdFdQc\nmZzPSIsImFsZyI6IlJTMjU2In0.eyJzdWIiOiJjYmMyNWY4ZS01MmE3LTQwZDUtYm\nJlMS0wYmYwYTI5N2MxMGIiLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZSwiaXNzIjoiaHR\n0cHM6XC9cL2NvZ25pdG8taWRwLnVzLWVhc3QtMS5hbWF6b25hd3MuY29tXC91cy1l\nYXN0LTFfTkU5MXphYXBYIiwiY29nbml0bzp1c2VybmFtZSI6Im1mLWludC10ZXN0Q\nHlvcG1haWwuY29tIiwib3JpZ2luX2p0aSI6IjcxMWI4NWZkLTdkZmUtNDQ4Zi1hMD\nk0LTBlMjdjNzIzYjc0YSIsImF1ZCI6IjZ1bTk5ZnYycXRiNmY3aXNlM2kwMzd2bmE\niLCJldmVudF9pZCI6IjZjYWNkYzQ4LTE2YTEtNDcyNi04YTYyLWJmZmVlZGJjZjE2\nNiIsInRva2VuX3VzZSI6ImlkIiwiYXV0aF90aW1lIjoxNjU3MzI1MzAyLCJleHAiO\njE2NTczMjg5MDIsImlhdCI6MTY1NzMyNTMwMiwianRpIjoiMjZlMzUzMGYtMTc3Mi\n00NzUzLTk1ZDgtNDM5NjE5N2UxM2YzIiwiZW1haWwiOiJtZi1pbnQtdGVzdEB5b3B\ntYWlsLmNvbSJ9.ZNnTbwnBTQfC57G9zQnVQO3gFZh9IPh9Z4hJ85vCmfCpody0bBj\nyAGKWH8at26TscM6OZJzs52HFAGxhGUf6chCIqQUlXNB2q3UwuHd7qh9imA0EESfMt70V7nxRTS17BXXAwiMZhPgngk29mTQm9M4psL6n7z\nDr0heEwS7bOGH9Nzp55hdNWWEHKPGQr81mFloXJnhQiyMAHC3BkJx_\ncBiTSAQcFcqqPJW1iH7lyC9thmsboxOlLG9IK6R5J\nWR2uz05CJvp49L3XneAG-IWw3OSQuVi81UFIcv5RnAgi3MuLG6J8uS9sIa6QNSHxxLYicURRZC7ak0mVBvlaVA'\n}\n\nresponse = requests.request(\"GET\", url, headers=headers, data=payload)\n\nresponse.status_code\n\n200\nWith the data in hand, we can read into Python as a GeoDataFrame.\nimport geopandas as gp\n\ngeo = gp.read_file(response.text)\n\ngeo.plot()\n\nAnd interactively with leafmap:\n\nimport leafmap.leafmap as leafmap\n\nm = leafmap.Map(center = [86.9066448, 35.8682], zoom = 7)\n\nm.add_gdf(geo, \"BCAT\")\n\nm\n\nPutting this all together, a start at a function would look like this:\n\ndef get_cori(dataset, geoids):\n  # headers = get_headers()\n  payload={}\n\n  sep = \",\"\n\n  geoid_sep = sep.join(geoids)\n\n  url = \"https://d25ssrwsq4u9bu.cloudfront.net/bcat/\" + dataset + \"/geojson\"\n\n  qry = sep.join(geoids)\n\n  req = requests.get(url, headers=headers, data=payload, params={'geoid_co':geoid_sep})\n\n  if (req.status_code == 200):\n    json = req.text\n    geo = gp.read_file(json)\n    return geo\n  else:\n    msg = req.text\n    raise SystemExit(msg\nThe result works as below:\nawards = get_cori(\"auction_904_subsidy_awards\", geoids = [\"48113\", \"48439\"])\nawards.plot()"
  },
  {
    "objectID": "The-coriverse-suite-of-packages.html",
    "href": "The-coriverse-suite-of-packages.html",
    "title": "Core proprietary packages",
    "section": "",
    "text": "Core proprietary packages\n\ncori.utils\n\nA collection of simple helper functions and small, useful data sets intended to make life easier and solve simple problems\n\n\n\ncori.db\n\nA collection of functions for working the CORI/RISI Postgres database\n\n\n\nRto\n\nA system agnostic wrapper for the CARTO API for uploading data\n\n\n\n\n\nCore External Packages\nThese packages are standard across MDA workflows, and should be preferred unless a specific use case demands otherwise.\n\ntidyverse\n\nPreferred suite of packages for most data analysis tasks\n\n\n\nsf\n\nCore MDA spatial analysis library\n\n\n\nleaflet\n\nAd-hoc mapping of spatial data\n\n\n\nrairtable\n\nEfficient, Tidyverse-friendly Airtable API interaction",
    "crumbs": [
      "Home",
      "Team Onboarding",
      "Core proprietary packages"
    ]
  },
  {
    "objectID": "QGIS.html",
    "href": "QGIS.html",
    "title": "CORI/RISI Mapping & Data Analysis (MDA)",
    "section": "",
    "text": "This will include some materials for the use of QGIS",
    "crumbs": [
      "Home",
      "Team Onboarding",
      "Connecting QGIS to CORI RDS DB"
    ]
  },
  {
    "objectID": "QGIS.html#connecting-qgis-to-cori-rds-db",
    "href": "QGIS.html#connecting-qgis-to-cori-rds-db",
    "title": "CORI/RISI Mapping & Data Analysis (MDA)",
    "section": "Connecting QGIS to CORI RDS DB",
    "text": "Connecting QGIS to CORI RDS DB\nIn QGIS:\n\nQGIS -&gt; Preferences -&gt; Authentication\n\n\n\nIn Authentication use the green ‚ûï to add a new way of doing it.\n\nIf it is the first time it will ask you to set up a master password (I think it is just for mac).\n\nUse the drop down menu to select Basic authentication\nPick a name: I am using cori\nProvide the correct Username and Password\n\nIt should look like this:\n\nIn QGIS Browser:\n\nRight click on PostgreSQL\nNew connections\n\n\n\nPick a name (again I am using CORI)\nHost: our aws RDS\nPort: we are using the default one (5432)\nDatabase name: the name of the DB\nIn Athentication select the previous configurations we have done\nSSL_mode need to be set on ‚ÄúPrefer‚Äù\nYou can ‚ÄúTest Connection‚Äù\n\n\nIf it is good you can press ok if not review the Host name other parameters.\nYou should be able to browse schemas and tables from the database.\n\nResources:\nQGIS LTR Doc: https://docs.qgis.org/3.28/en/docs/user_manual/auth_system/auth_overview.html\nTo interact with QGIS in R: https://r-spatial.github.io/qgisprocess/",
    "crumbs": [
      "Home",
      "Team Onboarding",
      "Connecting QGIS to CORI RDS DB"
    ]
  }
]